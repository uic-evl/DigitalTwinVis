{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0f747703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score,precision_recall_fscore_support\n",
    "from Constants import *\n",
    "from Preprocessing import *\n",
    "from survival_losses import lognormal_loss,conditional_lognormal_loss\n",
    "from Models import *\n",
    "import copy\n",
    "from Utils import *\n",
    "#this is a subset of https://github.com/autonlab/auton-survival/tree/master. may remove dcm to reduce dependencies\n",
    "from survival_models import dcm, dsm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a799344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OS (Calculated)</th>\n",
       "      <th>Locoregional control (Time)</th>\n",
       "      <th>FDM (months)</th>\n",
       "      <th>time_to_event</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.033333</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>4.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.466667</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>8.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "      <td>143.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>144.366667</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>144.366667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>148.366667</td>\n",
       "      <td>148.366667</td>\n",
       "      <td>136.033333</td>\n",
       "      <td>136.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "      <td>152.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "      <td>155.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OS (Calculated)  Locoregional control (Time)  FDM (months)  \\\n",
       "id                                                                  \n",
       "3             6.033333                     4.700000      6.033333   \n",
       "5             7.333333                     7.333333      7.333333   \n",
       "6             7.466667                     7.466667      7.466667   \n",
       "7             7.800000                     7.800000      7.800000   \n",
       "8             8.066667                     8.066667      8.066667   \n",
       "...                ...                          ...           ...   \n",
       "10201       143.200000                   143.200000    143.200000   \n",
       "10202       144.366667                   144.366667    144.366667   \n",
       "10203       148.366667                   148.366667    136.033333   \n",
       "10204       152.600000                   152.600000    152.600000   \n",
       "10205       155.533333                   155.533333    155.533333   \n",
       "\n",
       "       time_to_event  \n",
       "id                    \n",
       "3           4.700000  \n",
       "5           6.000000  \n",
       "6           6.000000  \n",
       "7           6.000000  \n",
       "8           8.066667  \n",
       "...              ...  \n",
       "10201     143.200000  \n",
       "10202       6.000000  \n",
       "10203     136.033333  \n",
       "10204     152.600000  \n",
       "10205     155.533333  \n",
       "\n",
       "[536 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DTDataset(use_smote=False)\n",
    "data.processed_df[Const.timeseries_outcomes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "178c353f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([389, 83])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr,xtst,ytr,ytst = transition_sample(3,data)\n",
    "xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5d2c81d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0.2411, 0.2563, 0.2586, 0.2398, 0.0926, 0.2780, 0.1258, 0.2425, 0.1349,\n",
       "          0.2364, 0.2210, 0.0969, 0.1478, 0.1118, 0.2433, 0.1603, 0.1714, 0.0827,\n",
       "          0.1958, 0.1780, 0.1214, 0.2280, 0.1447, 0.0909, 0.1439, 0.3231, 0.2110,\n",
       "          0.0576, 0.1478, 0.2340, 0.2161, 0.2609, 0.2840, 0.2186, 0.1298, 0.2145,\n",
       "          0.2073, 0.1926, 0.1290, 0.1581, 0.2466, 0.2025, 0.1468, 0.2398, 0.1728,\n",
       "          0.1198, 0.1546, 0.0653, 0.1836, 0.2446, 0.2187, 0.2789, 0.2013, 0.2538,\n",
       "          0.0162, 0.0536, 0.1278, 0.1473, 0.0899, 0.1406, 0.2324, 0.0946, 0.2540,\n",
       "          0.1385, 0.1470, 0.1294, 0.0396, 0.0359, 0.2384, 0.2156, 0.1498, 0.0351,\n",
       "          0.2155, 0.0481, 0.2357, 0.0812, 0.2084, 0.2777, 0.1958, 0.1724, 0.1889,\n",
       "          0.2640, 0.1361, 0.1823, 0.0311, 0.0235, 0.0164, 0.1160, 0.2780, 0.2303,\n",
       "          0.2138, 0.2067, 0.1564, 0.1295, 0.1384, 0.0952, 0.2723, 0.1478, 0.1117,\n",
       "          0.1650, 0.0225, 0.1487, 0.0921, 0.1286, 0.1713, 0.2177, 0.2705, 0.2554,\n",
       "          0.1881, 0.2945, 0.2443, 0.2399, 0.0442, 0.2706, 0.1061, 0.1703, 0.1540,\n",
       "          0.2129, 0.2290, 0.1591, 0.2608, 0.0645, 0.1514, 0.2360, 0.2235, 0.2592,\n",
       "          0.1186, 0.1958, 0.0359, 0.1930, 0.1313, 0.2202, 0.0437, 0.1576, 0.2716,\n",
       "          0.0289, 0.1893, 0.1035, 0.1428, 0.1992, 0.2427, 0.1407, 0.0134, 0.1374,\n",
       "          0.0441, 0.1412, 0.2433, 0.0505, 0.1339, 0.1667, 0.2158, 0.0671, 0.2154,\n",
       "          0.1135, 0.1500, 0.1503, 0.2704, 0.2733, 0.0707, 0.0686, 0.2096, 0.1434,\n",
       "          0.1738, 0.0876, 0.1985, 0.3232, 0.1439, 0.0319, 0.2009, 0.1478, 0.1266,\n",
       "          0.1989, 0.0147, 0.1839, 0.2509, 0.1277, 0.1303, 0.2678, 0.1320, 0.2394,\n",
       "          0.1470, 0.2289, 0.1011, 0.2387, 0.2202, 0.2310, 0.1235, 0.0874, 0.1568,\n",
       "          0.1091, 0.3211, 0.2404, 0.0142, 0.2521, 0.0694, 0.1448, 0.0781, 0.1383,\n",
       "          0.1511, 0.2527, 0.1135, 0.1425, 0.1593, 0.1171, 0.1416, 0.1050, 0.0389,\n",
       "          0.1354, 0.1291, 0.1196, 0.1501, 0.1143, 0.0168, 0.2459, 0.1626, 0.2080,\n",
       "          0.0366, 0.2438, 0.1469, 0.1167, 0.0289, 0.1615, 0.0490, 0.2730, 0.1551,\n",
       "          0.0253, 0.1765, 0.2628, 0.2078, 0.0286, 0.0987, 0.1281, 0.1423, 0.2258,\n",
       "          0.0833, 0.1364, 0.2201, 0.1319, 0.1275, 0.2347, 0.1237, 0.1261, 0.1979,\n",
       "          0.1886, 0.1409, 0.2278, 0.1329, 0.0355, 0.2284, 0.1298, 0.2315, 0.1824,\n",
       "          0.1297, 0.1326, 0.2010, 0.1426, 0.1373, 0.2541, 0.0953, 0.1237, 0.1967,\n",
       "          0.1788, 0.1382, 0.1302, 0.2055, 0.2394, 0.1492, 0.0855, 0.2140, 0.1214,\n",
       "          0.1981, 0.0791, 0.1351, 0.1754, 0.0894, 0.0829, 0.1345, 0.2444, 0.1332,\n",
       "          0.0339, 0.2256, 0.1327, 0.2231, 0.1491, 0.2484, 0.0833, 0.0861, 0.1673,\n",
       "          0.1292, 0.1496, 0.2091, 0.0559, 0.1268, 0.0842, 0.0867, 0.1926, 0.1921,\n",
       "          0.1272, 0.2030, 0.1377, 0.0418, 0.2674, 0.2604, 0.2476, 0.0666, 0.1337,\n",
       "          0.0995, 0.1579, 0.1134, 0.0831, 0.2356, 0.1296, 0.2154, 0.1316, 0.1483,\n",
       "          0.1618, 0.2231, 0.1725, 0.0461, 0.1858, 0.2617, 0.1950, 0.2403, 0.1396,\n",
       "          0.2350, 0.1290, 0.1155, 0.2666, 0.1445, 0.2517, 0.1159, 0.0626, 0.2053,\n",
       "          0.2154, 0.1547, 0.1277, 0.0265, 0.0447, 0.1364, 0.1277, 0.2487, 0.1274,\n",
       "          0.1048, 0.1579, 0.0553, 0.1658, 0.1771, 0.2271, 0.2608, 0.2326, 0.1121,\n",
       "          0.0097, 0.0178, 0.1337, 0.1756, 0.1168, 0.2417, 0.1308, 0.2823, 0.1616,\n",
       "          0.1497, 0.1881, 0.2141, 0.0291, 0.0542, 0.0384, 0.1258, 0.2232, 0.2246,\n",
       "          0.1187, 0.1262, 0.1580, 0.0332, 0.2365, 0.1295, 0.1292, 0.1476, 0.1474,\n",
       "          0.0586, 0.1056, 0.1609, 0.3629, 0.0304, 0.1228, 0.1963, 0.2740, 0.1439,\n",
       "          0.1060, 0.1112], dtype=torch.float64, grad_fn=<PermuteBackward0>)],\n",
       " (Parameter containing:\n",
       "  tensor([1., 1., 1.], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1.], requires_grad=True)))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_representation(inputdim, layers, activation, bias=False):\n",
    "    if activation == 'ReLU6': act = nn.ReLU6()\n",
    "    elif activation == 'ReLU': act = nn.ReLU()\n",
    "    elif activation == 'SeLU': act = nn.SELU()\n",
    "    elif activation == 'Tanh': act = nn.Tanh()\n",
    "\n",
    "    modules = []\n",
    "    prevdim = inputdim\n",
    "\n",
    "    for hidden in layers:\n",
    "        modules.append(nn.Linear(prevdim, hidden, bias=bias))\n",
    "        modules.append(act)\n",
    "        prevdim = hidden\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "class DSM(torch.nn.Module):\n",
    "\n",
    "    def _init_dsm_layers(self, lastdim, n_outcomes):\n",
    "\n",
    "        self.act = nn.Tanh()\n",
    "        self.shape = torch.nn.ParameterList([nn.Parameter(torch.ones(self.k)) for i in range(n_outcomes)])\n",
    "        self.scale = torch.nn.ParameterList([nn.Parameter(torch.ones(self.k)) for i in range(n_outcomes)])\n",
    "     \n",
    "        self.gate = torch.nn.ModuleList([nn.Sequential( nn.Linear(lastdim, self.k, bias=False) ) for i in range(n_outcomes)])\n",
    "\n",
    "        self.scaleg =  torch.nn.ModuleList([nn.Sequential( nn.Linear(lastdim, self.k, bias=True) ) for i in range(n_outcomes)])\n",
    "\n",
    "        self.shapeg = torch.nn.ModuleList([nn.Sequential( nn.Linear(lastdim, self.k, bias=False) ) for i in range(n_outcomes)])\n",
    "\n",
    "    def __init__(self, inputdim, k=3, layers=None, dist='LogNormal',\n",
    "               temp=1000., discount=1.0, optimizer='Adam',\n",
    "               input_dropout=0,\n",
    "               embedding_dropout=0.5,\n",
    "                 activation='ReLU6',\n",
    "                 n_outcomes=3,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        #I'm only using lognormal because weibull explosed\n",
    "        self.k = k\n",
    "        self.dist = dist\n",
    "        self.temp = float(temp)\n",
    "        self.discount = float(discount)\n",
    "        self.optimizer = optimizer\n",
    "        self.n_outcomes=n_outcomes\n",
    "\n",
    "        if layers is None: layers = [1000]\n",
    "        self.layers = layers\n",
    "\n",
    "        if len(layers) == 0: lastdim = inputdim\n",
    "        else: lastdim = layers[-1]\n",
    "\n",
    "        self._init_dsm_layers(lastdim,n_outcomes)\n",
    "        self.activation_name=activation\n",
    "        self.embedding = create_representation(inputdim, layers, activation)\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.squish = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def _cdf(self,shape,scale,logits,t_horizon):\n",
    "        k_ =  shape\n",
    "        b_ = scale\n",
    "        logits = self.squish(logits)\n",
    "        if isinstance(t_horizon,float) or isinstance(t_horizon,int):\n",
    "            t_horizon = [t_horizon]\n",
    "        t_horz = torch.tensor(t_horizon).double().to(logits.device)\n",
    "        t_horz = t_horz.repeat(shape.shape[0], 1)\n",
    "        cdfs = []\n",
    "        \n",
    "        for j in range(len(t_horizon)):\n",
    "            t = t_horz[:, j]\n",
    "            lcdfs = []\n",
    "\n",
    "            for g in range(self.k):\n",
    "                mu = k_[:, g]\n",
    "                sigma = b_[:, g]\n",
    "\n",
    "                s = torch.div(torch.log(t) - mu, torch.exp(sigma)*np.sqrt(2))\n",
    "                s = 0.5 - 0.5*torch.erf(s)\n",
    "                s = torch.log(s)\n",
    "                lcdfs.append(s)\n",
    "\n",
    "            lcdfs = torch.stack(lcdfs, dim=1)\n",
    "            lcdfs = lcdfs+logits\n",
    "            lcdfs = torch.logsumexp(lcdfs, dim=1)\n",
    "            cdfs.append(lcdfs)\n",
    "\n",
    "        return cdfs\n",
    "\n",
    "    def forward(self, x, t=None,**kwargs):\n",
    "        \"\"\"The forward function that is called when data is passed through DSM.\n",
    "\n",
    "        Args:\n",
    "          x:\n",
    "            a torch.tensor of the input features.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.input_dropout(x)\n",
    "        xrep = self.embedding(x)\n",
    "        xrep = self.embedding_dropout(xrep)\n",
    "        dim = x.shape[0]\n",
    "        \n",
    "        shapes = []\n",
    "        scales = []\n",
    "        logitss = []\n",
    "        survivals = []\n",
    "        for i in range(self.n_outcomes):\n",
    "            shape = self.act(self.shapeg[i](xrep)) + self.shape[i].expand(dim,-1)\n",
    "            scale = self.act(self.scaleg[i](xrep)) + self.scale[i].expand(dim,-1)\n",
    "            logits = self.gate[i](xrep)/self.temp\n",
    "        \n",
    "            if t is None:\n",
    "                shapes.append(shape)\n",
    "                scales.append(scale)\n",
    "                logitss.append(logits)\n",
    "            else:\n",
    "                cdf = self._cdf(shape,scale,logits,t)\n",
    "                survival = [torch.exp(c).T for c in cdf]\n",
    "                survivals.append(survival)\n",
    "        if t is None:\n",
    "            return shapes, scales, logitss\n",
    "        return survivals\n",
    "\n",
    "    def get_shape_scale(self,i=None, **kwargs):\n",
    "        if i is None:\n",
    "            return(self.shape, self.scale)\n",
    "        assert(i < self.n_outcomes and i >= 0)\n",
    "        return (self.shape[i],self.scale[i])\n",
    "    \n",
    "test = DSM(xtr.shape[1])\n",
    "test(xtr,42)[0], test.get_shape_scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432271da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tte_outcome(dataset,outcome,times=None, mintime = 1,maxtime = None,ids=None):\n",
    "    #turns the timeseires censored output into an array of shape n_timepoints\n",
    "    #time is by default 1 value until maxtime\n",
    "    df = dataset.processed_df\n",
    "    if ids is not None:\n",
    "        df = df.loc[ids]\n",
    "    values = df[outcome].values\n",
    "    if maxtime is None:\n",
    "        maxtime = values.max()\n",
    "    event = values <= maxtime\n",
    "    return torch.tensor(event), torch.tensor(values)\n",
    "\n",
    "def format_tte_outcomes(dataset,outcomes,**kwargs):\n",
    "    events = []\n",
    "    times = []\n",
    "    for outcome in outcomes:\n",
    "        e,t = format_tte_outcome(dataset,outcome,**kwargs)\n",
    "        events.append(e)\n",
    "        times.append(t)\n",
    "    events = torch.stack(events)\n",
    "    times = torch.stack(times)\n",
    "    print('events',events.shape)\n",
    "    return events,times\n",
    "\n",
    "def pretrain_dsm(dataset,\n",
    "                 model=None,\n",
    "                 outcomes = ['time_to_event'],\n",
    "                 maxtime=72,lr=.01,\n",
    "                 epochs=100000,\n",
    "                 patience=10,\n",
    "                 save_file=None,**model_kwargs):\n",
    "    train_ids, test_ids = get_tt_split(dataset.processed_df.reset_index())\n",
    "    \n",
    "    state = 3\n",
    "    xtrain = df_to_torch(dataset.get_input_state(step=state,ids=train_ids))\n",
    "    #event series (1 = happend, 0 = didn't happen or already happened), t = 1-d list of times used of shape ytrain.shape[1]\n",
    "    ytrain, ttrain = format_tte_outcomes(dataset,outcomes,ids=train_ids,maxtime=maxtime)\n",
    "    if model is None:\n",
    "        model = DSM(xtrain.shape[1],n_outcomes=len(outcomes),**model_kwargs)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    best_loss = 100000000000000000000000\n",
    "    steps_since_improvement = 0\n",
    "    if save_file is None: save_file= '../data/models/dsm_'+''.join(outcomes)+'.tar'\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        curr_loss = 0\n",
    "        for i in range(len(outcomes)):\n",
    "            shape, scale = model.get_shape_scale(i)\n",
    "            curr_loss += lognormal_loss(shape,scale,ttrain[i],ytrain[i],model.k)\n",
    "        curr_loss.backward()\n",
    "        print('pretrain epoch',epoch,'loss',curr_loss.item(),end='\\r')\n",
    "        optimizer.step()\n",
    "        if curr_loss.item() < best_loss:\n",
    "            best_loss = curr_loss.item()\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "            if steps_since_improvement > patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    print('best pretrain loss',best_loss,'epochs',epoch)\n",
    "    return model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,f1_score,balanced_accuracy_score,matthews_corrcoef\n",
    "def eval_model(model, xtest, ttest,ytest,timepoints = None,outcome_names=None):\n",
    "    #ttest is time of event, ytest is if it happened at all\n",
    "    if timepoints is None:\n",
    "        timepoints = [12,24,36,48]\n",
    "    ypreds = model(xtest,t=timepoints)\n",
    "    allres = {}\n",
    "    if outcome_names is None:\n",
    "        outcome_names = [str(i) for i in range(model.n_outcomes)]\n",
    "    for i, outcome in enumerate(outcome_names):\n",
    "        res = {}\n",
    "        for ii,time in enumerate(timepoints):\n",
    "            ypred =ypreds[i][ii].cpu().detach().numpy()\n",
    "            ytrue = (ttest[i] >= time).cpu().detach().numpy()\n",
    "            entry = {}\n",
    "            entry['roc_score'] = roc_auc_score(ytrue,ypred)\n",
    "            entry['f1'] = f1_score(ytrue,ypred >= .5)\n",
    "            entry['matthews'] = matthews_corrcoef(ytrue,ypred >= .5)\n",
    "            res[time] = entry\n",
    "        allres[outcome] = res\n",
    "    return allres\n",
    "\n",
    "def train_dsm(dataset, outcomes=Const.timeseries_outcomes,maxtime=72,save_file=None,main_epochs=1000,main_lr=.001,patience=10,**kwargs):\n",
    "    #ok they do something different so like check this later?\n",
    "    #the use this as a \"premodel\" and load the shape and scale weights?\n",
    "    if save_file is None:\n",
    "        save_file= '../data/models/dsm_'+''.join(outcomes)+'.tar'\n",
    "        pretrain_save_file =  '../data/models/dsm_'+''.join(outcomes)+'_pretrain.tar'\n",
    "        \n",
    "    \n",
    "    train_ids, test_ids = get_tt_split(dataset.processed_df.reset_index())\n",
    "    \n",
    "    state = 3\n",
    "    xtrain = df_to_torch(dataset.get_input_state(step=state,ids=train_ids))\n",
    "    xtest = df_to_torch(dataset.get_input_state(step=state,ids=test_ids))\n",
    "    #n_outcomes by n_items, ytrain is event y/n, ttrain is time of event or last followup\n",
    "    ytrain, ttrain = format_tte_outcomes(dataset,outcomes,ids=train_ids,maxtime=maxtime)\n",
    "    ytest, ttest = format_tte_outcomes(dataset,outcomes,ids=test_ids,maxtime=maxtime)\n",
    "    model = DSM(xtrain.shape[1],n_outcomes=len(outcomes),**kwargs)\n",
    "    premodel = pretrain_dsm(dataset,outcomes=outcomes,maxtime=maxtime,save_file=pretrain_save_file,epochs=10,**kwargs)\n",
    "    for i in range(premodel.n_outcomes):\n",
    "        model.shape[i].data = premodel.shape[i].data\n",
    "        model.scale[i].data = premodel.scale[i].data\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=main_lr)\n",
    "    best_loss = 100000000000000\n",
    "    best_metrics = {}\n",
    "    steps_since_improvement = 0\n",
    "    for epoch in range(main_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.train()\n",
    "        curr_loss = 0\n",
    "        shapes,scales,logitss = model(xtrain)\n",
    "        for i in range(model.n_outcomes):\n",
    "            curr_loss += conditional_lognormal_loss(shapes[i],scales[i],logitss[i],ttrain[i],ytrain[i],model.k,discount=model.discount)/model.n_outcomes \n",
    "        curr_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            shapes2,scales2,logitss2 = model(xtest)\n",
    "            val_loss = 0\n",
    "            for i in range(model.n_outcomes):\n",
    "                val_loss += conditional_lognormal_loss(shapes2[i],scales2[i],logitss2[i],ttest[i],ytest[i],model.k,discount=model.discount)/model.n_outcomes \n",
    "            val_metrics= eval_model(model,xtest,ttest,ytest)\n",
    "        print('val loss',val_loss.item())\n",
    "        print('val metrics',val_metrics)\n",
    "        print('____')\n",
    "        if val_loss.item() < best_loss:\n",
    "            best_loss = val_loss.item()\n",
    "            best_metrics = val_metrics\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "            if steps_since_improvement > patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    print('best_loss',best_loss)\n",
    "    print(best_metrics)\n",
    "    return model\n",
    "\n",
    "from survival_losses import * \n",
    "test = train_dsm(data,k=10)\n",
    "test(xtr,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f1f7bc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9100</th>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>0</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9102</th>\n",
       "      <td>0</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      event  time\n",
       "0         0  2029\n",
       "1         1     4\n",
       "2         1    47\n",
       "3         1   133\n",
       "4         0  2029\n",
       "...     ...   ...\n",
       "9100      0   350\n",
       "9101      0   347\n",
       "9102      0   346\n",
       "9103      1     7\n",
       "9104      1   198\n",
       "\n",
       "[9105 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc1055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
