{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7418d82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[133,\n",
       " 47,\n",
       " 35,\n",
       " 10,\n",
       " 279,\n",
       " 5056,\n",
       " 5035,\n",
       " 224,\n",
       " 209,\n",
       " 10063,\n",
       " 2006,\n",
       " 5020,\n",
       " 271,\n",
       " 10014,\n",
       " 5080,\n",
       " 10097,\n",
       " 10125,\n",
       " 10106,\n",
       " 2032,\n",
       " 10169,\n",
       " 2024,\n",
       " 286,\n",
       " 2015,\n",
       " 2019,\n",
       " 10026,\n",
       " 5040,\n",
       " 236,\n",
       " 187,\n",
       " 10161,\n",
       " 211,\n",
       " 5103,\n",
       " 10178,\n",
       " 2026,\n",
       " 10137,\n",
       " 184,\n",
       " 199,\n",
       " 10040,\n",
       " 272,\n",
       " 68,\n",
       " 5105,\n",
       " 10177,\n",
       " 228,\n",
       " 44,\n",
       " 242,\n",
       " 9,\n",
       " 5101,\n",
       " 10104,\n",
       " 10165,\n",
       " 10007,\n",
       " 10133,\n",
       " 10145,\n",
       " 10016,\n",
       " 264,\n",
       " 5098,\n",
       " 10023,\n",
       " 10050,\n",
       " 5120,\n",
       " 227,\n",
       " 5118,\n",
       " 2005,\n",
       " 5053,\n",
       " 10135,\n",
       " 5007,\n",
       " 10092,\n",
       " 36,\n",
       " 2001,\n",
       " 5115,\n",
       " 10005,\n",
       " 10102,\n",
       " 189,\n",
       " 5036,\n",
       " 10088,\n",
       " 254,\n",
       " 10130,\n",
       " 10086,\n",
       " 25,\n",
       " 5001,\n",
       " 5065,\n",
       " 10084,\n",
       " 195,\n",
       " 5099,\n",
       " 3,\n",
       " 5093,\n",
       " 10094,\n",
       " 7,\n",
       " 5038,\n",
       " 10068,\n",
       " 5032,\n",
       " 202,\n",
       " 274,\n",
       " 45,\n",
       " 2017,\n",
       " 10176,\n",
       " 217,\n",
       " 10160,\n",
       " 5082,\n",
       " 10012,\n",
       " 10017,\n",
       " 10100,\n",
       " 2031,\n",
       " 77,\n",
       " 10066,\n",
       " 5078,\n",
       " 117,\n",
       " 10010,\n",
       " 10170,\n",
       " 10190,\n",
       " 10058,\n",
       " 5049,\n",
       " 5086,\n",
       " 5052,\n",
       " 268,\n",
       " 2029,\n",
       " 5084,\n",
       " 10105,\n",
       " 10013,\n",
       " 245,\n",
       " 5048,\n",
       " 2020,\n",
       " 215,\n",
       " 10046,\n",
       " 5117,\n",
       " 5033,\n",
       " 267,\n",
       " 5003,\n",
       " 168,\n",
       " 31,\n",
       " 10049,\n",
       " 10180,\n",
       " 190,\n",
       " 287,\n",
       " 284,\n",
       " 5054,\n",
       " 10101,\n",
       " 208,\n",
       " 5077,\n",
       " 10091,\n",
       " 10172,\n",
       " 288,\n",
       " 5109,\n",
       " 10126,\n",
       " 10153,\n",
       " 10123,\n",
       " 5107,\n",
       " 194,\n",
       " 10131]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Const:\n",
    "    data_dir = '../data'\n",
    "    twin_data = data_dir + 'digital_twin_data.csv'\n",
    "    twin_ln_data = data_dir + 'digital_twin_ln_data.csv'\n",
    "    \n",
    "    rename_dict = {\n",
    "        'Dummy ID': 'id',\n",
    "        'Age at Diagnosis (Calculated)': 'age',\n",
    "        'Feeding tube 6m': 'FT',\n",
    "        'Affected Lymph node UPPER': 'affected_nodes',\n",
    "        'Aspiration rate(Y/N)': 'AS',\n",
    "        'Neck boost (Y/N)': 'neck_boost',\n",
    "        'Gender': 'gender',\n",
    "        'Tm Laterality (R/L)': 'laterality',\n",
    "        'AJCC 8th edition': 'ajcc8',\n",
    "        'AJCC 7th edition':'ajcc7',\n",
    "        'N_category_full': 'N-category',\n",
    "        'HPV/P16 status': 'hpv',\n",
    "        'Tumor subsite (BOT/Tonsil/Soft Palate/Pharyngeal wall/GPS/NOS)': 'subsite',\n",
    "        'Total dose': 'total_dose',\n",
    "        'Therapeutic combination': 'treatment',\n",
    "        'Smoking status at Diagnosis (Never/Former/Current)': 'smoking_status',\n",
    "        'Smoking status (Packs/Year)': 'packs_per_year',\n",
    "        'Overall Survival (1=alive,0=dead)': 'os',\n",
    "        'Dose/fraction (Gy)': 'dose_fraction'\n",
    "    }\n",
    "    \n",
    "    dlt_dict = {\n",
    "         'Allergic reaction to Cetuximab': 'DLT_Other',\n",
    "         'Cardiological (A-fib)': 'DLT_Other',\n",
    "         'Dermatological': 'DLT_Dermatological',\n",
    "         'Failure to Thrive': 'DLT_Other',\n",
    "         'Failure to thrive': 'DLT_Other',\n",
    "         'GIT [elevated liver enzymes]': 'DLT_Gastrointestinal',\n",
    "         'Gastrointestina': 'DLT_Gastrointestinal',\n",
    "         'Gastrointestinal': 'DLT_Gastrointestinal',\n",
    "         'General': 'DLT_Other',\n",
    "         'Hematological': 'DLT_Hematological',\n",
    "         'Hematological (Neutropenia)': 'DLT_Hematological',\n",
    "         'Hyponatremia': 'DLT_Other',\n",
    "         'Immunological': 'DLT_Other',\n",
    "         'Infection': 'DLT_Infection (Pneumonia)',\n",
    "         'NOS': 'DLT_Other',\n",
    "         'Nephrological': 'DLT_Nephrological',\n",
    "         'Nephrological (ARF)': 'DLT_Nephrological',\n",
    "         'Neurological': 'DLT_Neurological',\n",
    "         'Neutropenia': 'DLT_Hematological',\n",
    "         'Nutritional': 'DLT_Other',\n",
    "         'Pancreatitis': 'DLT_Other',\n",
    "         'Pulmonary': 'DLT_Other',\n",
    "         'Respiratory (Pneumonia)': 'DLT_Infection (Pneumonia)',\n",
    "         'Sepsis': 'DLT_Infection (Pneumonia)',\n",
    "         'Suboptimal response to treatment' : 'DLT_Other',\n",
    "         'Vascular': 'DLT_Vascular'\n",
    "    }\n",
    "    \n",
    "    decision1 = 'Decision 1 (Induction Chemo) Y/N'\n",
    "    decision2 = 'Decision 2 (CC / RT alone)'\n",
    "    decision3 = 'Decision 3 Neck Dissection (Y/N)'\n",
    "    decisions = [decision1,decision2, decision3]\n",
    "    outcomes = ['Overall Survival (4 Years)', 'FT', 'Aspiration rate Post-therapy']\n",
    "    \n",
    "    modification_types = {\n",
    "        0: 'no_dose_adjustment',\n",
    "        1: 'dose_modified',\n",
    "        2: 'dose_delayed',\n",
    "        3: 'dose_cancelled',\n",
    "        4: 'dose_delayed_&_modified',\n",
    "        5: 'regiment_modification',\n",
    "        9: 'unknown'\n",
    "    }\n",
    "    \n",
    "    cc_types = {\n",
    "        0: 'cc_none',\n",
    "        1: 'cc_platinum',\n",
    "        2: 'cc_cetuximab',\n",
    "        3: 'cc_others',\n",
    "    }\n",
    "    \n",
    "    primary_disease_states = ['CR Primary','PR Primary','SD Primary']\n",
    "    nodal_disease_states = [t.replace('Primary','Nodal') for t in primary_disease_states]\n",
    "    dlt1 = list(set(dlt_dict.values()))\n",
    "    \n",
    "    modifications =  list(modification_types.values())\n",
    "    state2 = modifications + primary_disease_states+nodal_disease_states +dlt1 #+['No imaging 0=N,1=Y']\n",
    "    \n",
    "    primary_disease_states2 = [t + ' 2' for t in primary_disease_states]\n",
    "    nodal_disease_states2 = [t + ' 2' for t in nodal_disease_states]\n",
    "    dlt2 = [d + ' 2' for d in dlt1]\n",
    "    \n",
    "    ccs = list(cc_types.values())\n",
    "    state3 = ccs + primary_disease_states2 + nodal_disease_states2 + dlt2\n",
    "    name_dict = {\n",
    "        'pd_state1': primary_disease_states,\n",
    "        'nd_state1': nodal_disease_states,\n",
    "        'chemo_state1': modifications,\n",
    "        'chemo_state2': ccs,\n",
    "        'pd_state2': primary_disease_states2,\n",
    "        'pd_state2': nodal_disease_states2,\n",
    "    }\n",
    "    \n",
    "    stratified_train_ids = [\n",
    "        5,6,8,11,13,14,15,16,17,18,21,23,24,26,27,28,32,33,37,38,39,40,\n",
    "        41,42,48,49,50,51,53,55,56,57,60,64,65,67,69,71,74,75,78,79,80,\n",
    "        81,82,87,88,91,94,96,99,103,109,116,119,120,121,125,148,150,\n",
    "        153,178,181,183,185,186,188,191,192,193,196,197,198,200,201,\n",
    "        203,204,205,206,207,210,212,213,214,216,218,219,220,221,222,\n",
    "        223,225,226,229,230,231,232,233,234,235,237,238,239,240,241,\n",
    "        243,244,246,247,248,249,251,252,253,255,256,257,258,259,260,\n",
    "        261,262,263,265,266,269,270,273,275,276,277,278,280,281,282,\n",
    "        283,285,289,2000,2002,2003,2004,2007,2008,2009,2010,2011,\n",
    "        2012,2013,2014,2016,2018,2021,2022,2023,2025,2027,2028,2030,\n",
    "        2033,5000,5002,5004,5005,5006,5008,5009,5010,5011,5012,5013,\n",
    "        5014,5015,5016,5017,5018,5019,5021,5022,5023,5024,5025,5026,\n",
    "        5027,5028,5029,5030,5031,5034,5037,5039,5041,5042,5043,5044,\n",
    "        5045,5047,5050,5051,5055,5057,5058,5059,5060,5061,5062,5063,\n",
    "        5064,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,\n",
    "        5079,5081,5083,5085,5087,5088,5089,5090,5091,5092,5094,5095,\n",
    "        5096,5097,5100,5102,5104,5106,5108,5110,5111,5112,5113,5114,\n",
    "        5119,10001,10002,10003,10004,10006,10008,10009,10011,10015,\n",
    "        10018,10019,10020,10021,10022,10024,10025,10027,10028,10029,\n",
    "        10031,10033,10034,10035,10036,10037,10038,10039,10041,10042,\n",
    "        10043,10044,10045,10047,10048,10051,10052,10053,10054,10055,\n",
    "        10056,10057,10059,10060,10061,10062,10064,10065,10067,10069,\n",
    "        10070,10071,10072,10073,10074,10075,10077,10078,10079,10080,\n",
    "        10081,10082,10083,10085,10087,10089,10090,10093,10095,10096,\n",
    "        10098,10099,10103,10107,10108,10109,10110,10111,10113,10114,\n",
    "        10115,10116,10117,10118,10119,10120,10121,10124,10127,10128,\n",
    "        10129,10132,10134,10136,10138,10139,10140,10141,10142,10143,\n",
    "        10144,10146,10147,10148,10149,10150,10151,10152,10154,10155,\n",
    "        10156,10157,10158,10159,10162,10163,10164,10167,10168,10171,\n",
    "        10173,10174,10175,10181,10182,10183,10184,10185,10186,10187,\n",
    "        10188,10189,10191,10192,10193,10194,10195,10196,10197,10198,\n",
    "        10199,10200,10201,10202,10203,10204,10205]\n",
    "    \n",
    "    stratified_test_ids = [\n",
    "        133,47,35,10,279,5056,5035,224,209,10063,2006,5020,271,10014,\n",
    "        5080,10097,10125,10106,2032,10169,2024,286,2015,2019,10026,\n",
    "        5040,236,187,10161,211,5103,10178,2026,10137,184,199,10040,\n",
    "        272,68,5105,10177,228,44,242,9,5101,10104,10165,10007,10133,\n",
    "        10145,10016,264,5098,10023,10050,5120,227,5118,2005,5053,10135,\n",
    "        5007,10092,36,2001,5115,10005,10102,189,5036,10088,254,10130,\n",
    "        10086,25,5001,5065,10084,195,5099,3,5093,10094,7,5038,10068,5032,\n",
    "        202,274,45,2017,10176,217,10160,5082,10012,10017,10100,2031,77,\n",
    "        10066,5078,117,10010,10170,10190,10058,5049,5086,5052,268,2029,\n",
    "        5084,10105,10013,245,5048,2020,215,10046,5117,5033,267,5003,168,\n",
    "        31,10049,10180,190,287,284,5054,10101,208,5077,10091,10172,288,5109,\n",
    "        10126,10153,10123,5107,194,10131]\n",
    "\n",
    "Const.stratified_test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02b309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_cleaned):\n",
    "    #this was Elisa's preprocessing except I removed all the Ifs because that's dumb\n",
    "    if len(data_cleaned.shape) < 2:\n",
    "        data_cleaned = pd.DataFrame([data], columns=data.index)\n",
    "        \n",
    "    data_cleaned.loc[data_cleaned['Aspiration rate Pre-therapy'] == 'N', 'Aspiration rate Pre-therapy'] = 0\n",
    "    data_cleaned.loc[data_cleaned['Aspiration rate Pre-therapy'] == 'Y', 'Aspiration rate Pre-therapy'] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'I', 'Pathological Grade'] = 1\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'II', 'Pathological Grade'] = 2\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'III', 'Pathological Grade'] = 3\n",
    "    data_cleaned.loc[data_cleaned['Pathological Grade'] == 'IV', 'Pathological Grade'] = 4\n",
    "\n",
    "    data_cleaned.loc[(data_cleaned['T-category'] == 'Tx') | (data_cleaned['T-category'] == 'Tis'), 'T-category'] = 1\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T1', 'T-category'] = 1\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T2', 'T-category'] = 2\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T3', 'T-category'] = 3\n",
    "    data_cleaned.loc[data_cleaned['T-category'] == 'T4', 'T-category'] = 4\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N0', 'N-category'] = 0\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N1', 'N-category'] = 1\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N2', 'N-category'] = 2\n",
    "    data_cleaned.loc[data_cleaned['N-category'] == 'N3', 'N-category'] = 3\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N0', 'N-category_8th_edition'] = 0\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N1', 'N-category_8th_edition'] = 1\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N2', 'N-category_8th_edition'] = 2\n",
    "    data_cleaned.loc[data_cleaned['N-category_8th_edition'] == 'N3', 'N-category_8th_edition'] = 3\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'I', 'AJCC 7th edition'] = 1\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'II', 'AJCC 7th edition'] = 2\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'III', 'AJCC 7th edition'] = 3\n",
    "    data_cleaned.loc[data_cleaned['AJCC 7th edition'] == 'IV', 'AJCC 7th edition'] = 4\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'I', 'AJCC 8th edition'] = 1\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'II', 'AJCC 8th edition'] = 2\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'III', 'AJCC 8th edition'] = 3\n",
    "    data_cleaned.loc[data_cleaned['AJCC 8th edition'] == 'IV', 'AJCC 8th edition'] = 4\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Gender'] == 'Male', 'Gender'] = 1\n",
    "    data_cleaned.loc[data_cleaned['Gender'] == 'Female', 'Gender'] = 0\n",
    "\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['HPV/P16 status'] == 'Positive', 'HPV/P16 status'] = 1\n",
    "    data_cleaned.loc[data_cleaned['HPV/P16 status'] == 'Negative', 'HPV/P16 status'] = -1\n",
    "    data_cleaned.loc[data_cleaned['HPV/P16 status'] == 'Unknown', 'HPV/P16 status'] = 0\n",
    "\n",
    "    data_cleaned.loc[data_cleaned[\n",
    "                         'Smoking status at Diagnosis (Never/Former/Current)'] == 'Formar', 'Smoking status at Diagnosis (Never/Former/Current)'] = .5\n",
    "    data_cleaned.loc[data_cleaned[\n",
    "                         'Smoking status at Diagnosis (Never/Former/Current)'] == 'Current', 'Smoking status at Diagnosis (Never/Former/Current)'] = 1\n",
    "    data_cleaned.loc[data_cleaned[\n",
    "                         'Smoking status at Diagnosis (Never/Former/Current)'] == 'Never', 'Smoking status at Diagnosis (Never/Former/Current)'] = 0\n",
    "\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Chemo Modification (Y/N)'] == 'Y', 'Chemo Modification (Y/N)'] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['DLT (Y/N)'] == 'N', 'DLT (Y/N)'] = 0\n",
    "    data_cleaned.loc[data_cleaned['DLT (Y/N)'] == 'Y', 'DLT (Y/N)'] = 1\n",
    "\n",
    "    data_cleaned['DLT_Other'] = 0\n",
    "    for index, row in data_cleaned.iterrows():\n",
    "        if row['DLT_Type'] == 'None':\n",
    "            continue\n",
    "        for i in re.split('&|and|,', row['DLT_Type']):\n",
    "            if i.strip() != '' and data_cleaned.loc[index, Const.dlt_dict[i.strip()]] == 0:\n",
    "                data_cleaned.loc[index, Const.dlt_dict[i.strip()]] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['Decision 2 (CC / RT alone)'] == 'RT alone', 'Decision 2 (CC / RT alone)'] = 0\n",
    "    data_cleaned.loc[data_cleaned['Decision 2 (CC / RT alone)'] == 'CC', 'Decision 2 (CC / RT alone)'] = 1\n",
    "\n",
    "    data_cleaned.loc[data_cleaned['CC modification (Y/N)'] == 'N', 'CC modification (Y/N)'] = 0\n",
    "    data_cleaned.loc[data_cleaned['CC modification (Y/N)'] == 'Y', 'CC modification (Y/N)'] = 1\n",
    "\n",
    "    data_cleaned['DLT_Dermatological 2'] = 0\n",
    "    data_cleaned['DLT_Neurological 2'] = 0\n",
    "    data_cleaned['DLT_Gastrointestinal 2'] = 0\n",
    "    data_cleaned['DLT_Hematological 2'] = 0\n",
    "    data_cleaned['DLT_Nephrological 2'] = 0\n",
    "    data_cleaned['DLT_Vascular 2'] = 0\n",
    "    data_cleaned['DLT_Infection (Pneumonia) 2'] = 0\n",
    "    data_cleaned['DLT_Other 2'] = 0\n",
    "    for index, row in data_cleaned.iterrows():\n",
    "        if row['DLT 2'] == 'None':\n",
    "            continue\n",
    "        for i in re.split('&|and|,', row['DLT 2']):\n",
    "            if i.strip() != '':\n",
    "                data_cleaned.loc[index, Const.dlt_dict[i.strip()] + ' 2'] = 1\n",
    "\n",
    "    data_cleaned.loc[\n",
    "        data_cleaned['Decision 3 Neck Dissection (Y/N)'] == 'N', 'Decision 3 Neck Dissection (Y/N)'] = 0\n",
    "    data_cleaned.loc[\n",
    "        data_cleaned['Decision 3 Neck Dissection (Y/N)'] == 'Y', 'Decision 3 Neck Dissection (Y/N)'] = 1\n",
    "\n",
    "    return data_cleaned\n",
    "\n",
    "def merge_editions(row,basecol='AJCC 8th edition',fallback='AJCC 7th edition'):\n",
    "    if pd.isnull(row[basecol]):\n",
    "        return row[fallback]\n",
    "    return row[basecol]\n",
    "\n",
    "\n",
    "def preprocess_dt_data(df,extra_to_keep=None):\n",
    "    \n",
    "    to_keep = ['id','hpv','age','packs_per_year','smoking_status','gender','Aspiration rate Pre-therapy','total_dose','dose_fraction'] \n",
    "    to_onehot = ['T-category','N-category','AJCC','Pathological Grade','subsite','treatment','ln_cluster']\n",
    "    if extra_to_keep is not None:\n",
    "        to_keep = to_keep + [c for c in extra_to_keep if c not in to_keep and c not in to_onehot]\n",
    "    \n",
    "    df['bilateral'] = df['laterality'].apply(lambda x: x.lower().strip() == 'bilateral')\n",
    "    to_keep.append('bilateral')\n",
    "    \n",
    "    decisions =Const.decisions\n",
    "    outcomes = Const.outcomes\n",
    "    \n",
    "    modification_types = {\n",
    "        0: 'no_dose_adjustment',\n",
    "        1: 'dose_modified',\n",
    "        2: 'dose_delayed',\n",
    "        3: 'dose_cancelled',\n",
    "        4: 'dose_delayed_&_modified',\n",
    "        5: 'regiment_modification',\n",
    "        9: 'unknown'\n",
    "    }\n",
    "    \n",
    "    cc_types = {\n",
    "        0: 'cc_none',\n",
    "        1: 'cc_platinum',\n",
    "        2: 'cc_cetuximab',\n",
    "        3: 'cc_others',\n",
    "    }\n",
    "    \n",
    "#     races_shortened = ['White/Caucasian','Hispanic/Latino','African American/Black']\n",
    "#     for race in races_shortened:\n",
    "#         df[race] = df['Race'].apply(lambda x: x.strip() == race)\n",
    "#         to_keep.append(race)\n",
    "    \n",
    "    for k,v in Const.cc_types.items():\n",
    "        df[v] = df['CC Regimen(0= none, 1= platinum based, 2= cetuximab based, 3= others, 9=unknown)'].apply(lambda x: int(Const.cc_types.get(int(x),0) == v))\n",
    "        to_keep.append(v)\n",
    "    for k,v in Const.modification_types.items():\n",
    "        name = 'Modification Type (0= no dose adjustment, 1=dose modified, 2=dose delayed, 3=dose cancelled, 4=dose delayed & modified, 5=regimen modification, 9=unknown)'\n",
    "        df[v] = df[name].apply(lambda x: int(Const.modification_types.get(int(x),0) == v))\n",
    "        to_keep.append(v)\n",
    "    #Features to keep. I think gender is is in \n",
    "    \n",
    "    for col in Const.dlt1 + Const.dlt2:\n",
    "        df[col] = (df[col] > 0).astype(float)\n",
    "    keywords = []\n",
    "    for keyword in keywords:\n",
    "        toadd = [c for c in df.columns if keyword in c and c not in to_keep]\n",
    "        to_keep = to_keep + toadd\n",
    "    \n",
    "    df['packs_per_year'] = df['packs_per_year'].apply(lambda x: str(x).replace('>','').replace('<','')).astype(float).fillna(0)\n",
    "    #so I'm actually not sure if this is biological sex or gender given this is texas\n",
    "    df['AJCC'] = df.apply(lambda row: merge_editions(row,'ajcc8','ajcc7'),axis=1)\n",
    "    df['N-category'] = df.apply(lambda row: merge_editions(row,'N-category_8th_edition','N-category'),axis=1)\n",
    "    \n",
    "    dummy_df = pd.get_dummies(df[to_onehot].fillna(0).astype(str),drop_first=False)\n",
    "    for col in dummy_df.columns:\n",
    "        df[col] = dummy_df[col]\n",
    "        to_keep.append(col)\n",
    "        \n",
    "    yn_to_binary = ['FT','Aspiration rate Post-therapy','Decision 1 (Induction Chemo) Y/N']\n",
    "    for col in yn_to_binary:\n",
    "        df[col] = df[col].apply(lambda x: int(x == 'Y'))\n",
    "        \n",
    "    to_keep = to_keep + [c for c in df.columns if 'DLT' in c]\n",
    "    \n",
    "    for statelist in [Const.state2,Const.state3,Const.decisions,Const.outcomes]:\n",
    "        toadd = [c for c in statelist if c not in to_keep]\n",
    "        to_keep = to_keep + toadd\n",
    "    return df[to_keep].set_index('id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c861bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([5,\n",
       "  6,\n",
       "  8,\n",
       "  11,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  21,\n",
       "  23,\n",
       "  24,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  32,\n",
       "  33,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  53,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  60,\n",
       "  64,\n",
       "  65,\n",
       "  67,\n",
       "  69,\n",
       "  71,\n",
       "  74,\n",
       "  75,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  87,\n",
       "  88,\n",
       "  91,\n",
       "  94,\n",
       "  96,\n",
       "  99,\n",
       "  103,\n",
       "  109,\n",
       "  116,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  125,\n",
       "  148,\n",
       "  150,\n",
       "  153,\n",
       "  178,\n",
       "  181,\n",
       "  183,\n",
       "  185,\n",
       "  186,\n",
       "  188,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  200,\n",
       "  201,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  210,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  216,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  225,\n",
       "  226,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  243,\n",
       "  244,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  265,\n",
       "  266,\n",
       "  269,\n",
       "  270,\n",
       "  273,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  285,\n",
       "  289,\n",
       "  2000,\n",
       "  2002,\n",
       "  2003,\n",
       "  2004,\n",
       "  2007,\n",
       "  2008,\n",
       "  2009,\n",
       "  2010,\n",
       "  2011,\n",
       "  2012,\n",
       "  2013,\n",
       "  2014,\n",
       "  2016,\n",
       "  2018,\n",
       "  2021,\n",
       "  2022,\n",
       "  2023,\n",
       "  2025,\n",
       "  2027,\n",
       "  2028,\n",
       "  2030,\n",
       "  2033,\n",
       "  5000,\n",
       "  5002,\n",
       "  5004,\n",
       "  5005,\n",
       "  5006,\n",
       "  5008,\n",
       "  5009,\n",
       "  5010,\n",
       "  5011,\n",
       "  5012,\n",
       "  5013,\n",
       "  5014,\n",
       "  5015,\n",
       "  5016,\n",
       "  5017,\n",
       "  5018,\n",
       "  5019,\n",
       "  5021,\n",
       "  5022,\n",
       "  5023,\n",
       "  5024,\n",
       "  5025,\n",
       "  5026,\n",
       "  5027,\n",
       "  5028,\n",
       "  5029,\n",
       "  5030,\n",
       "  5031,\n",
       "  5034,\n",
       "  5037,\n",
       "  5039,\n",
       "  5041,\n",
       "  5042,\n",
       "  5043,\n",
       "  5044,\n",
       "  5045,\n",
       "  5047,\n",
       "  5050,\n",
       "  5051,\n",
       "  5055,\n",
       "  5057,\n",
       "  5058,\n",
       "  5059,\n",
       "  5060,\n",
       "  5061,\n",
       "  5062,\n",
       "  5063,\n",
       "  5064,\n",
       "  5066,\n",
       "  5067,\n",
       "  5068,\n",
       "  5069,\n",
       "  5070,\n",
       "  5071,\n",
       "  5072,\n",
       "  5073,\n",
       "  5074,\n",
       "  5075,\n",
       "  5076,\n",
       "  5079,\n",
       "  5081,\n",
       "  5083,\n",
       "  5085,\n",
       "  5087,\n",
       "  5088,\n",
       "  5089,\n",
       "  5090,\n",
       "  5091,\n",
       "  5092,\n",
       "  5094,\n",
       "  5095,\n",
       "  5096,\n",
       "  5097,\n",
       "  5100,\n",
       "  5102,\n",
       "  5104,\n",
       "  5106,\n",
       "  5108,\n",
       "  5110,\n",
       "  5111,\n",
       "  5112,\n",
       "  5113,\n",
       "  5114,\n",
       "  5119,\n",
       "  10001,\n",
       "  10002,\n",
       "  10003,\n",
       "  10004,\n",
       "  10006,\n",
       "  10008,\n",
       "  10009,\n",
       "  10011,\n",
       "  10015,\n",
       "  10018,\n",
       "  10019,\n",
       "  10020,\n",
       "  10021,\n",
       "  10022,\n",
       "  10024,\n",
       "  10025,\n",
       "  10027,\n",
       "  10028,\n",
       "  10029,\n",
       "  10031,\n",
       "  10033,\n",
       "  10034,\n",
       "  10035,\n",
       "  10036,\n",
       "  10037,\n",
       "  10038,\n",
       "  10039,\n",
       "  10041,\n",
       "  10042,\n",
       "  10043,\n",
       "  10044,\n",
       "  10045,\n",
       "  10047,\n",
       "  10048,\n",
       "  10051,\n",
       "  10052,\n",
       "  10053,\n",
       "  10054,\n",
       "  10055,\n",
       "  10056,\n",
       "  10057,\n",
       "  10059,\n",
       "  10060,\n",
       "  10061,\n",
       "  10062,\n",
       "  10064,\n",
       "  10065,\n",
       "  10067,\n",
       "  10069,\n",
       "  10070,\n",
       "  10071,\n",
       "  10072,\n",
       "  10073,\n",
       "  10074,\n",
       "  10075,\n",
       "  10077,\n",
       "  10078,\n",
       "  10079,\n",
       "  10080,\n",
       "  10081,\n",
       "  10082,\n",
       "  10083,\n",
       "  10085,\n",
       "  10087,\n",
       "  10089,\n",
       "  10090,\n",
       "  10093,\n",
       "  10095,\n",
       "  10096,\n",
       "  10098,\n",
       "  10099,\n",
       "  10103,\n",
       "  10107,\n",
       "  10108,\n",
       "  10109,\n",
       "  10110,\n",
       "  10111,\n",
       "  10113,\n",
       "  10114,\n",
       "  10115,\n",
       "  10116,\n",
       "  10117,\n",
       "  10118,\n",
       "  10119,\n",
       "  10120,\n",
       "  10121,\n",
       "  10124,\n",
       "  10127,\n",
       "  10128,\n",
       "  10129,\n",
       "  10132,\n",
       "  10134,\n",
       "  10136,\n",
       "  10138,\n",
       "  10139,\n",
       "  10140,\n",
       "  10141,\n",
       "  10142,\n",
       "  10143,\n",
       "  10144,\n",
       "  10146,\n",
       "  10147,\n",
       "  10148,\n",
       "  10149,\n",
       "  10150,\n",
       "  10151,\n",
       "  10152,\n",
       "  10154,\n",
       "  10155,\n",
       "  10156,\n",
       "  10157,\n",
       "  10158,\n",
       "  10159,\n",
       "  10162,\n",
       "  10163,\n",
       "  10164,\n",
       "  10167,\n",
       "  10168,\n",
       "  10171,\n",
       "  10173,\n",
       "  10174,\n",
       "  10175,\n",
       "  10181,\n",
       "  10182,\n",
       "  10183,\n",
       "  10184,\n",
       "  10185,\n",
       "  10186,\n",
       "  10187,\n",
       "  10188,\n",
       "  10189,\n",
       "  10191,\n",
       "  10192,\n",
       "  10193,\n",
       "  10194,\n",
       "  10195,\n",
       "  10196,\n",
       "  10197,\n",
       "  10198,\n",
       "  10199,\n",
       "  10200,\n",
       "  10201,\n",
       "  10202,\n",
       "  10203,\n",
       "  10204,\n",
       "  10205],\n",
       " [133,\n",
       "  47,\n",
       "  35,\n",
       "  10,\n",
       "  279,\n",
       "  5056,\n",
       "  5035,\n",
       "  224,\n",
       "  209,\n",
       "  10063,\n",
       "  2006,\n",
       "  5020,\n",
       "  271,\n",
       "  10014,\n",
       "  5080,\n",
       "  10097,\n",
       "  10125,\n",
       "  10106,\n",
       "  2032,\n",
       "  10169,\n",
       "  2024,\n",
       "  286,\n",
       "  2015,\n",
       "  2019,\n",
       "  10026,\n",
       "  5040,\n",
       "  236,\n",
       "  187,\n",
       "  10161,\n",
       "  211,\n",
       "  5103,\n",
       "  10178,\n",
       "  2026,\n",
       "  10137,\n",
       "  184,\n",
       "  199,\n",
       "  10040,\n",
       "  272,\n",
       "  68,\n",
       "  5105,\n",
       "  10177,\n",
       "  228,\n",
       "  44,\n",
       "  242,\n",
       "  9,\n",
       "  5101,\n",
       "  10104,\n",
       "  10165,\n",
       "  10007,\n",
       "  10133,\n",
       "  10145,\n",
       "  10016,\n",
       "  264,\n",
       "  5098,\n",
       "  10023,\n",
       "  10050,\n",
       "  5120,\n",
       "  227,\n",
       "  5118,\n",
       "  2005,\n",
       "  5053,\n",
       "  10135,\n",
       "  5007,\n",
       "  10092,\n",
       "  36,\n",
       "  2001,\n",
       "  5115,\n",
       "  10005,\n",
       "  10102,\n",
       "  189,\n",
       "  5036,\n",
       "  10088,\n",
       "  254,\n",
       "  10130,\n",
       "  10086,\n",
       "  25,\n",
       "  5001,\n",
       "  5065,\n",
       "  10084,\n",
       "  195,\n",
       "  5099,\n",
       "  3,\n",
       "  5093,\n",
       "  10094,\n",
       "  7,\n",
       "  5038,\n",
       "  10068,\n",
       "  5032,\n",
       "  202,\n",
       "  274,\n",
       "  45,\n",
       "  2017,\n",
       "  10176,\n",
       "  217,\n",
       "  10160,\n",
       "  5082,\n",
       "  10012,\n",
       "  10017,\n",
       "  10100,\n",
       "  2031,\n",
       "  77,\n",
       "  10066,\n",
       "  5078,\n",
       "  117,\n",
       "  10010,\n",
       "  10170,\n",
       "  10190,\n",
       "  10058,\n",
       "  5049,\n",
       "  5086,\n",
       "  5052,\n",
       "  268,\n",
       "  2029,\n",
       "  5084,\n",
       "  10105,\n",
       "  10013,\n",
       "  245,\n",
       "  5048,\n",
       "  2020,\n",
       "  215,\n",
       "  10046,\n",
       "  5117,\n",
       "  5033,\n",
       "  267,\n",
       "  5003,\n",
       "  168,\n",
       "  31,\n",
       "  10049,\n",
       "  10180,\n",
       "  190,\n",
       "  287,\n",
       "  284,\n",
       "  5054,\n",
       "  10101,\n",
       "  208,\n",
       "  5077,\n",
       "  10091,\n",
       "  10172,\n",
       "  288,\n",
       "  5109,\n",
       "  10126,\n",
       "  10153,\n",
       "  10123,\n",
       "  5107,\n",
       "  194,\n",
       "  10131])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_digital_twin(file='../data/digital_twin_data.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    return df.rename(columns = Const.rename_dict)\n",
    "\n",
    "def get_dt_ids():\n",
    "    df = load_digital_twin()\n",
    "    return df.id.values\n",
    "\n",
    "def get_tt_split(ids=None,use_default_split=True,use_bagging_split=False,resample_training=False):\n",
    "        if ids is None:\n",
    "            ids = get_dt_ids()\n",
    "        #pre-made, stratified by decision and outcome 72:28\n",
    "        if use_default_split:\n",
    "            train_ids = Const.stratified_train_ids[:]\n",
    "            test_ids = Const.stratified_test_ids[:]\n",
    "        elif use_bagging_split:\n",
    "            train_ids = np.random.choice(ids,len(ids),replace=True)\n",
    "            test_ids = [i for i in ids if i not in train_ids]\n",
    "        else:\n",
    "            test_ids = ids[0: int(len(ids)*(1-split))]\n",
    "            train_ids = [i for i in ids if i not in test_ids]\n",
    "\n",
    "        if resample_training:\n",
    "            train_ids = np.random.choice(train_ids,len(train_ids),replace=True)\n",
    "            test_ids = [i for i in ids if i not in train_ids]\n",
    "        return train_ids,test_ids\n",
    "    \n",
    "get_tt_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e719a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 0.7276119402985075\n"
     ]
    }
   ],
   "source": [
    "class DTDataset():\n",
    "    \n",
    "    def __init__(self,data_file = '../data/digital_twin_data.csv',ln_data_file = '../data/digital_twin_ln_data.csv',ids=None):\n",
    "        df = pd.read_csv(data_file)\n",
    "        df = preprocess(df)\n",
    "        df = df.rename(columns = Const.rename_dict).copy()\n",
    "        df = df.drop('MRN OPC',axis=1)\n",
    "\n",
    "        ln_data = pd.read_csv(ln_data_file)\n",
    "        ln_data = ln_data.rename(columns={'cluster':'ln_cluster'})\n",
    "        self.ln_cols = [c for c in ln_data.columns if c not in df.columns]\n",
    "        df = df.merge(ln_data,on='id')\n",
    "        df.index = df.index.astype(int)\n",
    "        if ids is not None:\n",
    "            df = df[df.id.apply(lambda x: x in ids)]\n",
    "        self.processed_df = preprocess_dt_data(df,self.ln_cols).fillna(0)\n",
    "        \n",
    "        self.means = self.processed_df.mean(axis=0)\n",
    "        self.stds = self.processed_df.std(axis=0)\n",
    "        self.maxes = self.processed_df.max(axis=0)\n",
    "        self.mins = self.processed_df.min(axis=0)\n",
    "        \n",
    "        arrays = self.get_states()\n",
    "        self.state_sizes = {k: (v.shape[1] if v.ndim > 1 else 1) for k,v in arrays.items()}\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.processed_df\n",
    "    \n",
    "    def sample(self,frac=.5):\n",
    "        return self.processed_df.sample(frac=frac)\n",
    "    \n",
    "    def split_sample(self,ratio = .3):\n",
    "        assert(ratio > 0 and ratio <= 1)\n",
    "        df1 = self.processed_df.sample(frac=1-ratio)\n",
    "        df2 = self.processed_df.drop(index=df1.index)\n",
    "        return df1,df2\n",
    "    \n",
    "    def get_states(self,fixed=None,ids = None):\n",
    "        processed_df = self.processed_df.copy()\n",
    "        if ids is not None:\n",
    "            processed_df = processed_df.loc[ids]\n",
    "        if fixed is not None:\n",
    "            for col,val in fixed.items():\n",
    "                if col in processed_df.columns:\n",
    "                    processed_df[col] = val\n",
    "                else:\n",
    "                    print('bad fixed entry',col)\n",
    "                    \n",
    "        to_skip = ['CC Regimen(0= none, 1= platinum based, 2= cetuximab based, 3= others, 9=unknown)','DLT_Type','DLT 2'] + [c for c in processed_df.columns if 'treatment' in c]\n",
    "        other_states = set(Const.decisions + Const.state3 + Const.state2 + Const.outcomes  + to_skip)\n",
    "\n",
    "        base_state = sorted([c for c in processed_df.columns if c not in other_states])\n",
    "\n",
    "        dlt1 = Const.dlt1\n",
    "        dlt2 = Const.dlt2\n",
    "        \n",
    "        modifications = Const.modifications\n",
    "        ccs = Const.ccs\n",
    "        pds = Const.primary_disease_states\n",
    "        nds = Const.nodal_disease_states\n",
    "        pds2 = Const.primary_disease_states2\n",
    "        nds2 = Const.nodal_disease_states2\n",
    "        outcomes = Const.outcomes\n",
    "        decisions= Const.decisions\n",
    "        \n",
    "        #intermediate states are only udated values. Models should use baseline + state2 etc\n",
    "        results = {\n",
    "            'baseline': processed_df[base_state],\n",
    "            'pd_states1': processed_df[pds],\n",
    "            'nd_states1': processed_df[nds],\n",
    "            'modifications': processed_df[modifications],\n",
    "            'ccs': processed_df[ccs],\n",
    "            'pd_states2': processed_df[pds2],\n",
    "            'nd_states2': processed_df[nds2],\n",
    "            'outcomes': processed_df[outcomes],\n",
    "            'dlt1': processed_df[dlt1],\n",
    "            'dlt2': processed_df[dlt2],\n",
    "            'decision1': processed_df[decisions[0]],\n",
    "            'decision2': processed_df[decisions[1]],\n",
    "            'decision3': processed_df[decisions[2]],\n",
    "            'survival': processed_df[outcomes[0]],\n",
    "            'ft': processed_df[outcomes[1]],\n",
    "            'aspiration': processed_df[outcomes[2]],\n",
    "        }\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    def get_state(self,name,**kwargs):\n",
    "        return self.get_states(**kwargs)[name]\n",
    "    \n",
    "    def normalize(self,df):\n",
    "        means = self.means[df.columns]\n",
    "        std = self.stds[df.columns]\n",
    "        return ((df - means)/std).fillna(0)\n",
    "    \n",
    "    def get_intermediate_outcomes(self,step=1,**kwargs):\n",
    "        assert(step in [0,1,2,3])\n",
    "        states = self.get_states(**kwargs)\n",
    "        if step == 1:\n",
    "            keys = ['pd_states1','nd_states1','modifications','dlt1']\n",
    "        if step == 2:\n",
    "            keys =  ['pd_states2','nd_states2','ccs','dlt2']\n",
    "        if step == 3:\n",
    "            keys = ['survival','ft','aspiration']# ['decision1','decision2','decision3']\n",
    "        if step == 0:\n",
    "            keys = ['decision1','decision2','decision3']\n",
    "        if len(keys) < 2:\n",
    "            return states[keys[0]]\n",
    "        return [states[key] for key in keys]\n",
    "    \n",
    "    def get_input_state(self,step=1,**kwargs):\n",
    "        assert(step in [0,1,2,3])\n",
    "        states = self.get_states(**kwargs)\n",
    "        if step == 0:\n",
    "            keys = ['baseline']\n",
    "        if step == 1:\n",
    "            keys = ['baseline','decision1']\n",
    "        if step == 2:\n",
    "            keys =  ['baseline','pd_states1','nd_states1','modifications','dlt1','decision1','decision2']\n",
    "        if step == 3:\n",
    "            keys = ['baseline','pd_states2','nd_states2','ccs','dlt2','decision1','decision2','decision3']\n",
    "        arrays = [states[key] for key in keys]\n",
    "        if len(arrays) < 2:\n",
    "            return arrays[0]\n",
    "        return pd.concat(arrays,axis=1)\n",
    "    \n",
    "# data = DTDataset()\n",
    "test_ids = []\n",
    "outcomes = data.processed_df[Const.outcomes+Const.decisions]\n",
    "ids = outcomes.index.values\n",
    "for outcome in Const.outcomes+Const.decisions:\n",
    "    for res in [0,1]:\n",
    "        sample = outcomes[outcomes[outcome].astype(int) == res].sample(frac = .05).reset_index()\n",
    "        toadd = [i for i in sample.id.values if i not in test_ids]\n",
    "        test_ids.extend(toadd)\n",
    "train_ids = [i for i in ids if i not in test_ids]\n",
    "print(len(test_ids),len(train_ids)/len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7789e928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overall Survival (4 Years)          0.890411\n",
       "FT                                  0.219178\n",
       "Aspiration rate Post-therapy        0.178082\n",
       "Decision 1 (Induction Chemo) Y/N    0.376712\n",
       "Decision 2 (CC / RT alone)          0.808219\n",
       "Decision 3 Neck Dissection (Y/N)    0.253425\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes.loc[test_ids].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d8d7701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overall Survival (4 Years)          0.838462\n",
       "FT                                  0.169231\n",
       "Aspiration rate Post-therapy        0.184615\n",
       "Decision 1 (Induction Chemo) Y/N    0.356410\n",
       "Decision 2 (CC / RT alone)          0.748718\n",
       "Decision 3 Neck Dissection (Y/N)    0.189744\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes.loc[train_ids].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0933c94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'133,47,35,10,279,5056,5035,224,209,10063,2006,5020,271,10014,5080,10097,10125,10106,2032,10169,2024,286,2015,2019,10026,5040,236,187,10161,211,5103,10178,2026,10137,184,199,10040,272,68,5105,10177,228,44,242,9,5101,10104,10165,10007,10133,10145,10016,264,5098,10023,10050,5120,227,5118,2005,5053,10135,5007,10092,36,2001,5115,10005,10102,189,5036,10088,254,10130,10086,25,5001,5065,10084,195,5099,3,5093,10094,7,5038,10068,5032,202,274,45,2017,10176,217,10160,5082,10012,10017,10100,2031,77,10066,5078,117,10010,10170,10190,10058,5049,5086,5052,268,2029,5084,10105,10013,245,5048,2020,215,10046,5117,5033,267,5003,168,31,10049,10180,190,287,284,5054,10101,208,5077,10091,10172,288,5109,10126,10153,10123,5107,194,10131'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join([str(i) for i in test_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c748333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_torch(df,ttype  = torch.FloatTensor):\n",
    "    values = df.values.astype(float)\n",
    "    values = torch.from_numpy(values)\n",
    "    return values.type(ttype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87a4d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimulatorBase(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_layers = [1000],\n",
    "                 dropout = 0.5,\n",
    "                 input_dropout=0.1,\n",
    "                 state = 1,\n",
    "                 eps = 0.01,\n",
    "                ):\n",
    "        #predicts disease state (sd, pr, cr) for primar and nodal, then dose modications or cc type (depending on state), and [dlt ratings]\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.state = state\n",
    "        self.input_dropout = torch.nn.Dropout(input_dropout)\n",
    "        \n",
    "        first_layer =torch.nn.Linear(input_size,hidden_layers[0],bias=True)\n",
    "        layers = [first_layer,torch.nn.ReLU()]\n",
    "        curr_size = hidden_layers[0]\n",
    "        for ndim in hidden_layers[1:]:\n",
    "            layer = torch.nn.Linear(curr_size,ndim)\n",
    "            curr_size = ndim\n",
    "            layers.append(layer)\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(hidden_layers[-1])\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "        input_mean = torch.tensor([0])\n",
    "        input_std = torch.tensor([1])\n",
    "        self.eps = eps\n",
    "        self.register_buffer('input_mean', input_mean)\n",
    "        self.register_buffer('input_std',input_std)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.identifier = 'state'  +str(state) + '_input'+str(input_size) + '_dims' + ','.join([str(h) for h in hidden_layers]) + '_dropout' + str(input_dropout) + ',' + str(dropout)\n",
    "        \n",
    "    def normalize(self,x):\n",
    "        x = (x - self.input_mean + self.eps)/(self.input_std + self.eps)\n",
    "        return x\n",
    "    \n",
    "    def fit_normalizer(self,x):\n",
    "        input_mean = x.mean(axis=0)\n",
    "        input_std = x.std(axis=0)\n",
    "        self.register_buffer('input_mean', input_mean)\n",
    "        self.register_buffer('input_std',input_std)\n",
    "        return True\n",
    "        \n",
    "class OutcomeSimulator(SimulatorBase):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_layers = [500,500],\n",
    "                 dropout = 0.7,\n",
    "                 input_dropout=0.1,\n",
    "                 state = 1,\n",
    "                ):\n",
    "        #predicts disease state (sd, pr, cr) for primar and nodal, then dose modications or cc type (depending on state), and [dlt ratings]\n",
    "        super(OutcomeSimulator,self).__init__(input_size,hidden_layers=hidden_layers,dropout=dropout,input_dropout=input_dropout,state=state)\n",
    "    \n",
    "        self.disease_layer = torch.nn.Linear(hidden_layers[-1],len(Const.primary_disease_states))\n",
    "        self.nodal_disease_layer = torch.nn.Linear(hidden_layers[-1],len(Const.nodal_disease_states))\n",
    "        #dlt ratings are 0-4 even though they don't always appear\n",
    "        \n",
    "        self.dlt_layers = torch.nn.ModuleList([torch.nn.Linear(hidden_layers[-1],1) for i in Const.dlt1])\n",
    "        assert( state in [1,2])\n",
    "        if state == 1:\n",
    "#             self.dlt_layers = torch.nn.ModuleList([torch.nn.Linear(hidden_layers[-1],5) for i in Const.dlt1])\n",
    "            self.treatment_layer = torch.nn.Linear(hidden_layers[-1],len(Const.modifications))\n",
    "        else:\n",
    "            #we only have dlt yes or no for the second state?\n",
    "#             self.dlt_layers = torch.nn.ModuleList([torch.nn.Linear(hidden_layers[-1],2) for i in Const.dlt2])\n",
    "            self.treatment_layer = torch.nn.Linear(hidden_layers[-1],len(Const.ccs))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.normalize(x)\n",
    "        x = self.input_dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "#         x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pd = self.disease_layer(x)\n",
    "        x_nd = self.nodal_disease_layer(x)\n",
    "        x_mod = self.treatment_layer(x)\n",
    "        x_dlts = [layer(x) for layer in self.dlt_layers]\n",
    "        \n",
    "        x_pd = self.softmax(x_pd)\n",
    "        x_nd = self.softmax(x_nd)\n",
    "        x_mod = self.softmax(x_mod)\n",
    "        #dlts are array of nbatch x n_dlts x predictions\n",
    "#         x_dlts = torch.stack([self.sigmoid(xx) for xx in x_dlts],axis=1)\n",
    "        x_dlts = torch.cat([self.sigmoid(xx) for xx in x_dlts],axis=1)\n",
    "        return [x_pd, x_nd, x_mod, x_dlts]\n",
    "    \n",
    "class EndpointSimulator(SimulatorBase):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_layers = [500],\n",
    "                 dropout = 0.7,\n",
    "                 input_dropout=0.1,\n",
    "                 state = 1,\n",
    "                ):\n",
    "        #predicts disease state (sd, pr, cr) for primar and nodal, then dose modications or cc type (depending on state), and [dlt ratings]\n",
    "        super(EndpointSimulator,self).__init__(input_size,hidden_layers=hidden_layers,dropout=dropout,input_dropout=input_dropout,state=state)\n",
    "        \n",
    "        self.outcome_layer = torch.nn.Linear(hidden_layers[-1],len(Const.outcomes))\n",
    "      \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.normalize(x)\n",
    "        x = self.input_dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "#         x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x= self.outcome_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "OutcomeSimulator(3).input_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef50362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nllloss(ytrue,ypred):\n",
    "    #nll loss with argmax added in\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    return loss(ypred,ytrue.argmax(axis=1))\n",
    "\n",
    "def state_loss(ytrue,ypred,weights=[1,1,1,1]):\n",
    "    pd_loss = nllloss(ytrue[0],ypred[0])*weights[0]\n",
    "    nd_loss = nllloss(ytrue[1],ypred[1])*weights[1]\n",
    "    mod_loss = nllloss(ytrue[2],ypred[2])*weights[2]\n",
    "    loss = pd_loss + nd_loss + mod_loss\n",
    "    dlt_true = ytrue[3]\n",
    "    dlt_pred = ypred[3]\n",
    "    ndlt = dlt_true.shape[1]\n",
    "#     nloss = torch.nn.NLLLoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    for i in range(ndlt):\n",
    "        dlt_loss = bce(dlt_pred[:,i].view(-1),dlt_true[:,i].view(-1))\n",
    "        loss += dlt_loss*weights[3]/ndlt\n",
    "    return loss\n",
    "\n",
    "def outcome_loss(ytrue,ypred,weights=[1,1,1]):\n",
    "    loss = 0\n",
    "    nloss = torch.nn.BCELoss()\n",
    "    for i in range(len(weights)):\n",
    "        iloss = nloss(ypred[:,i],ytrue[i])*weights[i]\n",
    "        loss += iloss\n",
    "    return loss\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score\n",
    "\n",
    "def mc_metrics(yt,yp,numpy=False,is_dlt=False):\n",
    "    if not numpy:\n",
    "        yt = yt .cpu().detach().numpy()\n",
    "        yp = yp.cpu().detach().numpy()\n",
    "    #dlt prediction (binary)\n",
    "    if is_dlt:\n",
    "        acc = accuracy_score(yt,yp>.5)\n",
    "        if yt.sum() > 1:\n",
    "            auc = roc_auc_score(yt,yp)\n",
    "        else:\n",
    "            auc=-1\n",
    "        error = np.mean((yt-yp)**2)\n",
    "        return {'accuracy': acc, 'mse': error, 'auc': auc}\n",
    "    #this is a catch for when I se the dlt prediction format (encoded integer ordinal, predict as a categorical and take the argmax)\n",
    "    elif yt.ndim > 1:\n",
    "        try:\n",
    "            bacc = balanced_accuracy_score(yt.argmax(axis=1),yp.argmax(axis=1))\n",
    "        except:\n",
    "            bacc = -1\n",
    "        try:\n",
    "            roc_micro = roc_auc_score(yt,yp,average='micro')\n",
    "        except:\n",
    "            roc_micro=-1\n",
    "        try:\n",
    "            roc_macro = roc_auc_score(yt,yp,average='macro')\n",
    "        except:\n",
    "            roc_macro = -1\n",
    "        return {'accuracy': bacc, 'roc_micro': roc_micro,'roc_macro': roc_macro}\n",
    "    #outcomes (binary)\n",
    "    else:\n",
    "        if yp.ndim > 1:\n",
    "            yp = yp.argmax(axis=1)\n",
    "        try:\n",
    "            bacc = accuracy_score(yt,yp)\n",
    "        except:\n",
    "            bacc = -1\n",
    "        try:\n",
    "            roc = roc_auc_score(yt,yp)\n",
    "        except:\n",
    "            roc = -1\n",
    "        error = np.mean((yt-yp)**2)\n",
    "        return {'accuracy': bacc, 'mse': error, 'auc': roc}\n",
    "\n",
    "def state_metrics(ytrue,ypred,numpy=False):\n",
    "    pd_metrics = mc_metrics(ytrue[0],ypred[0],numpy=numpy)\n",
    "    nd_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    mod_metrics = mc_metrics(ytrue[1],ypred[1],numpy=numpy)\n",
    "    \n",
    "    dlt_metrics = []\n",
    "    dlt_true = ytrue[3]\n",
    "    dlt_pred = ypred[3]\n",
    "    ndlt = dlt_true.shape[1]\n",
    "    nloss = torch.nn.NLLLoss()\n",
    "    for i in range(ndlt):\n",
    "        dm = mc_metrics(dlt_true[:,i],dlt_pred[:,i].view(-1),is_dlt=True)\n",
    "        dlt_metrics.append(dm)\n",
    "    dlt_acc =[d['accuracy'] for d in dlt_metrics]\n",
    "    dlt_error = [d['mse'] for d in dlt_metrics]\n",
    "    dlt_auc = [d['auc'] for d in dlt_metrics]\n",
    "    return {'pd': pd_metrics,'nd': nd_metrics,'mod': mod_metrics,'dlts': {'accuracy': dlt_acc,'accuracy_mean': np.mean(dlt_acc),'auc': dlt_auc,'auc_mean': np.mean(dlt_auc)}}\n",
    "    \n",
    "def outcome_metrics(ytrue,ypred,numpy=False):\n",
    "    res = {}\n",
    "    for i, outcome in enumerate(Const.outcomes):\n",
    "        metrics = mc_metrics(ytrue[i],ypred[:,i])\n",
    "        res[outcome] = metrics\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95667ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_state_rf(model_args={}):\n",
    "    ids = get_dt_ids()\n",
    "    \n",
    "    dataset = DTDataset()\n",
    "\n",
    "    train_ids = ids[0:int(len(ids)*.7)]\n",
    "    test_ids = ids[int(len(ids)*.7):]\n",
    "    \n",
    "    #most things are multiclass, dlts are several ordinal and outcomes are multiple binary\n",
    "    xtrain1 = dataset.get_state('baseline',ids=train_ids)\n",
    "    xtest1 = dataset.get_state('baseline',ids=test_ids)\n",
    "    \n",
    "    xtrain2 = dataset.get_input_state(step=2,ids=train_ids)\n",
    "    xtest2 = dataset.get_input_state(step=2,ids=test_ids)\n",
    "    \n",
    "    xtrain3 = dataset.get_input_state(step=3,ids=train_ids)\n",
    "    xtest3 = dataset.get_input_state(step=3,ids=test_ids)\n",
    "    \n",
    "    [pd1_train,nd1_train, mod_train,dlts1_train] = dataset.get_intermediate_outcomes(ids=train_ids)\n",
    "    [pd2_train,nd2_train, cc_train,dlts2_train] = dataset.get_intermediate_outcomes(step=2,ids=train_ids)\n",
    "    [pd1_test,nd1_test, mod_test,dlts1_test] = dataset.get_intermediate_outcomes(ids=test_ids)\n",
    "    [pd2_test,nd2_test, cc_test,dlts2_test] = dataset.get_intermediate_outcomes(step=2,ids=test_ids)\n",
    "    outcomes_train = dataset.get_state('outcomes',ids=train_ids)\n",
    "    outcomes_test = dataset.get_state('outcomes',ids=test_ids)\n",
    "    \n",
    "\n",
    "    def train_multiclass_rf(xtrain,xtest,ytrain,ytest):\n",
    "        model = RandomForestClassifier(class_weight='balanced',**model_args).fit(xtrain,ytrain)\n",
    "        ypred = model.predict(xtest)\n",
    "        metrics = mc_metrics(ytest.values,ypred,numpy=True)\n",
    "        return model, metrics\n",
    "    \n",
    "    all_metrics = {}\n",
    "    pd1_model, all_metrics['pd1'] = train_multiclass_rf(xtrain1,xtest1,pd1_train,pd1_test)\n",
    "    nd1_model, all_metrics['nd1']  = train_multiclass_rf(xtrain1,xtest1,nd1_train,nd1_test)\n",
    "    mod_model, all_metrics['mod']  = train_multiclass_rf(xtrain1,xtest1,mod_train,mod_test)\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "# train_state_rf({'max_depth': 5,'n_estimators': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a9536fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 11.321202278137207\n",
      "val loss 11.089699745178223\n",
      "______________\n",
      "epoch 1 train loss 11.161775588989258\n",
      "val loss 10.948609352111816\n",
      "______________\n",
      "epoch 2 train loss 10.995708465576172\n",
      "val loss 10.809993743896484\n",
      "______________\n",
      "epoch 3 train loss 10.84663200378418\n",
      "val loss 10.673552513122559\n",
      "______________\n",
      "epoch 4 train loss 10.72703742980957\n",
      "val loss 10.53877067565918\n",
      "______________\n",
      "epoch 5 train loss 10.583897590637207\n",
      "val loss 10.405231475830078\n",
      "______________\n",
      "epoch 6 train loss 10.429883003234863\n",
      "val loss 10.272747993469238\n",
      "______________\n",
      "epoch 7 train loss 10.273176193237305\n",
      "val loss 10.140966415405273\n",
      "______________\n",
      "epoch 8 train loss 10.197378158569336\n",
      "val loss 10.009674072265625\n",
      "______________\n",
      "epoch 9 train loss 10.046801567077637\n",
      "val loss 9.878469467163086\n",
      "______________\n",
      "epoch 10 train loss 9.916838645935059\n",
      "val loss 9.747241020202637\n",
      "______________\n",
      "epoch 11 train loss 9.772366523742676\n",
      "val loss 9.615583419799805\n",
      "______________\n",
      "epoch 12 train loss 9.635926246643066\n",
      "val loss 9.483182907104492\n",
      "______________\n",
      "epoch 13 train loss 9.518918991088867\n",
      "val loss 9.350027084350586\n",
      "______________\n",
      "epoch 14 train loss 9.384162902832031\n",
      "val loss 9.215749740600586\n",
      "______________\n",
      "epoch 15 train loss 9.212592124938965\n",
      "val loss 9.080344200134277\n",
      "______________\n",
      "epoch 16 train loss 9.119681358337402\n",
      "val loss 8.943922996520996\n",
      "______________\n",
      "epoch 17 train loss 8.94459056854248\n",
      "val loss 8.80627155303955\n",
      "______________\n",
      "epoch 18 train loss 8.789530754089355\n",
      "val loss 8.667572975158691\n",
      "______________\n",
      "epoch 19 train loss 8.652303695678711\n",
      "val loss 8.527726173400879\n",
      "______________\n",
      "epoch 20 train loss 8.535725593566895\n",
      "val loss 8.386780738830566\n",
      "______________\n",
      "epoch 21 train loss 8.35890007019043\n",
      "val loss 8.244921684265137\n",
      "______________\n",
      "epoch 22 train loss 8.225691795349121\n",
      "val loss 8.102279663085938\n",
      "______________\n",
      "epoch 23 train loss 8.084277153015137\n",
      "val loss 7.958920001983643\n",
      "______________\n",
      "epoch 24 train loss 7.948785305023193\n",
      "val loss 7.815014362335205\n",
      "______________\n",
      "epoch 25 train loss 7.7720255851745605\n",
      "val loss 7.670680522918701\n",
      "______________\n",
      "epoch 26 train loss 7.650429725646973\n",
      "val loss 7.526385307312012\n",
      "______________\n",
      "epoch 27 train loss 7.433878421783447\n",
      "val loss 7.382139682769775\n",
      "______________\n",
      "epoch 28 train loss 7.347415924072266\n",
      "val loss 7.238341331481934\n",
      "______________\n",
      "epoch 29 train loss 7.213089466094971\n",
      "val loss 7.095389366149902\n",
      "______________\n",
      "epoch 30 train loss 7.088626861572266\n",
      "val loss 6.953492164611816\n",
      "______________\n",
      "epoch 31 train loss 6.870017051696777\n",
      "val loss 6.81298828125\n",
      "______________\n",
      "epoch 32 train loss 6.77333402633667\n",
      "val loss 6.674206256866455\n",
      "______________\n",
      "epoch 33 train loss 6.660219669342041\n",
      "val loss 6.537261486053467\n",
      "______________\n",
      "epoch 34 train loss 6.505627155303955\n",
      "val loss 6.4024248123168945\n",
      "______________\n",
      "epoch 35 train loss 6.371317386627197\n",
      "val loss 6.269927501678467\n",
      "______________\n",
      "epoch 36 train loss 6.253842353820801\n",
      "val loss 6.140220642089844\n",
      "______________\n",
      "epoch 37 train loss 6.069307327270508\n",
      "val loss 6.013398170471191\n",
      "______________\n",
      "epoch 38 train loss 5.921905994415283\n",
      "val loss 5.8897480964660645\n",
      "______________\n",
      "epoch 39 train loss 5.820695400238037\n",
      "val loss 5.769312381744385\n",
      "______________\n",
      "epoch 40 train loss 5.640746116638184\n",
      "val loss 5.65217399597168\n",
      "______________\n",
      "epoch 41 train loss 5.55871057510376\n",
      "val loss 5.538542747497559\n",
      "______________\n",
      "epoch 42 train loss 5.48453950881958\n",
      "val loss 5.428534984588623\n",
      "______________\n",
      "epoch 43 train loss 5.30890417098999\n",
      "val loss 5.3221116065979\n",
      "______________\n",
      "epoch 44 train loss 5.220830917358398\n",
      "val loss 5.219470024108887\n",
      "______________\n",
      "epoch 45 train loss 5.118905067443848\n",
      "val loss 5.120502471923828\n",
      "______________\n",
      "epoch 46 train loss 5.001461505889893\n",
      "val loss 5.025279998779297\n",
      "______________\n",
      "epoch 47 train loss 4.942719459533691\n",
      "val loss 4.933712959289551\n",
      "______________\n",
      "epoch 48 train loss 4.812746047973633\n",
      "val loss 4.845818519592285\n",
      "______________\n",
      "epoch 49 train loss 4.7132768630981445\n",
      "val loss 4.761537551879883\n",
      "______________\n",
      "epoch 50 train loss 4.626904487609863\n",
      "val loss 4.680660247802734\n",
      "______________\n",
      "epoch 51 train loss 4.628994464874268\n",
      "val loss 4.603176593780518\n",
      "______________\n",
      "epoch 52 train loss 4.484522819519043\n",
      "val loss 4.52888298034668\n",
      "______________\n",
      "epoch 53 train loss 4.428491115570068\n",
      "val loss 4.457787036895752\n",
      "______________\n",
      "epoch 54 train loss 4.3708415031433105\n",
      "val loss 4.389769554138184\n",
      "______________\n",
      "epoch 55 train loss 4.286918640136719\n",
      "val loss 4.3248372077941895\n",
      "______________\n",
      "epoch 56 train loss 4.191723823547363\n",
      "val loss 4.262736797332764\n",
      "______________\n",
      "epoch 57 train loss 4.121004581451416\n",
      "val loss 4.2034125328063965\n",
      "______________\n",
      "epoch 58 train loss 4.0931901931762695\n",
      "val loss 4.146667957305908\n",
      "______________\n",
      "epoch 59 train loss 4.0034003257751465\n",
      "val loss 4.092452526092529\n",
      "______________\n",
      "epoch 60 train loss 3.944303035736084\n",
      "val loss 4.040606498718262\n",
      "______________\n",
      "epoch 61 train loss 3.9662280082702637\n",
      "val loss 3.9910640716552734\n",
      "______________\n",
      "epoch 62 train loss 3.8981802463531494\n",
      "val loss 3.9437828063964844\n",
      "______________\n",
      "epoch 63 train loss 3.858149528503418\n",
      "val loss 3.8985438346862793\n",
      "______________\n",
      "epoch 64 train loss 3.843966484069824\n",
      "val loss 3.855313301086426\n",
      "______________\n",
      "epoch 65 train loss 3.7292168140411377\n",
      "val loss 3.8140177726745605\n",
      "______________\n",
      "epoch 66 train loss 3.6639552116394043\n",
      "val loss 3.7744672298431396\n",
      "______________\n",
      "epoch 67 train loss 3.6538913249969482\n",
      "val loss 3.7365033626556396\n",
      "______________\n",
      "epoch 68 train loss 3.6014456748962402\n",
      "val loss 3.700144052505493\n",
      "______________\n",
      "epoch 69 train loss 3.5563206672668457\n",
      "val loss 3.665290594100952\n",
      "______________\n",
      "epoch 70 train loss 3.544560670852661\n",
      "val loss 3.6318399906158447\n",
      "______________\n",
      "epoch 71 train loss 3.523379325866699\n",
      "val loss 3.5997304916381836\n",
      "______________\n",
      "epoch 72 train loss 3.488635301589966\n",
      "val loss 3.5688564777374268\n",
      "______________\n",
      "epoch 73 train loss 3.416991710662842\n",
      "val loss 3.5392539501190186\n",
      "______________\n",
      "epoch 74 train loss 3.34277081489563\n",
      "val loss 3.51080060005188\n",
      "______________\n",
      "epoch 75 train loss 3.3981757164001465\n",
      "val loss 3.483421564102173\n",
      "______________\n",
      "epoch 76 train loss 3.351102828979492\n",
      "val loss 3.457085371017456\n",
      "______________\n",
      "epoch 77 train loss 3.3048315048217773\n",
      "val loss 3.431600332260132\n",
      "______________\n",
      "epoch 78 train loss 3.3277688026428223\n",
      "val loss 3.407045841217041\n",
      "______________\n",
      "epoch 79 train loss 3.244565486907959\n",
      "val loss 3.383378267288208\n",
      "______________\n",
      "epoch 80 train loss 3.2074031829833984\n",
      "val loss 3.3605587482452393\n",
      "______________\n",
      "epoch 81 train loss 3.1551079750061035\n",
      "val loss 3.3385064601898193\n",
      "______________\n",
      "epoch 82 train loss 3.199367046356201\n",
      "val loss 3.3171801567077637\n",
      "______________\n",
      "epoch 83 train loss 3.197890043258667\n",
      "val loss 3.2964532375335693\n",
      "______________\n",
      "epoch 84 train loss 3.204407215118408\n",
      "val loss 3.2763121128082275\n",
      "______________\n",
      "epoch 85 train loss 3.1398978233337402\n",
      "val loss 3.256784200668335\n",
      "______________\n",
      "epoch 86 train loss 3.1157143115997314\n",
      "val loss 3.237739086151123\n",
      "______________\n",
      "epoch 87 train loss 3.146627187728882\n",
      "val loss 3.2192041873931885\n",
      "______________\n",
      "epoch 88 train loss 3.0313913822174072\n",
      "val loss 3.20124888420105\n",
      "______________\n",
      "epoch 89 train loss 3.109095573425293\n",
      "val loss 3.183741331100464\n",
      "______________\n",
      "epoch 90 train loss 3.0576131343841553\n",
      "val loss 3.1666810512542725\n",
      "______________\n",
      "epoch 91 train loss 2.9889395236968994\n",
      "val loss 3.150047540664673\n",
      "______________\n",
      "epoch 92 train loss 2.9442293643951416\n",
      "val loss 3.1337697505950928\n",
      "______________\n",
      "epoch 93 train loss 2.9443039894104004\n",
      "val loss 3.1178269386291504\n",
      "______________\n",
      "epoch 94 train loss 2.9533262252807617\n",
      "val loss 3.102311134338379\n",
      "______________\n",
      "epoch 95 train loss 2.9438204765319824\n",
      "val loss 3.0871622562408447\n",
      "______________\n",
      "epoch 96 train loss 2.956495523452759\n",
      "val loss 3.0723867416381836\n",
      "______________\n",
      "epoch 97 train loss 2.912395477294922\n",
      "val loss 3.057908773422241\n",
      "______________\n",
      "epoch 98 train loss 2.9623942375183105\n",
      "val loss 3.0437564849853516\n",
      "______________\n",
      "epoch 99 train loss 2.8256936073303223\n",
      "val loss 3.029902696609497\n",
      "______________\n",
      "epoch 100 train loss 2.8899004459381104\n",
      "val loss 3.0163216590881348\n",
      "______________\n",
      "epoch 101 train loss 2.805036783218384\n",
      "val loss 3.0029821395874023\n",
      "______________\n",
      "epoch 102 train loss 2.8445076942443848\n",
      "val loss 2.98983097076416\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 103 train loss 2.832357168197632\n",
      "val loss 2.9768619537353516\n",
      "______________\n",
      "epoch 104 train loss 2.7890467643737793\n",
      "val loss 2.964097738265991\n",
      "______________\n",
      "epoch 105 train loss 2.791548490524292\n",
      "val loss 2.9515721797943115\n",
      "______________\n",
      "epoch 106 train loss 2.8215065002441406\n",
      "val loss 2.9391398429870605\n",
      "______________\n",
      "epoch 107 train loss 2.779176712036133\n",
      "val loss 2.926924705505371\n",
      "______________\n",
      "epoch 108 train loss 2.730639934539795\n",
      "val loss 2.914900302886963\n",
      "______________\n",
      "epoch 109 train loss 2.76462459564209\n",
      "val loss 2.902996301651001\n",
      "______________\n",
      "epoch 110 train loss 2.742753744125366\n",
      "val loss 2.8912649154663086\n",
      "______________\n",
      "epoch 111 train loss 2.7175374031066895\n",
      "val loss 2.879735231399536\n",
      "______________\n",
      "epoch 112 train loss 2.693880558013916\n",
      "val loss 2.8682944774627686\n",
      "______________\n",
      "epoch 113 train loss 2.7065536975860596\n",
      "val loss 2.8569464683532715\n",
      "______________\n",
      "epoch 114 train loss 2.577432155609131\n",
      "val loss 2.845661163330078\n",
      "______________\n",
      "epoch 115 train loss 2.7289700508117676\n",
      "val loss 2.8345234394073486\n",
      "______________\n",
      "epoch 116 train loss 2.6185319423675537\n",
      "val loss 2.8235180377960205\n",
      "______________\n",
      "epoch 117 train loss 2.593435287475586\n",
      "val loss 2.812624216079712\n",
      "______________\n",
      "epoch 118 train loss 2.570556402206421\n",
      "val loss 2.801858425140381\n",
      "______________\n",
      "epoch 119 train loss 2.5815141201019287\n",
      "val loss 2.791227340698242\n",
      "______________\n",
      "epoch 120 train loss 2.5949807167053223\n",
      "val loss 2.7807412147521973\n",
      "______________\n",
      "epoch 121 train loss 2.50577974319458\n",
      "val loss 2.770404815673828\n",
      "______________\n",
      "epoch 122 train loss 2.609855890274048\n",
      "val loss 2.7602028846740723\n",
      "______________\n",
      "epoch 123 train loss 2.5452609062194824\n",
      "val loss 2.7501091957092285\n",
      "______________\n",
      "epoch 124 train loss 2.5151586532592773\n",
      "val loss 2.7401363849639893\n",
      "______________\n",
      "epoch 125 train loss 2.522512912750244\n",
      "val loss 2.7302310466766357\n",
      "______________\n",
      "epoch 126 train loss 2.477328062057495\n",
      "val loss 2.720412254333496\n",
      "______________\n",
      "epoch 127 train loss 2.495412826538086\n",
      "val loss 2.710615873336792\n",
      "______________\n",
      "epoch 128 train loss 2.49680757522583\n",
      "val loss 2.7008581161499023\n",
      "______________\n",
      "epoch 129 train loss 2.509348154067993\n",
      "val loss 2.6912474632263184\n",
      "______________\n",
      "epoch 130 train loss 2.4892148971557617\n",
      "val loss 2.681711196899414\n",
      "______________\n",
      "epoch 131 train loss 2.4350666999816895\n",
      "val loss 2.6723382472991943\n",
      "______________\n",
      "epoch 132 train loss 2.405580520629883\n",
      "val loss 2.663054943084717\n",
      "______________\n",
      "epoch 133 train loss 2.421168327331543\n",
      "val loss 2.653838634490967\n",
      "______________\n",
      "epoch 134 train loss 2.4209060668945312\n",
      "val loss 2.644725799560547\n",
      "______________\n",
      "epoch 135 train loss 2.3904061317443848\n",
      "val loss 2.6357924938201904\n",
      "______________\n",
      "epoch 136 train loss 2.361938238143921\n",
      "val loss 2.6269242763519287\n",
      "______________\n",
      "epoch 137 train loss 2.3929710388183594\n",
      "val loss 2.6181817054748535\n",
      "______________\n",
      "epoch 138 train loss 2.4350850582122803\n",
      "val loss 2.6095640659332275\n",
      "______________\n",
      "epoch 139 train loss 2.3700995445251465\n",
      "val loss 2.6010403633117676\n",
      "______________\n",
      "epoch 140 train loss 2.4662418365478516\n",
      "val loss 2.592582941055298\n",
      "______________\n",
      "epoch 141 train loss 2.3565707206726074\n",
      "val loss 2.584122896194458\n",
      "______________\n",
      "epoch 142 train loss 2.3569860458374023\n",
      "val loss 2.575719118118286\n",
      "______________\n",
      "epoch 143 train loss 2.3591079711914062\n",
      "val loss 2.5673611164093018\n",
      "______________\n",
      "epoch 144 train loss 2.261549949645996\n",
      "val loss 2.5590980052948\n",
      "______________\n",
      "epoch 145 train loss 2.325482130050659\n",
      "val loss 2.5508785247802734\n",
      "______________\n",
      "epoch 146 train loss 2.3385586738586426\n",
      "val loss 2.5426042079925537\n",
      "______________\n",
      "epoch 147 train loss 2.340977430343628\n",
      "val loss 2.534343957901001\n",
      "______________\n",
      "epoch 148 train loss 2.2786409854888916\n",
      "val loss 2.526219606399536\n",
      "______________\n",
      "epoch 149 train loss 2.3559048175811768\n",
      "val loss 2.518221855163574\n",
      "______________\n",
      "epoch 150 train loss 2.2783401012420654\n",
      "val loss 2.5103096961975098\n",
      "______________\n",
      "epoch 151 train loss 2.2722573280334473\n",
      "val loss 2.502556562423706\n",
      "______________\n",
      "epoch 152 train loss 2.3072638511657715\n",
      "val loss 2.49493145942688\n",
      "______________\n",
      "epoch 153 train loss 2.229135274887085\n",
      "val loss 2.4873719215393066\n",
      "______________\n",
      "epoch 154 train loss 2.3047337532043457\n",
      "val loss 2.4799673557281494\n",
      "______________\n",
      "epoch 155 train loss 2.2571167945861816\n",
      "val loss 2.472663402557373\n",
      "______________\n",
      "epoch 156 train loss 2.205003499984741\n",
      "val loss 2.465472459793091\n",
      "______________\n",
      "epoch 157 train loss 2.195673942565918\n",
      "val loss 2.458381175994873\n",
      "______________\n",
      "epoch 158 train loss 2.1604433059692383\n",
      "val loss 2.4513940811157227\n",
      "______________\n",
      "epoch 159 train loss 2.119826555252075\n",
      "val loss 2.4445300102233887\n",
      "______________\n",
      "epoch 160 train loss 2.20615291595459\n",
      "val loss 2.437790632247925\n",
      "______________\n",
      "epoch 161 train loss 2.2061386108398438\n",
      "val loss 2.4311728477478027\n",
      "______________\n",
      "epoch 162 train loss 2.1960017681121826\n",
      "val loss 2.424731969833374\n",
      "______________\n",
      "epoch 163 train loss 2.145249605178833\n",
      "val loss 2.418327808380127\n",
      "______________\n",
      "epoch 164 train loss 2.216731548309326\n",
      "val loss 2.411998987197876\n",
      "______________\n",
      "epoch 165 train loss 2.171668767929077\n",
      "val loss 2.405754327774048\n",
      "______________\n",
      "epoch 166 train loss 2.139552116394043\n",
      "val loss 2.399567127227783\n",
      "______________\n",
      "epoch 167 train loss 2.1155571937561035\n",
      "val loss 2.39347505569458\n",
      "______________\n",
      "epoch 168 train loss 2.1372287273406982\n",
      "val loss 2.387441396713257\n",
      "______________\n",
      "epoch 169 train loss 2.111614227294922\n",
      "val loss 2.381513833999634\n",
      "______________\n",
      "epoch 170 train loss 2.1383280754089355\n",
      "val loss 2.3756067752838135\n",
      "______________\n",
      "epoch 171 train loss 2.0674993991851807\n",
      "val loss 2.369758129119873\n",
      "______________\n",
      "epoch 172 train loss 2.097923755645752\n",
      "val loss 2.3639206886291504\n",
      "______________\n",
      "epoch 173 train loss 2.0448131561279297\n",
      "val loss 2.358210325241089\n",
      "______________\n",
      "epoch 174 train loss 2.0502283573150635\n",
      "val loss 2.352534532546997\n",
      "______________\n",
      "epoch 175 train loss 2.0694634914398193\n",
      "val loss 2.346946954727173\n",
      "______________\n",
      "epoch 176 train loss 2.0168874263763428\n",
      "val loss 2.341477870941162\n",
      "______________\n",
      "epoch 177 train loss 2.061915874481201\n",
      "val loss 2.3360416889190674\n",
      "______________\n",
      "epoch 178 train loss 2.0946402549743652\n",
      "val loss 2.3307125568389893\n",
      "______________\n",
      "epoch 179 train loss 2.028822898864746\n",
      "val loss 2.3254168033599854\n",
      "______________\n",
      "epoch 180 train loss 2.061055898666382\n",
      "val loss 2.320195198059082\n",
      "______________\n",
      "epoch 181 train loss 2.0063095092773438\n",
      "val loss 2.3150713443756104\n",
      "______________\n",
      "epoch 182 train loss 2.0307445526123047\n",
      "val loss 2.310016632080078\n",
      "______________\n",
      "epoch 183 train loss 2.0495731830596924\n",
      "val loss 2.3050739765167236\n",
      "______________\n",
      "epoch 184 train loss 2.080209255218506\n",
      "val loss 2.3002142906188965\n",
      "______________\n",
      "epoch 185 train loss 2.0056488513946533\n",
      "val loss 2.2954177856445312\n",
      "______________\n",
      "epoch 186 train loss 1.9305237531661987\n",
      "val loss 2.290712356567383\n",
      "______________\n",
      "epoch 187 train loss 1.9643255472183228\n",
      "val loss 2.2859911918640137\n",
      "______________\n",
      "epoch 188 train loss 2.0209295749664307\n",
      "val loss 2.281294822692871\n",
      "______________\n",
      "epoch 189 train loss 1.9879529476165771\n",
      "val loss 2.276646137237549\n",
      "______________\n",
      "epoch 190 train loss 1.9707272052764893\n",
      "val loss 2.272031307220459\n",
      "______________\n",
      "epoch 191 train loss 2.009340763092041\n",
      "val loss 2.267449378967285\n",
      "______________\n",
      "epoch 192 train loss 1.9821109771728516\n",
      "val loss 2.262927770614624\n",
      "______________\n",
      "epoch 193 train loss 1.9366841316223145\n",
      "val loss 2.258471965789795\n",
      "______________\n",
      "epoch 194 train loss 1.953792929649353\n",
      "val loss 2.2541110515594482\n",
      "______________\n",
      "epoch 195 train loss 1.9680341482162476\n",
      "val loss 2.2498040199279785\n",
      "______________\n",
      "epoch 196 train loss 1.8985153436660767\n",
      "val loss 2.2456986904144287\n",
      "______________\n",
      "epoch 197 train loss 1.9191913604736328\n",
      "val loss 2.241722345352173\n",
      "______________\n",
      "epoch 198 train loss 1.9058114290237427\n",
      "val loss 2.2378149032592773\n",
      "______________\n",
      "epoch 199 train loss 1.9669809341430664\n",
      "val loss 2.23390793800354\n",
      "______________\n",
      "epoch 200 train loss 1.9571669101715088\n",
      "val loss 2.22993540763855\n",
      "______________\n",
      "epoch 201 train loss 1.9072105884552002\n",
      "val loss 2.2260119915008545\n",
      "______________\n",
      "epoch 202 train loss 1.9174399375915527\n",
      "val loss 2.2222490310668945\n",
      "______________\n",
      "epoch 203 train loss 1.8766568899154663\n",
      "val loss 2.2184805870056152\n",
      "______________\n",
      "epoch 204 train loss 1.8832753896713257\n",
      "val loss 2.2147679328918457\n",
      "______________\n",
      "epoch 205 train loss 1.941367745399475\n",
      "val loss 2.211024522781372\n",
      "______________\n",
      "epoch 206 train loss 1.8391859531402588\n",
      "val loss 2.207317590713501\n",
      "______________\n",
      "epoch 207 train loss 1.8483401536941528\n",
      "val loss 2.203670024871826\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 208 train loss 1.949485182762146\n",
      "val loss 2.200037956237793\n",
      "______________\n",
      "epoch 209 train loss 1.838420033454895\n",
      "val loss 2.196558952331543\n",
      "______________\n",
      "epoch 210 train loss 1.88373601436615\n",
      "val loss 2.193074941635132\n",
      "______________\n",
      "epoch 211 train loss 1.8465343713760376\n",
      "val loss 2.1896445751190186\n",
      "______________\n",
      "epoch 212 train loss 1.8421432971954346\n",
      "val loss 2.1863186359405518\n",
      "______________\n",
      "epoch 213 train loss 1.8484703302383423\n",
      "val loss 2.183077812194824\n",
      "______________\n",
      "epoch 214 train loss 1.878557801246643\n",
      "val loss 2.179844379425049\n",
      "______________\n",
      "epoch 215 train loss 1.846774697303772\n",
      "val loss 2.176680088043213\n",
      "______________\n",
      "epoch 216 train loss 1.8362205028533936\n",
      "val loss 2.173482894897461\n",
      "______________\n",
      "epoch 217 train loss 1.7877600193023682\n",
      "val loss 2.1703855991363525\n",
      "______________\n",
      "epoch 218 train loss 1.7917028665542603\n",
      "val loss 2.1672887802124023\n",
      "______________\n",
      "epoch 219 train loss 1.8314651250839233\n",
      "val loss 2.1642162799835205\n",
      "______________\n",
      "epoch 220 train loss 1.7981772422790527\n",
      "val loss 2.1611223220825195\n",
      "______________\n",
      "epoch 221 train loss 1.8087550401687622\n",
      "val loss 2.1579668521881104\n",
      "______________\n",
      "epoch 222 train loss 1.8255283832550049\n",
      "val loss 2.1548373699188232\n",
      "______________\n",
      "epoch 223 train loss 1.7679810523986816\n",
      "val loss 2.1516761779785156\n",
      "______________\n",
      "epoch 224 train loss 1.8142271041870117\n",
      "val loss 2.1486353874206543\n",
      "______________\n",
      "epoch 225 train loss 1.8141133785247803\n",
      "val loss 2.1456124782562256\n",
      "______________\n",
      "epoch 226 train loss 1.7779662609100342\n",
      "val loss 2.142658233642578\n",
      "______________\n",
      "epoch 227 train loss 1.816767692565918\n",
      "val loss 2.1397342681884766\n",
      "______________\n",
      "epoch 228 train loss 1.80526864528656\n",
      "val loss 2.1368484497070312\n",
      "______________\n",
      "epoch 229 train loss 1.8188278675079346\n",
      "val loss 2.1339495182037354\n",
      "______________\n",
      "epoch 230 train loss 1.7845630645751953\n",
      "val loss 2.1311423778533936\n",
      "______________\n",
      "epoch 231 train loss 1.7701780796051025\n",
      "val loss 2.1284067630767822\n",
      "______________\n",
      "epoch 232 train loss 1.7670286893844604\n",
      "val loss 2.1256649494171143\n",
      "______________\n",
      "epoch 233 train loss 1.775115966796875\n",
      "val loss 2.122887372970581\n",
      "______________\n",
      "epoch 234 train loss 1.772247076034546\n",
      "val loss 2.120140552520752\n",
      "______________\n",
      "epoch 235 train loss 1.7279833555221558\n",
      "val loss 2.1174280643463135\n",
      "______________\n",
      "epoch 236 train loss 1.730505347251892\n",
      "val loss 2.114759922027588\n",
      "______________\n",
      "epoch 237 train loss 1.7733252048492432\n",
      "val loss 2.1120762825012207\n",
      "______________\n",
      "epoch 238 train loss 1.7200613021850586\n",
      "val loss 2.1095502376556396\n",
      "______________\n",
      "epoch 239 train loss 1.7533729076385498\n",
      "val loss 2.1070008277893066\n",
      "______________\n",
      "epoch 240 train loss 1.717699646949768\n",
      "val loss 2.104513168334961\n",
      "______________\n",
      "epoch 241 train loss 1.7177079916000366\n",
      "val loss 2.1022613048553467\n",
      "______________\n",
      "epoch 242 train loss 1.697580337524414\n",
      "val loss 2.100168466567993\n",
      "______________\n",
      "epoch 243 train loss 1.7090647220611572\n",
      "val loss 2.098126173019409\n",
      "______________\n",
      "epoch 244 train loss 1.7704839706420898\n",
      "val loss 2.0959742069244385\n",
      "______________\n",
      "epoch 245 train loss 1.732861876487732\n",
      "val loss 2.093771457672119\n",
      "______________\n",
      "epoch 246 train loss 1.6771124601364136\n",
      "val loss 2.0915815830230713\n",
      "______________\n",
      "epoch 247 train loss 1.7301862239837646\n",
      "val loss 2.08947491645813\n",
      "______________\n",
      "epoch 248 train loss 1.6802703142166138\n",
      "val loss 2.087336778640747\n",
      "______________\n",
      "epoch 249 train loss 1.6972475051879883\n",
      "val loss 2.085218667984009\n",
      "______________\n",
      "epoch 250 train loss 1.7195580005645752\n",
      "val loss 2.083057165145874\n",
      "______________\n",
      "epoch 251 train loss 1.681996464729309\n",
      "val loss 2.080906629562378\n",
      "______________\n",
      "epoch 252 train loss 1.70612370967865\n",
      "val loss 2.078799247741699\n",
      "______________\n",
      "epoch 253 train loss 1.720872402191162\n",
      "val loss 2.076686143875122\n",
      "______________\n",
      "epoch 254 train loss 1.7153606414794922\n",
      "val loss 2.074475049972534\n",
      "______________\n",
      "epoch 255 train loss 1.7186672687530518\n",
      "val loss 2.072364568710327\n",
      "______________\n",
      "epoch 256 train loss 1.7110204696655273\n",
      "val loss 2.0702872276306152\n",
      "______________\n",
      "epoch 257 train loss 1.725757122039795\n",
      "val loss 2.068302869796753\n",
      "______________\n",
      "epoch 258 train loss 1.6444778442382812\n",
      "val loss 2.066363573074341\n",
      "______________\n",
      "epoch 259 train loss 1.652146816253662\n",
      "val loss 2.0644707679748535\n",
      "______________\n",
      "epoch 260 train loss 1.61611008644104\n",
      "val loss 2.062558174133301\n",
      "______________\n",
      "epoch 261 train loss 1.6807191371917725\n",
      "val loss 2.0606019496917725\n",
      "______________\n",
      "epoch 262 train loss 1.6756654977798462\n",
      "val loss 2.058694362640381\n",
      "______________\n",
      "epoch 263 train loss 1.6043628454208374\n",
      "val loss 2.0568740367889404\n",
      "______________\n",
      "epoch 264 train loss 1.6529581546783447\n",
      "val loss 2.0551252365112305\n",
      "______________\n",
      "epoch 265 train loss 1.6793674230575562\n",
      "val loss 2.053483486175537\n",
      "______________\n",
      "epoch 266 train loss 1.6482620239257812\n",
      "val loss 2.0519165992736816\n",
      "______________\n",
      "epoch 267 train loss 1.671257734298706\n",
      "val loss 2.050375461578369\n",
      "______________\n",
      "epoch 268 train loss 1.636877179145813\n",
      "val loss 2.048860549926758\n",
      "______________\n",
      "epoch 269 train loss 1.729356050491333\n",
      "val loss 2.0472564697265625\n",
      "______________\n",
      "epoch 270 train loss 1.7124090194702148\n",
      "val loss 2.0457067489624023\n",
      "______________\n",
      "epoch 271 train loss 1.6454590559005737\n",
      "val loss 2.0441925525665283\n",
      "______________\n",
      "epoch 272 train loss 1.612480640411377\n",
      "val loss 2.042786121368408\n",
      "______________\n",
      "epoch 273 train loss 1.668444037437439\n",
      "val loss 2.04146409034729\n",
      "______________\n",
      "epoch 274 train loss 1.6199407577514648\n",
      "val loss 2.0401833057403564\n",
      "______________\n",
      "epoch 275 train loss 1.685978651046753\n",
      "val loss 2.038801670074463\n",
      "______________\n",
      "epoch 276 train loss 1.6753082275390625\n",
      "val loss 2.0373730659484863\n",
      "______________\n",
      "epoch 277 train loss 1.6084134578704834\n",
      "val loss 2.0359625816345215\n",
      "______________\n",
      "epoch 278 train loss 1.5908033847808838\n",
      "val loss 2.0345897674560547\n",
      "______________\n",
      "epoch 279 train loss 1.6321203708648682\n",
      "val loss 2.033267021179199\n",
      "______________\n",
      "epoch 280 train loss 1.663606882095337\n",
      "val loss 2.031801700592041\n",
      "______________\n",
      "epoch 281 train loss 1.6387742757797241\n",
      "val loss 2.0303122997283936\n",
      "______________\n",
      "epoch 282 train loss 1.6095926761627197\n",
      "val loss 2.028844118118286\n",
      "______________\n",
      "epoch 283 train loss 1.537768840789795\n",
      "val loss 2.027423858642578\n",
      "______________\n",
      "epoch 284 train loss 1.5914236307144165\n",
      "val loss 2.026149034500122\n",
      "______________\n",
      "epoch 285 train loss 1.585462212562561\n",
      "val loss 2.0248966217041016\n",
      "______________\n",
      "epoch 286 train loss 1.5768710374832153\n",
      "val loss 2.023700714111328\n",
      "______________\n",
      "epoch 287 train loss 1.6097073554992676\n",
      "val loss 2.0226635932922363\n",
      "______________\n",
      "epoch 288 train loss 1.6164544820785522\n",
      "val loss 2.0215718746185303\n",
      "______________\n",
      "epoch 289 train loss 1.6024984121322632\n",
      "val loss 2.0203230381011963\n",
      "______________\n",
      "epoch 290 train loss 1.558030366897583\n",
      "val loss 2.0190396308898926\n",
      "______________\n",
      "epoch 291 train loss 1.5844887495040894\n",
      "val loss 2.017763376235962\n",
      "______________\n",
      "epoch 292 train loss 1.6268208026885986\n",
      "val loss 2.016577959060669\n",
      "______________\n",
      "epoch 293 train loss 1.5927810668945312\n",
      "val loss 2.0153207778930664\n",
      "______________\n",
      "epoch 294 train loss 1.5895439386367798\n",
      "val loss 2.0140223503112793\n",
      "______________\n",
      "epoch 295 train loss 1.5917729139328003\n",
      "val loss 2.0128488540649414\n",
      "______________\n",
      "epoch 296 train loss 1.5737239122390747\n",
      "val loss 2.011667251586914\n",
      "______________\n",
      "epoch 297 train loss 1.560638427734375\n",
      "val loss 2.010612726211548\n",
      "______________\n",
      "epoch 298 train loss 1.5943459272384644\n",
      "val loss 2.0096194744110107\n",
      "______________\n",
      "epoch 299 train loss 1.5880318880081177\n",
      "val loss 2.008629083633423\n",
      "______________\n",
      "epoch 300 train loss 1.5826356410980225\n",
      "val loss 2.0076820850372314\n",
      "______________\n",
      "epoch 301 train loss 1.5850507020950317\n",
      "val loss 2.0067145824432373\n",
      "______________\n",
      "epoch 302 train loss 1.5588880777359009\n",
      "val loss 2.0057733058929443\n",
      "______________\n",
      "epoch 303 train loss 1.6012365818023682\n",
      "val loss 2.0049550533294678\n",
      "______________\n",
      "epoch 304 train loss 1.520240068435669\n",
      "val loss 2.0039520263671875\n",
      "______________\n",
      "epoch 305 train loss 1.5512663125991821\n",
      "val loss 2.002918004989624\n",
      "______________\n",
      "epoch 306 train loss 1.5732245445251465\n",
      "val loss 2.0017240047454834\n",
      "______________\n",
      "epoch 307 train loss 1.5599806308746338\n",
      "val loss 2.0006303787231445\n",
      "______________\n",
      "epoch 308 train loss 1.5357768535614014\n",
      "val loss 1.9996010065078735\n",
      "______________\n",
      "epoch 309 train loss 1.6308221817016602\n",
      "val loss 1.9984004497528076\n",
      "______________\n",
      "epoch 310 train loss 1.5491880178451538\n",
      "val loss 1.997041940689087\n",
      "______________\n",
      "epoch 311 train loss 1.5510512590408325\n",
      "val loss 1.9957623481750488\n",
      "______________\n",
      "epoch 312 train loss 1.5252187252044678\n",
      "val loss 1.9945852756500244\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 313 train loss 1.5138837099075317\n",
      "val loss 1.9934993982315063\n",
      "______________\n",
      "epoch 314 train loss 1.5356178283691406\n",
      "val loss 1.9923373460769653\n",
      "______________\n",
      "epoch 315 train loss 1.4871935844421387\n",
      "val loss 1.991365909576416\n",
      "______________\n",
      "epoch 316 train loss 1.532147765159607\n",
      "val loss 1.9905414581298828\n",
      "______________\n",
      "epoch 317 train loss 1.5246645212173462\n",
      "val loss 1.9898080825805664\n",
      "______________\n",
      "epoch 318 train loss 1.5088404417037964\n",
      "val loss 1.9891016483306885\n",
      "______________\n",
      "epoch 319 train loss 1.5125858783721924\n",
      "val loss 1.9883733987808228\n",
      "______________\n",
      "epoch 320 train loss 1.5278825759887695\n",
      "val loss 1.9875526428222656\n",
      "______________\n",
      "epoch 321 train loss 1.545936942100525\n",
      "val loss 1.9867939949035645\n",
      "______________\n",
      "epoch 322 train loss 1.5151132345199585\n",
      "val loss 1.9861658811569214\n",
      "______________\n",
      "epoch 323 train loss 1.5465316772460938\n",
      "val loss 1.9856656789779663\n",
      "______________\n",
      "epoch 324 train loss 1.4800807237625122\n",
      "val loss 1.985215663909912\n",
      "______________\n",
      "epoch 325 train loss 1.43134605884552\n",
      "val loss 1.9847218990325928\n",
      "______________\n",
      "epoch 326 train loss 1.5252383947372437\n",
      "val loss 1.9841972589492798\n",
      "______________\n",
      "epoch 327 train loss 1.4571443796157837\n",
      "val loss 1.9836961030960083\n",
      "______________\n",
      "epoch 328 train loss 1.553823471069336\n",
      "val loss 1.9832024574279785\n",
      "______________\n",
      "epoch 329 train loss 1.4922399520874023\n",
      "val loss 1.9826853275299072\n",
      "______________\n",
      "epoch 330 train loss 1.5026283264160156\n",
      "val loss 1.982134461402893\n",
      "______________\n",
      "epoch 331 train loss 1.4601492881774902\n",
      "val loss 1.9814088344573975\n",
      "______________\n",
      "epoch 332 train loss 1.4746649265289307\n",
      "val loss 1.980837345123291\n",
      "______________\n",
      "epoch 333 train loss 1.50750732421875\n",
      "val loss 1.9802662134170532\n",
      "______________\n",
      "epoch 334 train loss 1.4881690740585327\n",
      "val loss 1.9796555042266846\n",
      "______________\n",
      "epoch 335 train loss 1.485021948814392\n",
      "val loss 1.9790316820144653\n",
      "______________\n",
      "epoch 336 train loss 1.4957568645477295\n",
      "val loss 1.978445291519165\n",
      "______________\n",
      "epoch 337 train loss 1.4486331939697266\n",
      "val loss 1.9781180620193481\n",
      "______________\n",
      "epoch 338 train loss 1.5428924560546875\n",
      "val loss 1.9776813983917236\n",
      "______________\n",
      "epoch 339 train loss 1.4747098684310913\n",
      "val loss 1.977215051651001\n",
      "______________\n",
      "epoch 340 train loss 1.463014006614685\n",
      "val loss 1.9766420125961304\n",
      "______________\n",
      "epoch 341 train loss 1.5169233083724976\n",
      "val loss 1.9758882522583008\n",
      "______________\n",
      "epoch 342 train loss 1.5301823616027832\n",
      "val loss 1.9751230478286743\n",
      "______________\n",
      "epoch 343 train loss 1.4599319696426392\n",
      "val loss 1.9744439125061035\n",
      "______________\n",
      "epoch 344 train loss 1.4788472652435303\n",
      "val loss 1.9738624095916748\n",
      "______________\n",
      "epoch 345 train loss 1.4656250476837158\n",
      "val loss 1.973238229751587\n",
      "______________\n",
      "epoch 346 train loss 1.4509187936782837\n",
      "val loss 1.9726043939590454\n",
      "______________\n",
      "epoch 347 train loss 1.5058600902557373\n",
      "val loss 1.9718739986419678\n",
      "______________\n",
      "epoch 348 train loss 1.5230991840362549\n",
      "val loss 1.9710118770599365\n",
      "______________\n",
      "epoch 349 train loss 1.4530242681503296\n",
      "val loss 1.9702309370040894\n",
      "______________\n",
      "epoch 350 train loss 1.4971427917480469\n",
      "val loss 1.9695006608963013\n",
      "______________\n",
      "epoch 351 train loss 1.5154399871826172\n",
      "val loss 1.9688549041748047\n",
      "______________\n",
      "epoch 352 train loss 1.4655542373657227\n",
      "val loss 1.9682596921920776\n",
      "______________\n",
      "epoch 353 train loss 1.4906489849090576\n",
      "val loss 1.9676272869110107\n",
      "______________\n",
      "epoch 354 train loss 1.4757357835769653\n",
      "val loss 1.966941237449646\n",
      "______________\n",
      "epoch 355 train loss 1.4720687866210938\n",
      "val loss 1.9663647413253784\n",
      "______________\n",
      "epoch 356 train loss 1.4655340909957886\n",
      "val loss 1.9658228158950806\n",
      "______________\n",
      "epoch 357 train loss 1.4796395301818848\n",
      "val loss 1.9653249979019165\n",
      "______________\n",
      "epoch 358 train loss 1.4341092109680176\n",
      "val loss 1.9649920463562012\n",
      "______________\n",
      "epoch 359 train loss 1.483838677406311\n",
      "val loss 1.9646968841552734\n",
      "______________\n",
      "epoch 360 train loss 1.4854226112365723\n",
      "val loss 1.9643917083740234\n",
      "______________\n",
      "epoch 361 train loss 1.4727039337158203\n",
      "val loss 1.9640332460403442\n",
      "______________\n",
      "epoch 362 train loss 1.453829050064087\n",
      "val loss 1.9638938903808594\n",
      "______________\n",
      "epoch 363 train loss 1.444686770439148\n",
      "val loss 1.963773250579834\n",
      "______________\n",
      "epoch 364 train loss 1.4524261951446533\n",
      "val loss 1.9635934829711914\n",
      "______________\n",
      "epoch 365 train loss 1.3957608938217163\n",
      "val loss 1.9635142087936401\n",
      "______________\n",
      "epoch 366 train loss 1.4612494707107544\n",
      "val loss 1.9633543491363525\n",
      "______________\n",
      "epoch 367 train loss 1.4054136276245117\n",
      "val loss 1.963156819343567\n",
      "______________\n",
      "epoch 368 train loss 1.4512780904769897\n",
      "val loss 1.962962031364441\n",
      "______________\n",
      "epoch 369 train loss 1.4146538972854614\n",
      "val loss 1.962790846824646\n",
      "______________\n",
      "epoch 370 train loss 1.4255939722061157\n",
      "val loss 1.9628320932388306\n",
      "______________\n",
      "epoch 371 train loss 1.439092993736267\n",
      "val loss 1.9627068042755127\n",
      "______________\n",
      "epoch 372 train loss 1.4445669651031494\n",
      "val loss 1.962441086769104\n",
      "______________\n",
      "epoch 373 train loss 1.4281471967697144\n",
      "val loss 1.9621520042419434\n",
      "______________\n",
      "epoch 374 train loss 1.4658446311950684\n",
      "val loss 1.961731195449829\n",
      "______________\n",
      "epoch 375 train loss 1.39162015914917\n",
      "val loss 1.9612703323364258\n",
      "______________\n",
      "epoch 376 train loss 1.4120813608169556\n",
      "val loss 1.9609436988830566\n",
      "______________\n",
      "epoch 377 train loss 1.4316189289093018\n",
      "val loss 1.960522174835205\n",
      "______________\n",
      "epoch 378 train loss 1.3890125751495361\n",
      "val loss 1.9600783586502075\n",
      "______________\n",
      "epoch 379 train loss 1.3971627950668335\n",
      "val loss 1.9598186016082764\n",
      "______________\n",
      "epoch 380 train loss 1.4289714097976685\n",
      "val loss 1.9595612287521362\n",
      "______________\n",
      "epoch 381 train loss 1.3928759098052979\n",
      "val loss 1.9594650268554688\n",
      "______________\n",
      "epoch 382 train loss 1.4322702884674072\n",
      "val loss 1.9593005180358887\n",
      "______________\n",
      "epoch 383 train loss 1.4023256301879883\n",
      "val loss 1.9591716527938843\n",
      "______________\n",
      "epoch 384 train loss 1.4764790534973145\n",
      "val loss 1.9590539932250977\n",
      "______________\n",
      "epoch 385 train loss 1.4194456338882446\n",
      "val loss 1.958939552307129\n",
      "______________\n",
      "epoch 386 train loss 1.4118993282318115\n",
      "val loss 1.9588916301727295\n",
      "______________\n",
      "epoch 387 train loss 1.406998872756958\n",
      "val loss 1.9588048458099365\n",
      "______________\n",
      "epoch 388 train loss 1.3740428686141968\n",
      "val loss 1.9588048458099365\n",
      "______________\n",
      "epoch 389 train loss 1.367841362953186\n",
      "val loss 1.9588665962219238\n",
      "______________\n",
      "epoch 390 train loss 1.3998018503189087\n",
      "val loss 1.9589135646820068\n",
      "______________\n",
      "epoch 391 train loss 1.3832412958145142\n",
      "val loss 1.958968997001648\n",
      "______________\n",
      "epoch 392 train loss 1.4338265657424927\n",
      "val loss 1.9588953256607056\n",
      "______________\n",
      "epoch 393 train loss 1.3929890394210815\n",
      "val loss 1.9588866233825684\n",
      "______________\n",
      "epoch 394 train loss 1.350609302520752\n",
      "val loss 1.9588892459869385\n",
      "______________\n",
      "epoch 395 train loss 1.3504472970962524\n",
      "val loss 1.9588825702667236\n",
      "______________\n",
      "epoch 396 train loss 1.3652667999267578\n",
      "val loss 1.958741545677185\n",
      "______________\n",
      "epoch 397 train loss 1.3878939151763916\n",
      "val loss 1.9585257768630981\n",
      "______________\n",
      "epoch 398 train loss 1.4190871715545654\n",
      "val loss 1.95820152759552\n",
      "______________\n",
      "epoch 399 train loss 1.4002783298492432\n",
      "val loss 1.957940697669983\n",
      "______________\n",
      "epoch 400 train loss 1.3819268941879272\n",
      "val loss 1.9577128887176514\n",
      "______________\n",
      "epoch 401 train loss 1.3973791599273682\n",
      "val loss 1.9572620391845703\n",
      "______________\n",
      "epoch 402 train loss 1.444547176361084\n",
      "val loss 1.9567198753356934\n",
      "______________\n",
      "epoch 403 train loss 1.3969403505325317\n",
      "val loss 1.956131935119629\n",
      "______________\n",
      "epoch 404 train loss 1.4008450508117676\n",
      "val loss 1.9557161331176758\n",
      "______________\n",
      "epoch 405 train loss 1.373045563697815\n",
      "val loss 1.955320119857788\n",
      "______________\n",
      "epoch 406 train loss 1.3590850830078125\n",
      "val loss 1.955003023147583\n",
      "______________\n",
      "epoch 407 train loss 1.3341598510742188\n",
      "val loss 1.9547799825668335\n",
      "______________\n",
      "epoch 408 train loss 1.3551913499832153\n",
      "val loss 1.954750895500183\n",
      "______________\n",
      "epoch 409 train loss 1.411198616027832\n",
      "val loss 1.9546209573745728\n",
      "______________\n",
      "epoch 410 train loss 1.357100009918213\n",
      "val loss 1.9545735120773315\n",
      "______________\n",
      "epoch 411 train loss 1.3715217113494873\n",
      "val loss 1.9546147584915161\n",
      "______________\n",
      "epoch 412 train loss 1.3643656969070435\n",
      "val loss 1.9546762704849243\n",
      "______________\n",
      "epoch 413 train loss 1.348587155342102\n",
      "val loss 1.9545910358428955\n",
      "______________\n",
      "epoch 414 train loss 1.399572730064392\n",
      "val loss 1.954521656036377\n",
      "______________\n",
      "epoch 415 train loss 1.3916714191436768\n",
      "val loss 1.9543043375015259\n",
      "______________\n",
      "epoch 416 train loss 1.3150283098220825\n",
      "val loss 1.9541842937469482\n",
      "______________\n",
      "epoch 417 train loss 1.365578055381775\n",
      "val loss 1.9542460441589355\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 418 train loss 1.3642195463180542\n",
      "val loss 1.9542382955551147\n",
      "______________\n",
      "epoch 419 train loss 1.3180352449417114\n",
      "val loss 1.9541406631469727\n",
      "______________\n",
      "epoch 420 train loss 1.3588937520980835\n",
      "val loss 1.9539542198181152\n",
      "______________\n",
      "epoch 421 train loss 1.3461133241653442\n",
      "val loss 1.9537417888641357\n",
      "______________\n",
      "epoch 422 train loss 1.3705823421478271\n",
      "val loss 1.9533976316452026\n",
      "______________\n",
      "epoch 423 train loss 1.2959614992141724\n",
      "val loss 1.9532790184020996\n",
      "______________\n",
      "epoch 424 train loss 1.3617805242538452\n",
      "val loss 1.9531097412109375\n",
      "______________\n",
      "epoch 425 train loss 1.3407927751541138\n",
      "val loss 1.952874779701233\n",
      "______________\n",
      "epoch 426 train loss 1.2977375984191895\n",
      "val loss 1.9526695013046265\n",
      "______________\n",
      "epoch 427 train loss 1.349993348121643\n",
      "val loss 1.952338695526123\n",
      "______________\n",
      "epoch 428 train loss 1.3461475372314453\n",
      "val loss 1.9522383213043213\n",
      "______________\n",
      "epoch 429 train loss 1.3086047172546387\n",
      "val loss 1.9522358179092407\n",
      "______________\n",
      "epoch 430 train loss 1.3052406311035156\n",
      "val loss 1.952223777770996\n",
      "______________\n",
      "epoch 431 train loss 1.3104761838912964\n",
      "val loss 1.9522560834884644\n",
      "______________\n",
      "epoch 432 train loss 1.2593454122543335\n",
      "val loss 1.9523515701293945\n",
      "______________\n",
      "epoch 433 train loss 1.3186578750610352\n",
      "val loss 1.9523870944976807\n",
      "______________\n",
      "epoch 434 train loss 1.3665647506713867\n",
      "val loss 1.9523333311080933\n",
      "______________\n",
      "epoch 435 train loss 1.3356486558914185\n",
      "val loss 1.952294111251831\n",
      "______________\n",
      "epoch 436 train loss 1.3024401664733887\n",
      "val loss 1.9523471593856812\n",
      "______________\n",
      "epoch 437 train loss 1.349497675895691\n",
      "val loss 1.9525885581970215\n",
      "______________\n",
      "epoch 438 train loss 1.332807183265686\n",
      "val loss 1.9528391361236572\n",
      "______________\n",
      "epoch 439 train loss 1.4017850160598755\n",
      "val loss 1.9529516696929932\n",
      "______________\n",
      "epoch 440 train loss 1.3827292919158936\n",
      "val loss 1.9530214071273804\n",
      "______________\n",
      "epoch 441 train loss 1.2714991569519043\n",
      "val loss 1.9529579877853394\n",
      "______________\n",
      "best loss 1.952223777770996 {'pd': {'accuracy': 0.544234360410831, 'roc_micro': 0.7122240683598386, 'roc_macro': 0.6682570122210697}, 'nd': {'accuracy': 0.6052176646236053, 'roc_micro': 0.7188434207302131, 'roc_macro': 0.6703426404513361}, 'mod': {'accuracy': 0.6052176646236053, 'roc_micro': 0.7188434207302131, 'roc_macro': 0.6703426404513361}, 'dlts': {'accuracy': [0.9178082191780822, 0.9657534246575342, 1.0, 0.9794520547945206, 0.9246575342465754, 1.0, 1.0, 0.9726027397260274], 'accuracy_mean': 0.9700342465753424, 'auc': [0.8694029850746269, 0.9134751773049645, -1, 0.8881118881118881, 0.9744107744107744, -1, -1, 0.9238095238095239], 'auc_mean': 0.1961512935889722}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OutcomeSimulator(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=63, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (disease_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       "  (nodal_disease_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       "  (dlt_layers): ModuleList(\n",
       "    (0): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (1): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (4): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (5): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (6): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (7): Linear(in_features=500, out_features=1, bias=True)\n",
       "  )\n",
       "  (treatment_layer): Linear(in_features=500, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_state(model_args={},\n",
    "                state=1,\n",
    "                split=.7,\n",
    "                lr=.0001,\n",
    "                epochs=1000,\n",
    "                patience=10,\n",
    "                weights=[1,1,1,10],\n",
    "                save_path='../data/models/',\n",
    "                use_default_split=True,\n",
    "                use_bagging_split=False,\n",
    "                resample_training=False,#use bootstraping on training data after splitting\n",
    "                n_validation_trainsteps=2,\n",
    "                file_suffix=''):\n",
    "    \n",
    "    ids = get_dt_ids()\n",
    "    \n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    \n",
    "    dataset = DTDataset()\n",
    "    \n",
    "    xtrain = dataset.get_input_state(step=state,ids=train_ids)\n",
    "    xtest = dataset.get_input_state(step=state,ids=test_ids)\n",
    "    ytrain = dataset.get_intermediate_outcomes(step=state,ids=train_ids)\n",
    "    ytest = dataset.get_intermediate_outcomes(step=state,ids=test_ids)\n",
    "    \n",
    "\n",
    "    if state < 3:\n",
    "        model = OutcomeSimulator(xtrain.shape[1],state=state,**model_args)\n",
    "        lfunc = state_loss\n",
    "    else:\n",
    "        model = EndpointSimulator(xtrain.shape[1],**model_args)\n",
    "        weights = weights[:3]\n",
    "        lfunc = outcome_loss\n",
    "        \n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    save_file = save_path + 'model_' + model.identifier + '_split' + str(split) + '_resample' + str(resample_training) +  '_hash' + hashcode + file_suffix + '.tar'\n",
    "    xtrain = df_to_torch(xtrain)\n",
    "    xtest = df_to_torch(xtest)\n",
    "    ytrain = [df_to_torch(t) for t in ytrain]\n",
    "    ytest= [df_to_torch(t) for t in ytest]\n",
    "    \n",
    "    model.fit_normalizer(xtrain)\n",
    "#     normalize = lambda x: (x - xtrain.mean(axis=0)+.01)/(xtrain.std(axis=0)+.01)\n",
    "#     unnormalize = lambda x: (x * (xtrain.std(axis=0) +.01)) + xtrain.mean(axis=0) - .01\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    best_val_loss = 1000000000000000000000000000\n",
    "    best_loss_metrics = {}\n",
    "    last_epoch = False\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xtrain_sample = xtrain#[torch.randint(len(xtrain),(len(xtrain),) )]\n",
    "        ypred = model(xtrain_sample)\n",
    "        loss = lfunc(ytrain,ypred,weights=weights)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch',epoch,'train loss',loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        yval = model(xtest)\n",
    "        val_loss = lfunc(ytest,yval,weights=weights)\n",
    "        if state < 3:\n",
    "            val_metrics = state_metrics(ytest,yval)\n",
    "        else:\n",
    "            val_metrics = outcome_metrics(ytest,yval)\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_loss_metrics = val_metrics\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        print('val loss',val_loss.item())\n",
    "        print('______________')\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('best loss',best_val_loss,best_loss_metrics)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    \n",
    "    #train one step on validation data\n",
    "    for i in range(n_validation_trainsteps):\n",
    "        model.train()\n",
    "        yval = model(xtest)\n",
    "        val_loss = lfunc(ytest,yval,weights=weights)\n",
    "        val_loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(),save_file)\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = train_state(state=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27cdf2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 10.57856559753418\n",
      "val loss 10.362581253051758\n",
      "______________\n",
      "epoch 1 train loss 10.434846878051758\n",
      "val loss 10.217052459716797\n",
      "______________\n",
      "epoch 2 train loss 10.315289497375488\n",
      "val loss 10.075185775756836\n",
      "______________\n",
      "epoch 3 train loss 10.122429847717285\n",
      "val loss 9.936813354492188\n",
      "______________\n",
      "epoch 4 train loss 10.021270751953125\n",
      "val loss 9.800786972045898\n",
      "______________\n",
      "epoch 5 train loss 9.910812377929688\n",
      "val loss 9.666879653930664\n",
      "______________\n",
      "epoch 6 train loss 9.745349884033203\n",
      "val loss 9.534619331359863\n",
      "______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1854: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 train loss 9.647045135498047\n",
      "val loss 9.403688430786133\n",
      "______________\n",
      "epoch 8 train loss 9.494948387145996\n",
      "val loss 9.273765563964844\n",
      "______________\n",
      "epoch 9 train loss 9.372906684875488\n",
      "val loss 9.144530296325684\n",
      "______________\n",
      "epoch 10 train loss 9.221898078918457\n",
      "val loss 9.015680313110352\n",
      "______________\n",
      "epoch 11 train loss 9.106910705566406\n",
      "val loss 8.887109756469727\n",
      "______________\n",
      "epoch 12 train loss 8.969865798950195\n",
      "val loss 8.758516311645508\n",
      "______________\n",
      "epoch 13 train loss 8.84380054473877\n",
      "val loss 8.629800796508789\n",
      "______________\n",
      "epoch 14 train loss 8.7363862991333\n",
      "val loss 8.50080680847168\n",
      "______________\n",
      "epoch 15 train loss 8.607129096984863\n",
      "val loss 8.371540069580078\n",
      "______________\n",
      "epoch 16 train loss 8.47675895690918\n",
      "val loss 8.242104530334473\n",
      "______________\n",
      "epoch 17 train loss 8.370414733886719\n",
      "val loss 8.112591743469238\n",
      "______________\n",
      "epoch 18 train loss 8.172536849975586\n",
      "val loss 7.982658863067627\n",
      "______________\n",
      "epoch 19 train loss 8.088911056518555\n",
      "val loss 7.852324485778809\n",
      "______________\n",
      "epoch 20 train loss 7.931215286254883\n",
      "val loss 7.721750259399414\n",
      "______________\n",
      "epoch 21 train loss 7.783174514770508\n",
      "val loss 7.591155529022217\n",
      "______________\n",
      "epoch 22 train loss 7.684610366821289\n",
      "val loss 7.460926532745361\n",
      "______________\n",
      "epoch 23 train loss 7.536048889160156\n",
      "val loss 7.330971717834473\n",
      "______________\n",
      "epoch 24 train loss 7.445095062255859\n",
      "val loss 7.201475620269775\n",
      "______________\n",
      "epoch 25 train loss 7.30693244934082\n",
      "val loss 7.07270622253418\n",
      "______________\n",
      "epoch 26 train loss 7.157285690307617\n",
      "val loss 6.94485330581665\n",
      "______________\n",
      "epoch 27 train loss 7.022189617156982\n",
      "val loss 6.818234443664551\n",
      "______________\n",
      "epoch 28 train loss 6.912902355194092\n",
      "val loss 6.693050384521484\n",
      "______________\n",
      "epoch 29 train loss 6.842715263366699\n",
      "val loss 6.569520473480225\n",
      "______________\n",
      "epoch 30 train loss 6.689066410064697\n",
      "val loss 6.4478960037231445\n",
      "______________\n",
      "epoch 31 train loss 6.546247959136963\n",
      "val loss 6.328477382659912\n",
      "______________\n",
      "epoch 32 train loss 6.435227870941162\n",
      "val loss 6.211316108703613\n",
      "______________\n",
      "epoch 33 train loss 6.272462844848633\n",
      "val loss 6.0967512130737305\n",
      "______________\n",
      "epoch 34 train loss 6.189236640930176\n",
      "val loss 5.984971046447754\n",
      "______________\n",
      "epoch 35 train loss 6.014164447784424\n",
      "val loss 5.876064777374268\n",
      "______________\n",
      "epoch 36 train loss 5.915470123291016\n",
      "val loss 5.770167350769043\n",
      "______________\n",
      "epoch 37 train loss 5.843249797821045\n",
      "val loss 5.667434215545654\n",
      "______________\n",
      "epoch 38 train loss 5.75197172164917\n",
      "val loss 5.567949295043945\n",
      "______________\n",
      "epoch 39 train loss 5.594964027404785\n",
      "val loss 5.471550941467285\n",
      "______________\n",
      "epoch 40 train loss 5.489516258239746\n",
      "val loss 5.378295421600342\n",
      "______________\n",
      "epoch 41 train loss 5.468122482299805\n",
      "val loss 5.288272857666016\n",
      "______________\n",
      "epoch 42 train loss 5.320662021636963\n",
      "val loss 5.201523780822754\n",
      "______________\n",
      "epoch 43 train loss 5.279577255249023\n",
      "val loss 5.117990970611572\n",
      "______________\n",
      "epoch 44 train loss 5.2146124839782715\n",
      "val loss 5.037778377532959\n",
      "______________\n",
      "epoch 45 train loss 5.072663307189941\n",
      "val loss 4.96088981628418\n",
      "______________\n",
      "epoch 46 train loss 5.019069671630859\n",
      "val loss 4.887242317199707\n",
      "______________\n",
      "epoch 47 train loss 4.96295166015625\n",
      "val loss 4.816588401794434\n",
      "______________\n",
      "epoch 48 train loss 4.914340019226074\n",
      "val loss 4.748997688293457\n",
      "______________\n",
      "epoch 49 train loss 4.837789535522461\n",
      "val loss 4.684177875518799\n",
      "______________\n",
      "epoch 50 train loss 4.736698627471924\n",
      "val loss 4.622167587280273\n",
      "______________\n",
      "epoch 51 train loss 4.629420280456543\n",
      "val loss 4.562809944152832\n",
      "______________\n",
      "epoch 52 train loss 4.55628776550293\n",
      "val loss 4.506284236907959\n",
      "______________\n",
      "epoch 53 train loss 4.521576404571533\n",
      "val loss 4.452286720275879\n",
      "______________\n",
      "epoch 54 train loss 4.568192481994629\n",
      "val loss 4.4009599685668945\n",
      "______________\n",
      "epoch 55 train loss 4.410561561584473\n",
      "val loss 4.352283477783203\n",
      "______________\n",
      "epoch 56 train loss 4.354618549346924\n",
      "val loss 4.305770397186279\n",
      "______________\n",
      "epoch 57 train loss 4.326539993286133\n",
      "val loss 4.261419773101807\n",
      "______________\n",
      "epoch 58 train loss 4.236379146575928\n",
      "val loss 4.219151973724365\n",
      "______________\n",
      "epoch 59 train loss 4.26748514175415\n",
      "val loss 4.179059028625488\n",
      "______________\n",
      "epoch 60 train loss 4.224842071533203\n",
      "val loss 4.140791416168213\n",
      "______________\n",
      "epoch 61 train loss 4.160116672515869\n",
      "val loss 4.104480743408203\n",
      "______________\n",
      "epoch 62 train loss 4.098390102386475\n",
      "val loss 4.070094585418701\n",
      "______________\n",
      "epoch 63 train loss 4.065885066986084\n",
      "val loss 4.037257194519043\n",
      "______________\n",
      "epoch 64 train loss 4.029848098754883\n",
      "val loss 4.005796432495117\n",
      "______________\n",
      "epoch 65 train loss 4.029131889343262\n",
      "val loss 3.975778341293335\n",
      "______________\n",
      "epoch 66 train loss 4.0321455001831055\n",
      "val loss 3.9469234943389893\n",
      "______________\n",
      "epoch 67 train loss 3.9884374141693115\n",
      "val loss 3.919666290283203\n",
      "______________\n",
      "epoch 68 train loss 3.9135377407073975\n",
      "val loss 3.8937833309173584\n",
      "______________\n",
      "epoch 69 train loss 3.9383819103240967\n",
      "val loss 3.869384288787842\n",
      "______________\n",
      "epoch 70 train loss 3.872220516204834\n",
      "val loss 3.8466155529022217\n",
      "______________\n",
      "epoch 71 train loss 3.8761444091796875\n",
      "val loss 3.8251256942749023\n",
      "______________\n",
      "epoch 72 train loss 3.811492681503296\n",
      "val loss 3.8046741485595703\n",
      "______________\n",
      "epoch 73 train loss 3.863337755203247\n",
      "val loss 3.7854433059692383\n",
      "______________\n",
      "epoch 74 train loss 3.7895445823669434\n",
      "val loss 3.767287492752075\n",
      "______________\n",
      "epoch 75 train loss 3.708068370819092\n",
      "val loss 3.750133991241455\n",
      "______________\n",
      "epoch 76 train loss 3.7924246788024902\n",
      "val loss 3.733811378479004\n",
      "______________\n",
      "epoch 77 train loss 3.813380718231201\n",
      "val loss 3.718327522277832\n",
      "______________\n",
      "epoch 78 train loss 3.771658182144165\n",
      "val loss 3.7035677433013916\n",
      "______________\n",
      "epoch 79 train loss 3.660799026489258\n",
      "val loss 3.68931245803833\n",
      "______________\n",
      "epoch 80 train loss 3.6624903678894043\n",
      "val loss 3.6756458282470703\n",
      "______________\n",
      "epoch 81 train loss 3.7315633296966553\n",
      "val loss 3.662583351135254\n",
      "______________\n",
      "epoch 82 train loss 3.6791279315948486\n",
      "val loss 3.6497936248779297\n",
      "______________\n",
      "epoch 83 train loss 3.6149725914001465\n",
      "val loss 3.637451171875\n",
      "______________\n",
      "epoch 84 train loss 3.6207990646362305\n",
      "val loss 3.62603759765625\n",
      "______________\n",
      "epoch 85 train loss 3.6191158294677734\n",
      "val loss 3.6151657104492188\n",
      "______________\n",
      "epoch 86 train loss 3.6048777103424072\n",
      "val loss 3.605095624923706\n",
      "______________\n",
      "epoch 87 train loss 3.6090853214263916\n",
      "val loss 3.5953986644744873\n",
      "______________\n",
      "epoch 88 train loss 3.5624196529388428\n",
      "val loss 3.5864126682281494\n",
      "______________\n",
      "epoch 89 train loss 3.627410888671875\n",
      "val loss 3.5776193141937256\n",
      "______________\n",
      "epoch 90 train loss 3.5454599857330322\n",
      "val loss 3.569263458251953\n",
      "______________\n",
      "epoch 91 train loss 3.525822639465332\n",
      "val loss 3.561464786529541\n",
      "______________\n",
      "epoch 92 train loss 3.534179210662842\n",
      "val loss 3.5537946224212646\n",
      "______________\n",
      "epoch 93 train loss 3.5289883613586426\n",
      "val loss 3.5459301471710205\n",
      "______________\n",
      "epoch 94 train loss 3.548091173171997\n",
      "val loss 3.5380988121032715\n",
      "______________\n",
      "epoch 95 train loss 3.528390884399414\n",
      "val loss 3.530409812927246\n",
      "______________\n",
      "epoch 96 train loss 3.4492568969726562\n",
      "val loss 3.5225412845611572\n",
      "______________\n",
      "epoch 97 train loss 3.482124090194702\n",
      "val loss 3.514946460723877\n",
      "______________\n",
      "epoch 98 train loss 3.4555320739746094\n",
      "val loss 3.507931709289551\n",
      "______________\n",
      "epoch 99 train loss 3.4525976181030273\n",
      "val loss 3.5014116764068604\n",
      "______________\n",
      "epoch 100 train loss 3.4961280822753906\n",
      "val loss 3.4952852725982666\n",
      "______________\n",
      "epoch 101 train loss 3.449310779571533\n",
      "val loss 3.489234685897827\n",
      "______________\n",
      "epoch 102 train loss 3.431062936782837\n",
      "val loss 3.48307204246521\n",
      "______________\n",
      "epoch 103 train loss 3.3934879302978516\n",
      "val loss 3.4767582416534424\n",
      "______________\n",
      "epoch 104 train loss 3.4258716106414795\n",
      "val loss 3.4708149433135986\n",
      "______________\n",
      "epoch 105 train loss 3.3810811042785645\n",
      "val loss 3.465095281600952\n",
      "______________\n",
      "epoch 106 train loss 3.3936522006988525\n",
      "val loss 3.459465980529785\n",
      "______________\n",
      "epoch 107 train loss 3.4026036262512207\n",
      "val loss 3.4538097381591797\n",
      "______________\n",
      "epoch 108 train loss 3.412590503692627\n",
      "val loss 3.448544979095459\n",
      "______________\n",
      "epoch 109 train loss 3.3546273708343506\n",
      "val loss 3.443436622619629\n",
      "______________\n",
      "epoch 110 train loss 3.3329014778137207\n",
      "val loss 3.4386560916900635\n",
      "______________\n",
      "epoch 111 train loss 3.3275644779205322\n",
      "val loss 3.434217929840088\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 112 train loss 3.3845198154449463\n",
      "val loss 3.429992914199829\n",
      "______________\n",
      "epoch 113 train loss 3.3996238708496094\n",
      "val loss 3.425759792327881\n",
      "______________\n",
      "epoch 114 train loss 3.309802293777466\n",
      "val loss 3.42155122756958\n",
      "______________\n",
      "epoch 115 train loss 3.2966971397399902\n",
      "val loss 3.417501449584961\n",
      "______________\n",
      "epoch 116 train loss 3.307574987411499\n",
      "val loss 3.413276433944702\n",
      "______________\n",
      "epoch 117 train loss 3.327221393585205\n",
      "val loss 3.4087932109832764\n",
      "______________\n",
      "epoch 118 train loss 3.2286341190338135\n",
      "val loss 3.404167413711548\n",
      "______________\n",
      "epoch 119 train loss 3.332287311553955\n",
      "val loss 3.399627208709717\n",
      "______________\n",
      "epoch 120 train loss 3.339592456817627\n",
      "val loss 3.395141839981079\n",
      "______________\n",
      "epoch 121 train loss 3.310680627822876\n",
      "val loss 3.3910908699035645\n",
      "______________\n",
      "epoch 122 train loss 3.3207902908325195\n",
      "val loss 3.386984348297119\n",
      "______________\n",
      "epoch 123 train loss 3.2160208225250244\n",
      "val loss 3.383061170578003\n",
      "______________\n",
      "epoch 124 train loss 3.26340389251709\n",
      "val loss 3.3793280124664307\n",
      "______________\n",
      "epoch 125 train loss 3.2016241550445557\n",
      "val loss 3.375792980194092\n",
      "______________\n",
      "epoch 126 train loss 3.2576372623443604\n",
      "val loss 3.37227201461792\n",
      "______________\n",
      "epoch 127 train loss 3.154719352722168\n",
      "val loss 3.368746280670166\n",
      "______________\n",
      "epoch 128 train loss 3.182291030883789\n",
      "val loss 3.365433931350708\n",
      "______________\n",
      "epoch 129 train loss 3.152482509613037\n",
      "val loss 3.3619275093078613\n",
      "______________\n",
      "epoch 130 train loss 3.2003917694091797\n",
      "val loss 3.358262062072754\n",
      "______________\n",
      "epoch 131 train loss 3.1695899963378906\n",
      "val loss 3.3545870780944824\n",
      "______________\n",
      "epoch 132 train loss 3.2240633964538574\n",
      "val loss 3.350785255432129\n",
      "______________\n",
      "epoch 133 train loss 3.1871795654296875\n",
      "val loss 3.3464322090148926\n",
      "______________\n",
      "epoch 134 train loss 3.2070376873016357\n",
      "val loss 3.341911554336548\n",
      "______________\n",
      "epoch 135 train loss 3.222142219543457\n",
      "val loss 3.337294816970825\n",
      "______________\n",
      "epoch 136 train loss 3.2088096141815186\n",
      "val loss 3.3323676586151123\n",
      "______________\n",
      "epoch 137 train loss 3.171724796295166\n",
      "val loss 3.3276360034942627\n",
      "______________\n",
      "epoch 138 train loss 3.2332053184509277\n",
      "val loss 3.323106050491333\n",
      "______________\n",
      "epoch 139 train loss 3.127189874649048\n",
      "val loss 3.318833827972412\n",
      "______________\n",
      "epoch 140 train loss 3.2062745094299316\n",
      "val loss 3.3150699138641357\n",
      "______________\n",
      "epoch 141 train loss 3.139702081680298\n",
      "val loss 3.3116307258605957\n",
      "______________\n",
      "epoch 142 train loss 3.1515579223632812\n",
      "val loss 3.3082621097564697\n",
      "______________\n",
      "epoch 143 train loss 3.1469738483428955\n",
      "val loss 3.3051090240478516\n",
      "______________\n",
      "epoch 144 train loss 3.0655834674835205\n",
      "val loss 3.302031993865967\n",
      "______________\n",
      "epoch 145 train loss 3.1281723976135254\n",
      "val loss 3.299098014831543\n",
      "______________\n",
      "epoch 146 train loss 3.0430169105529785\n",
      "val loss 3.2963972091674805\n",
      "______________\n",
      "epoch 147 train loss 3.0577807426452637\n",
      "val loss 3.2935802936553955\n",
      "______________\n",
      "epoch 148 train loss 3.058814764022827\n",
      "val loss 3.2908198833465576\n",
      "______________\n",
      "epoch 149 train loss 3.0628762245178223\n",
      "val loss 3.288238525390625\n",
      "______________\n",
      "epoch 150 train loss 3.106391429901123\n",
      "val loss 3.285875082015991\n",
      "______________\n",
      "epoch 151 train loss 3.0709142684936523\n",
      "val loss 3.283629894256592\n",
      "______________\n",
      "epoch 152 train loss 3.038231134414673\n",
      "val loss 3.2818944454193115\n",
      "______________\n",
      "epoch 153 train loss 3.0936429500579834\n",
      "val loss 3.279982805252075\n",
      "______________\n",
      "epoch 154 train loss 3.0698025226593018\n",
      "val loss 3.278132438659668\n",
      "______________\n",
      "epoch 155 train loss 3.133882522583008\n",
      "val loss 3.27618670463562\n",
      "______________\n",
      "epoch 156 train loss 3.084770441055298\n",
      "val loss 3.2743778228759766\n",
      "______________\n",
      "epoch 157 train loss 3.0564916133880615\n",
      "val loss 3.27250599861145\n",
      "______________\n",
      "epoch 158 train loss 3.050341844558716\n",
      "val loss 3.270219087600708\n",
      "______________\n",
      "epoch 159 train loss 3.0411741733551025\n",
      "val loss 3.2680091857910156\n",
      "______________\n",
      "epoch 160 train loss 3.013244390487671\n",
      "val loss 3.265746831893921\n",
      "______________\n",
      "epoch 161 train loss 3.05379581451416\n",
      "val loss 3.2636516094207764\n",
      "______________\n",
      "epoch 162 train loss 3.029332160949707\n",
      "val loss 3.261305570602417\n",
      "______________\n",
      "epoch 163 train loss 3.0551347732543945\n",
      "val loss 3.258936643600464\n",
      "______________\n",
      "epoch 164 train loss 2.9997689723968506\n",
      "val loss 3.256427526473999\n",
      "______________\n",
      "epoch 165 train loss 3.0287692546844482\n",
      "val loss 3.25398588180542\n",
      "______________\n",
      "epoch 166 train loss 3.0454261302948\n",
      "val loss 3.2511391639709473\n",
      "______________\n",
      "epoch 167 train loss 2.9623684883117676\n",
      "val loss 3.24812388420105\n",
      "______________\n",
      "epoch 168 train loss 2.95857310295105\n",
      "val loss 3.2452664375305176\n",
      "______________\n",
      "epoch 169 train loss 2.9662790298461914\n",
      "val loss 3.242511510848999\n",
      "______________\n",
      "epoch 170 train loss 2.9812700748443604\n",
      "val loss 3.2398993968963623\n",
      "______________\n",
      "epoch 171 train loss 2.9942431449890137\n",
      "val loss 3.2376456260681152\n",
      "______________\n",
      "epoch 172 train loss 2.9908907413482666\n",
      "val loss 3.2355451583862305\n",
      "______________\n",
      "epoch 173 train loss 2.976855516433716\n",
      "val loss 3.233445644378662\n",
      "______________\n",
      "epoch 174 train loss 2.995363712310791\n",
      "val loss 3.2314236164093018\n",
      "______________\n",
      "epoch 175 train loss 2.970249652862549\n",
      "val loss 3.2296245098114014\n",
      "______________\n",
      "epoch 176 train loss 2.959216833114624\n",
      "val loss 3.2277307510375977\n",
      "______________\n",
      "epoch 177 train loss 2.9401965141296387\n",
      "val loss 3.225719690322876\n",
      "______________\n",
      "epoch 178 train loss 3.010277509689331\n",
      "val loss 3.223719596862793\n",
      "______________\n",
      "epoch 179 train loss 2.9727087020874023\n",
      "val loss 3.2218916416168213\n",
      "______________\n",
      "epoch 180 train loss 2.9086697101593018\n",
      "val loss 3.220332384109497\n",
      "______________\n",
      "epoch 181 train loss 2.8937742710113525\n",
      "val loss 3.2186851501464844\n",
      "______________\n",
      "epoch 182 train loss 2.913705587387085\n",
      "val loss 3.2172436714172363\n",
      "______________\n",
      "epoch 183 train loss 2.9506936073303223\n",
      "val loss 3.215801954269409\n",
      "______________\n",
      "epoch 184 train loss 2.8648743629455566\n",
      "val loss 3.214421272277832\n",
      "______________\n",
      "epoch 185 train loss 2.8780477046966553\n",
      "val loss 3.213287115097046\n",
      "______________\n",
      "epoch 186 train loss 2.834411382675171\n",
      "val loss 3.212200880050659\n",
      "______________\n",
      "epoch 187 train loss 2.86407732963562\n",
      "val loss 3.211280345916748\n",
      "______________\n",
      "epoch 188 train loss 2.9091949462890625\n",
      "val loss 3.210448741912842\n",
      "______________\n",
      "epoch 189 train loss 2.869197130203247\n",
      "val loss 3.2099947929382324\n",
      "______________\n",
      "epoch 190 train loss 2.852177619934082\n",
      "val loss 3.2094335556030273\n",
      "______________\n",
      "epoch 191 train loss 2.8638765811920166\n",
      "val loss 3.208831787109375\n",
      "______________\n",
      "epoch 192 train loss 2.845813035964966\n",
      "val loss 3.20806884765625\n",
      "______________\n",
      "epoch 193 train loss 2.8482024669647217\n",
      "val loss 3.207219123840332\n",
      "______________\n",
      "epoch 194 train loss 2.833662986755371\n",
      "val loss 3.2060093879699707\n",
      "______________\n",
      "epoch 195 train loss 2.785583972930908\n",
      "val loss 3.2044787406921387\n",
      "______________\n",
      "epoch 196 train loss 2.874502182006836\n",
      "val loss 3.2027812004089355\n",
      "______________\n",
      "epoch 197 train loss 2.813678026199341\n",
      "val loss 3.2012553215026855\n",
      "______________\n",
      "epoch 198 train loss 2.872140407562256\n",
      "val loss 3.199643850326538\n",
      "______________\n",
      "epoch 199 train loss 2.8179314136505127\n",
      "val loss 3.1979830265045166\n",
      "______________\n",
      "epoch 200 train loss 2.8029940128326416\n",
      "val loss 3.196397066116333\n",
      "______________\n",
      "epoch 201 train loss 2.8840904235839844\n",
      "val loss 3.194793224334717\n",
      "______________\n",
      "epoch 202 train loss 2.8297693729400635\n",
      "val loss 3.1928882598876953\n",
      "______________\n",
      "epoch 203 train loss 2.8243796825408936\n",
      "val loss 3.191121816635132\n",
      "______________\n",
      "epoch 204 train loss 2.7919137477874756\n",
      "val loss 3.189539670944214\n",
      "______________\n",
      "epoch 205 train loss 2.842752456665039\n",
      "val loss 3.1881961822509766\n",
      "______________\n",
      "epoch 206 train loss 2.8585093021392822\n",
      "val loss 3.187136173248291\n",
      "______________\n",
      "epoch 207 train loss 2.774784564971924\n",
      "val loss 3.1861977577209473\n",
      "______________\n",
      "epoch 208 train loss 2.7373366355895996\n",
      "val loss 3.185227394104004\n",
      "______________\n",
      "epoch 209 train loss 2.7395734786987305\n",
      "val loss 3.1845862865448\n",
      "______________\n",
      "epoch 210 train loss 2.8276455402374268\n",
      "val loss 3.1837217807769775\n",
      "______________\n",
      "epoch 211 train loss 2.7885830402374268\n",
      "val loss 3.182832956314087\n",
      "______________\n",
      "epoch 212 train loss 2.755150556564331\n",
      "val loss 3.1817855834960938\n",
      "______________\n",
      "epoch 213 train loss 2.861333131790161\n",
      "val loss 3.1808784008026123\n",
      "______________\n",
      "epoch 214 train loss 2.767794132232666\n",
      "val loss 3.1803417205810547\n",
      "______________\n",
      "epoch 215 train loss 2.829385995864868\n",
      "val loss 3.179669141769409\n",
      "______________\n",
      "epoch 216 train loss 2.7855379581451416\n",
      "val loss 3.1788861751556396\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 217 train loss 2.7304670810699463\n",
      "val loss 3.1781890392303467\n",
      "______________\n",
      "epoch 218 train loss 2.748652696609497\n",
      "val loss 3.178069591522217\n",
      "______________\n",
      "epoch 219 train loss 2.7663278579711914\n",
      "val loss 3.1777775287628174\n",
      "______________\n",
      "epoch 220 train loss 2.74898099899292\n",
      "val loss 3.1775193214416504\n",
      "______________\n",
      "epoch 221 train loss 2.777428388595581\n",
      "val loss 3.176987648010254\n",
      "______________\n",
      "epoch 222 train loss 2.7731614112854004\n",
      "val loss 3.1758391857147217\n",
      "______________\n",
      "epoch 223 train loss 2.7273526191711426\n",
      "val loss 3.174546957015991\n",
      "______________\n",
      "epoch 224 train loss 2.704627513885498\n",
      "val loss 3.1734626293182373\n",
      "______________\n",
      "epoch 225 train loss 2.826051950454712\n",
      "val loss 3.172553300857544\n",
      "______________\n",
      "epoch 226 train loss 2.726706027984619\n",
      "val loss 3.171705961227417\n",
      "______________\n",
      "epoch 227 train loss 2.7222886085510254\n",
      "val loss 3.1705539226531982\n",
      "______________\n",
      "epoch 228 train loss 2.7674498558044434\n",
      "val loss 3.169396162033081\n",
      "______________\n",
      "epoch 229 train loss 2.7084877490997314\n",
      "val loss 3.1683971881866455\n",
      "______________\n",
      "epoch 230 train loss 2.705296516418457\n",
      "val loss 3.1673667430877686\n",
      "______________\n",
      "epoch 231 train loss 2.679445266723633\n",
      "val loss 3.1664812564849854\n",
      "______________\n",
      "epoch 232 train loss 2.7019524574279785\n",
      "val loss 3.1656112670898438\n",
      "______________\n",
      "epoch 233 train loss 2.6837122440338135\n",
      "val loss 3.165003538131714\n",
      "______________\n",
      "epoch 234 train loss 2.6935880184173584\n",
      "val loss 3.164708137512207\n",
      "______________\n",
      "epoch 235 train loss 2.717148542404175\n",
      "val loss 3.164489984512329\n",
      "______________\n",
      "epoch 236 train loss 2.67598295211792\n",
      "val loss 3.164306879043579\n",
      "______________\n",
      "epoch 237 train loss 2.6535441875457764\n",
      "val loss 3.164720296859741\n",
      "______________\n",
      "epoch 238 train loss 2.740426540374756\n",
      "val loss 3.165234327316284\n",
      "______________\n",
      "epoch 239 train loss 2.7101900577545166\n",
      "val loss 3.165708065032959\n",
      "______________\n",
      "epoch 240 train loss 2.6362555027008057\n",
      "val loss 3.166224479675293\n",
      "______________\n",
      "epoch 241 train loss 2.62790584564209\n",
      "val loss 3.1663079261779785\n",
      "______________\n",
      "epoch 242 train loss 2.6522555351257324\n",
      "val loss 3.166037082672119\n",
      "______________\n",
      "epoch 243 train loss 2.667706251144409\n",
      "val loss 3.1656742095947266\n",
      "______________\n",
      "epoch 244 train loss 2.652662992477417\n",
      "val loss 3.1651973724365234\n",
      "______________\n",
      "epoch 245 train loss 2.6569039821624756\n",
      "val loss 3.165151357650757\n",
      "______________\n",
      "epoch 246 train loss 2.652303695678711\n",
      "val loss 3.164869785308838\n",
      "______________\n",
      "epoch 247 train loss 2.6542274951934814\n",
      "val loss 3.1645562648773193\n",
      "______________\n",
      "best loss 3.164306879043579 {'pd': {'accuracy': 0.5, 'roc_micro': 0.9366836553277231, 'roc_macro': -1}, 'nd': {'accuracy': 0.316587552742616, 'roc_micro': 0.7342941700151584, 'roc_macro': 0.5445191628499555}, 'mod': {'accuracy': 0.316587552742616, 'roc_micro': 0.7342941700151584, 'roc_macro': 0.5445191628499555}, 'dlts': {'accuracy': [0.952054794520548, 0.9657534246575342, 1.0, 0.9794520547945206, 0.9178082191780822, 1.0, 1.0, 0.9657534246575342], 'accuracy_mean': 0.9726027397260275, 'auc': [0.7266187050359711, 0.5148936170212766, -1, 0.36596736596736595, 0.6299751243781095, -1, -1, 0.5574468085106383], 'auc_mean': -0.025637297385829816}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OutcomeSimulator(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=85, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (disease_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       "  (nodal_disease_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       "  (dlt_layers): ModuleList(\n",
       "    (0): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (1): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (4): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (5): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (6): Linear(in_features=500, out_features=1, bias=True)\n",
       "    (7): Linear(in_features=500, out_features=1, bias=True)\n",
       "  )\n",
       "  (treatment_layer): Linear(in_features=500, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = train_state(state=2)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c33d908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 2.2150607109069824\n",
      "val loss 2.1706087589263916\n",
      "______________\n",
      "epoch 1 train loss 2.187577247619629\n",
      "val loss 2.1569275856018066\n",
      "______________\n",
      "epoch 2 train loss 2.212770938873291\n",
      "val loss 2.1434311866760254\n",
      "______________\n",
      "epoch 3 train loss 2.170140266418457\n",
      "val loss 2.1301097869873047\n",
      "______________\n",
      "epoch 4 train loss 2.152308940887451\n",
      "val loss 2.1169543266296387\n",
      "______________\n",
      "epoch 5 train loss 2.126622438430786\n",
      "val loss 2.103971004486084\n",
      "______________\n",
      "epoch 6 train loss 2.14467191696167\n",
      "val loss 2.0911693572998047\n",
      "______________\n",
      "epoch 7 train loss 2.1148276329040527\n",
      "val loss 2.0785794258117676\n",
      "______________\n",
      "epoch 8 train loss 2.1283211708068848\n",
      "val loss 2.0662038326263428\n",
      "______________\n",
      "epoch 9 train loss 2.0900354385375977\n",
      "val loss 2.0540413856506348\n",
      "______________\n",
      "epoch 10 train loss 2.062458038330078\n",
      "val loss 2.042065143585205\n",
      "______________\n",
      "epoch 11 train loss 2.0469765663146973\n",
      "val loss 2.030250072479248\n",
      "______________\n",
      "epoch 12 train loss 2.0734148025512695\n",
      "val loss 2.0186238288879395\n",
      "______________\n",
      "epoch 13 train loss 2.0401628017425537\n",
      "val loss 2.0071606636047363\n",
      "______________\n",
      "epoch 14 train loss 2.022751808166504\n",
      "val loss 1.9958993196487427\n",
      "______________\n",
      "epoch 15 train loss 2.0019569396972656\n",
      "val loss 1.9848506450653076\n",
      "______________\n",
      "epoch 16 train loss 2.0266103744506836\n",
      "val loss 1.9739840030670166\n",
      "______________\n",
      "epoch 17 train loss 1.9910788536071777\n",
      "val loss 1.9632949829101562\n",
      "______________\n",
      "epoch 18 train loss 1.9661312103271484\n",
      "val loss 1.9527688026428223\n",
      "______________\n",
      "epoch 19 train loss 1.946643352508545\n",
      "val loss 1.942439079284668\n",
      "______________\n",
      "epoch 20 train loss 1.9441511631011963\n",
      "val loss 1.9322738647460938\n",
      "______________\n",
      "epoch 21 train loss 1.9436063766479492\n",
      "val loss 1.9222657680511475\n",
      "______________\n",
      "epoch 22 train loss 1.9121150970458984\n",
      "val loss 1.9124295711517334\n",
      "______________\n",
      "epoch 23 train loss 1.917093276977539\n",
      "val loss 1.9027419090270996\n",
      "______________\n",
      "epoch 24 train loss 1.922013282775879\n",
      "val loss 1.8932080268859863\n",
      "______________\n",
      "epoch 25 train loss 1.9080921411514282\n",
      "val loss 1.8838448524475098\n",
      "______________\n",
      "epoch 26 train loss 1.8923743963241577\n",
      "val loss 1.874631404876709\n",
      "______________\n",
      "epoch 27 train loss 1.8912488222122192\n",
      "val loss 1.8655714988708496\n",
      "______________\n",
      "epoch 28 train loss 1.8653230667114258\n",
      "val loss 1.85666024684906\n",
      "______________\n",
      "epoch 29 train loss 1.825305700302124\n",
      "val loss 1.8479084968566895\n",
      "______________\n",
      "epoch 30 train loss 1.804072618484497\n",
      "val loss 1.8393168449401855\n",
      "______________\n",
      "epoch 31 train loss 1.876985788345337\n",
      "val loss 1.8308662176132202\n",
      "______________\n",
      "epoch 32 train loss 1.8141090869903564\n",
      "val loss 1.8225666284561157\n",
      "______________\n",
      "epoch 33 train loss 1.827746033668518\n",
      "val loss 1.8144116401672363\n",
      "______________\n",
      "epoch 34 train loss 1.8106091022491455\n",
      "val loss 1.806410312652588\n",
      "______________\n",
      "epoch 35 train loss 1.7736680507659912\n",
      "val loss 1.7985765933990479\n",
      "______________\n",
      "epoch 36 train loss 1.7797551155090332\n",
      "val loss 1.790881872177124\n",
      "______________\n",
      "epoch 37 train loss 1.7868926525115967\n",
      "val loss 1.783315658569336\n",
      "______________\n",
      "epoch 38 train loss 1.7456638813018799\n",
      "val loss 1.7758822441101074\n",
      "______________\n",
      "epoch 39 train loss 1.7623411417007446\n",
      "val loss 1.7685706615447998\n",
      "______________\n",
      "epoch 40 train loss 1.763558030128479\n",
      "val loss 1.7613877058029175\n",
      "______________\n",
      "epoch 41 train loss 1.7503485679626465\n",
      "val loss 1.754335641860962\n",
      "______________\n",
      "epoch 42 train loss 1.7344248294830322\n",
      "val loss 1.7474170923233032\n",
      "______________\n",
      "epoch 43 train loss 1.7306370735168457\n",
      "val loss 1.740644097328186\n",
      "______________\n",
      "epoch 44 train loss 1.7073426246643066\n",
      "val loss 1.73399019241333\n",
      "______________\n",
      "epoch 45 train loss 1.7026844024658203\n",
      "val loss 1.727449893951416\n",
      "______________\n",
      "epoch 46 train loss 1.699489712715149\n",
      "val loss 1.7210181951522827\n",
      "______________\n",
      "epoch 47 train loss 1.6745880842208862\n",
      "val loss 1.714707612991333\n",
      "______________\n",
      "epoch 48 train loss 1.6816112995147705\n",
      "val loss 1.7085089683532715\n",
      "______________\n",
      "epoch 49 train loss 1.6986321210861206\n",
      "val loss 1.7024116516113281\n",
      "______________\n",
      "epoch 50 train loss 1.6814144849777222\n",
      "val loss 1.6964263916015625\n",
      "______________\n",
      "epoch 51 train loss 1.673215389251709\n",
      "val loss 1.6905412673950195\n",
      "______________\n",
      "epoch 52 train loss 1.6435478925704956\n",
      "val loss 1.6847608089447021\n",
      "______________\n",
      "epoch 53 train loss 1.6412636041641235\n",
      "val loss 1.6790788173675537\n",
      "______________\n",
      "epoch 54 train loss 1.66840398311615\n",
      "val loss 1.6734907627105713\n",
      "______________\n",
      "epoch 55 train loss 1.6555182933807373\n",
      "val loss 1.6679987907409668\n",
      "______________\n",
      "epoch 56 train loss 1.6599401235580444\n",
      "val loss 1.6626008749008179\n",
      "______________\n",
      "epoch 57 train loss 1.648267149925232\n",
      "val loss 1.6573126316070557\n",
      "______________\n",
      "epoch 58 train loss 1.6293003559112549\n",
      "val loss 1.6521143913269043\n",
      "______________\n",
      "epoch 59 train loss 1.6048038005828857\n",
      "val loss 1.6470062732696533\n",
      "______________\n",
      "epoch 60 train loss 1.6385042667388916\n",
      "val loss 1.6419878005981445\n",
      "______________\n",
      "epoch 61 train loss 1.595162034034729\n",
      "val loss 1.6370582580566406\n",
      "______________\n",
      "epoch 62 train loss 1.6039234399795532\n",
      "val loss 1.6322274208068848\n",
      "______________\n",
      "epoch 63 train loss 1.5831682682037354\n",
      "val loss 1.6274778842926025\n",
      "______________\n",
      "epoch 64 train loss 1.5837631225585938\n",
      "val loss 1.622808575630188\n",
      "______________\n",
      "epoch 65 train loss 1.5882798433303833\n",
      "val loss 1.6182076930999756\n",
      "______________\n",
      "epoch 66 train loss 1.5646708011627197\n",
      "val loss 1.6136817932128906\n",
      "______________\n",
      "epoch 67 train loss 1.5590654611587524\n",
      "val loss 1.6092259883880615\n",
      "______________\n",
      "epoch 68 train loss 1.5514459609985352\n",
      "val loss 1.6048270463943481\n",
      "______________\n",
      "epoch 69 train loss 1.5745762586593628\n",
      "val loss 1.600511074066162\n",
      "______________\n",
      "epoch 70 train loss 1.5740036964416504\n",
      "val loss 1.5962626934051514\n",
      "______________\n",
      "epoch 71 train loss 1.5619962215423584\n",
      "val loss 1.592083215713501\n",
      "______________\n",
      "epoch 72 train loss 1.5543177127838135\n",
      "val loss 1.5879535675048828\n",
      "______________\n",
      "epoch 73 train loss 1.5468807220458984\n",
      "val loss 1.5838897228240967\n",
      "______________\n",
      "epoch 74 train loss 1.5233709812164307\n",
      "val loss 1.5798982381820679\n",
      "______________\n",
      "epoch 75 train loss 1.545464277267456\n",
      "val loss 1.5759625434875488\n",
      "______________\n",
      "epoch 76 train loss 1.5163229703903198\n",
      "val loss 1.5720759630203247\n",
      "______________\n",
      "epoch 77 train loss 1.514268159866333\n",
      "val loss 1.568266749382019\n",
      "______________\n",
      "epoch 78 train loss 1.5157809257507324\n",
      "val loss 1.5645265579223633\n",
      "______________\n",
      "epoch 79 train loss 1.5002652406692505\n",
      "val loss 1.5608363151550293\n",
      "______________\n",
      "epoch 80 train loss 1.494968056678772\n",
      "val loss 1.5572165250778198\n",
      "______________\n",
      "epoch 81 train loss 1.4846519231796265\n",
      "val loss 1.5536444187164307\n",
      "______________\n",
      "epoch 82 train loss 1.51555335521698\n",
      "val loss 1.550124168395996\n",
      "______________\n",
      "epoch 83 train loss 1.4949004650115967\n",
      "val loss 1.5466594696044922\n",
      "______________\n",
      "epoch 84 train loss 1.4802584648132324\n",
      "val loss 1.543247938156128\n",
      "______________\n",
      "epoch 85 train loss 1.4767496585845947\n",
      "val loss 1.5399115085601807\n",
      "______________\n",
      "epoch 86 train loss 1.4531973600387573\n",
      "val loss 1.5366294384002686\n",
      "______________\n",
      "epoch 87 train loss 1.4668245315551758\n",
      "val loss 1.533402442932129\n",
      "______________\n",
      "epoch 88 train loss 1.4791189432144165\n",
      "val loss 1.5302374362945557\n",
      "______________\n",
      "epoch 89 train loss 1.4566185474395752\n",
      "val loss 1.5271167755126953\n",
      "______________\n",
      "epoch 90 train loss 1.44550621509552\n",
      "val loss 1.5240577459335327\n",
      "______________\n",
      "epoch 91 train loss 1.4616206884384155\n",
      "val loss 1.5210403203964233\n",
      "______________\n",
      "epoch 92 train loss 1.4712398052215576\n",
      "val loss 1.5180556774139404\n",
      "______________\n",
      "epoch 93 train loss 1.4390161037445068\n",
      "val loss 1.5151150226593018\n",
      "______________\n",
      "epoch 94 train loss 1.4484786987304688\n",
      "val loss 1.512223243713379\n",
      "______________\n",
      "epoch 95 train loss 1.4167250394821167\n",
      "val loss 1.5093731880187988\n",
      "______________\n",
      "epoch 96 train loss 1.4290121793746948\n",
      "val loss 1.5065630674362183\n",
      "______________\n",
      "epoch 97 train loss 1.417175531387329\n",
      "val loss 1.5038036108016968\n",
      "______________\n",
      "epoch 98 train loss 1.4488626718521118\n",
      "val loss 1.5010638236999512\n",
      "______________\n",
      "epoch 99 train loss 1.4464737176895142\n",
      "val loss 1.4983851909637451\n",
      "______________\n",
      "epoch 100 train loss 1.405637502670288\n",
      "val loss 1.4957423210144043\n",
      "______________\n",
      "epoch 101 train loss 1.3958834409713745\n",
      "val loss 1.4931265115737915\n",
      "______________\n",
      "epoch 102 train loss 1.4259434938430786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 1.4905436038970947\n",
      "______________\n",
      "epoch 103 train loss 1.4160832166671753\n",
      "val loss 1.4880157709121704\n",
      "______________\n",
      "epoch 104 train loss 1.408632516860962\n",
      "val loss 1.485527753829956\n",
      "______________\n",
      "epoch 105 train loss 1.4423518180847168\n",
      "val loss 1.4830715656280518\n",
      "______________\n",
      "epoch 106 train loss 1.3963558673858643\n",
      "val loss 1.4806634187698364\n",
      "______________\n",
      "epoch 107 train loss 1.4090056419372559\n",
      "val loss 1.47830331325531\n",
      "______________\n",
      "epoch 108 train loss 1.368709683418274\n",
      "val loss 1.4759641885757446\n",
      "______________\n",
      "epoch 109 train loss 1.397509217262268\n",
      "val loss 1.4736557006835938\n",
      "______________\n",
      "epoch 110 train loss 1.3965606689453125\n",
      "val loss 1.4713646173477173\n",
      "______________\n",
      "epoch 111 train loss 1.3788970708847046\n",
      "val loss 1.4691144227981567\n",
      "______________\n",
      "epoch 112 train loss 1.3915494680404663\n",
      "val loss 1.4668760299682617\n",
      "______________\n",
      "epoch 113 train loss 1.3736129999160767\n",
      "val loss 1.4646542072296143\n",
      "______________\n",
      "epoch 114 train loss 1.4062837362289429\n",
      "val loss 1.4624583721160889\n",
      "______________\n",
      "epoch 115 train loss 1.3828747272491455\n",
      "val loss 1.46028470993042\n",
      "______________\n",
      "epoch 116 train loss 1.3774815797805786\n",
      "val loss 1.4581278562545776\n",
      "______________\n",
      "epoch 117 train loss 1.3956453800201416\n",
      "val loss 1.4560070037841797\n",
      "______________\n",
      "epoch 118 train loss 1.3835105895996094\n",
      "val loss 1.4538955688476562\n",
      "______________\n",
      "epoch 119 train loss 1.369621992111206\n",
      "val loss 1.4518084526062012\n",
      "______________\n",
      "epoch 120 train loss 1.357975721359253\n",
      "val loss 1.4497394561767578\n",
      "______________\n",
      "epoch 121 train loss 1.3566746711730957\n",
      "val loss 1.4477107524871826\n",
      "______________\n",
      "epoch 122 train loss 1.350039005279541\n",
      "val loss 1.445701241493225\n",
      "______________\n",
      "epoch 123 train loss 1.3509609699249268\n",
      "val loss 1.4437235593795776\n",
      "______________\n",
      "epoch 124 train loss 1.3402104377746582\n",
      "val loss 1.4417755603790283\n",
      "______________\n",
      "epoch 125 train loss 1.350054383277893\n",
      "val loss 1.4398438930511475\n",
      "______________\n",
      "epoch 126 train loss 1.3390061855316162\n",
      "val loss 1.437937617301941\n",
      "______________\n",
      "epoch 127 train loss 1.357157826423645\n",
      "val loss 1.4360395669937134\n",
      "______________\n",
      "epoch 128 train loss 1.3163931369781494\n",
      "val loss 1.4341723918914795\n",
      "______________\n",
      "epoch 129 train loss 1.3605670928955078\n",
      "val loss 1.432323932647705\n",
      "______________\n",
      "epoch 130 train loss 1.3156079053878784\n",
      "val loss 1.4304993152618408\n",
      "______________\n",
      "epoch 131 train loss 1.3415170907974243\n",
      "val loss 1.4286963939666748\n",
      "______________\n",
      "epoch 132 train loss 1.3167256116867065\n",
      "val loss 1.4269176721572876\n",
      "______________\n",
      "epoch 133 train loss 1.3192400932312012\n",
      "val loss 1.4251737594604492\n",
      "______________\n",
      "epoch 134 train loss 1.3387411832809448\n",
      "val loss 1.4234530925750732\n",
      "______________\n",
      "epoch 135 train loss 1.3256332874298096\n",
      "val loss 1.4217644929885864\n",
      "______________\n",
      "epoch 136 train loss 1.3180627822875977\n",
      "val loss 1.4200962781906128\n",
      "______________\n",
      "epoch 137 train loss 1.2950975894927979\n",
      "val loss 1.418463945388794\n",
      "______________\n",
      "epoch 138 train loss 1.3067795038223267\n",
      "val loss 1.4168481826782227\n",
      "______________\n",
      "epoch 139 train loss 1.3141673803329468\n",
      "val loss 1.4152506589889526\n",
      "______________\n",
      "epoch 140 train loss 1.3086462020874023\n",
      "val loss 1.4136707782745361\n",
      "______________\n",
      "epoch 141 train loss 1.320792555809021\n",
      "val loss 1.4121012687683105\n",
      "______________\n",
      "epoch 142 train loss 1.3284826278686523\n",
      "val loss 1.410544753074646\n",
      "______________\n",
      "epoch 143 train loss 1.3149383068084717\n",
      "val loss 1.409011721611023\n",
      "______________\n",
      "epoch 144 train loss 1.277690052986145\n",
      "val loss 1.4074925184249878\n",
      "______________\n",
      "epoch 145 train loss 1.302578330039978\n",
      "val loss 1.4059990644454956\n",
      "______________\n",
      "epoch 146 train loss 1.3017343282699585\n",
      "val loss 1.404526710510254\n",
      "______________\n",
      "epoch 147 train loss 1.2972347736358643\n",
      "val loss 1.4030792713165283\n",
      "______________\n",
      "epoch 148 train loss 1.3042205572128296\n",
      "val loss 1.4016684293746948\n",
      "______________\n",
      "epoch 149 train loss 1.2898651361465454\n",
      "val loss 1.400260329246521\n",
      "______________\n",
      "epoch 150 train loss 1.3159648180007935\n",
      "val loss 1.398848295211792\n",
      "______________\n",
      "epoch 151 train loss 1.2876156568527222\n",
      "val loss 1.3974467515945435\n",
      "______________\n",
      "epoch 152 train loss 1.2800241708755493\n",
      "val loss 1.396069884300232\n",
      "______________\n",
      "epoch 153 train loss 1.2538011074066162\n",
      "val loss 1.3947136402130127\n",
      "______________\n",
      "epoch 154 train loss 1.2686941623687744\n",
      "val loss 1.3933703899383545\n",
      "______________\n",
      "epoch 155 train loss 1.2640173435211182\n",
      "val loss 1.3920453786849976\n",
      "______________\n",
      "epoch 156 train loss 1.2954150438308716\n",
      "val loss 1.390738844871521\n",
      "______________\n",
      "epoch 157 train loss 1.2906742095947266\n",
      "val loss 1.3894495964050293\n",
      "______________\n",
      "epoch 158 train loss 1.2695378065109253\n",
      "val loss 1.3881785869598389\n",
      "______________\n",
      "epoch 159 train loss 1.2726259231567383\n",
      "val loss 1.3869190216064453\n",
      "______________\n",
      "epoch 160 train loss 1.2789652347564697\n",
      "val loss 1.3856782913208008\n",
      "______________\n",
      "epoch 161 train loss 1.3123682737350464\n",
      "val loss 1.3844561576843262\n",
      "______________\n",
      "epoch 162 train loss 1.288057804107666\n",
      "val loss 1.3832486867904663\n",
      "______________\n",
      "epoch 163 train loss 1.265954613685608\n",
      "val loss 1.3820629119873047\n",
      "______________\n",
      "epoch 164 train loss 1.2752734422683716\n",
      "val loss 1.3808952569961548\n",
      "______________\n",
      "epoch 165 train loss 1.2673413753509521\n",
      "val loss 1.379760503768921\n",
      "______________\n",
      "epoch 166 train loss 1.2435858249664307\n",
      "val loss 1.3786306381225586\n",
      "______________\n",
      "epoch 167 train loss 1.2583582401275635\n",
      "val loss 1.377509593963623\n",
      "______________\n",
      "epoch 168 train loss 1.2673745155334473\n",
      "val loss 1.3763948678970337\n",
      "______________\n",
      "epoch 169 train loss 1.2271009683609009\n",
      "val loss 1.375302791595459\n",
      "______________\n",
      "epoch 170 train loss 1.260645866394043\n",
      "val loss 1.3742202520370483\n",
      "______________\n",
      "epoch 171 train loss 1.2602877616882324\n",
      "val loss 1.373142123222351\n",
      "______________\n",
      "epoch 172 train loss 1.243431568145752\n",
      "val loss 1.3720616102218628\n",
      "______________\n",
      "epoch 173 train loss 1.2653822898864746\n",
      "val loss 1.3709936141967773\n",
      "______________\n",
      "epoch 174 train loss 1.2406961917877197\n",
      "val loss 1.3699349164962769\n",
      "______________\n",
      "epoch 175 train loss 1.262523889541626\n",
      "val loss 1.368890643119812\n",
      "______________\n",
      "epoch 176 train loss 1.2587370872497559\n",
      "val loss 1.3678648471832275\n",
      "______________\n",
      "epoch 177 train loss 1.2523857355117798\n",
      "val loss 1.3668484687805176\n",
      "______________\n",
      "epoch 178 train loss 1.2510673999786377\n",
      "val loss 1.3658487796783447\n",
      "______________\n",
      "epoch 179 train loss 1.237115502357483\n",
      "val loss 1.3648474216461182\n",
      "______________\n",
      "epoch 180 train loss 1.2334251403808594\n",
      "val loss 1.3638591766357422\n",
      "______________\n",
      "epoch 181 train loss 1.2299823760986328\n",
      "val loss 1.3628878593444824\n",
      "______________\n",
      "epoch 182 train loss 1.2166821956634521\n",
      "val loss 1.3619318008422852\n",
      "______________\n",
      "epoch 183 train loss 1.2177335023880005\n",
      "val loss 1.3609840869903564\n",
      "______________\n",
      "epoch 184 train loss 1.2400829792022705\n",
      "val loss 1.3600561618804932\n",
      "______________\n",
      "epoch 185 train loss 1.2416260242462158\n",
      "val loss 1.3591424226760864\n",
      "______________\n",
      "epoch 186 train loss 1.219602346420288\n",
      "val loss 1.3582189083099365\n",
      "______________\n",
      "epoch 187 train loss 1.2406800985336304\n",
      "val loss 1.3572934865951538\n",
      "______________\n",
      "epoch 188 train loss 1.2126119136810303\n",
      "val loss 1.3563635349273682\n",
      "______________\n",
      "epoch 189 train loss 1.2464385032653809\n",
      "val loss 1.3554446697235107\n",
      "______________\n",
      "epoch 190 train loss 1.192545771598816\n",
      "val loss 1.3545238971710205\n",
      "______________\n",
      "epoch 191 train loss 1.2001922130584717\n",
      "val loss 1.3536102771759033\n",
      "______________\n",
      "epoch 192 train loss 1.219290018081665\n",
      "val loss 1.35271418094635\n",
      "______________\n",
      "epoch 193 train loss 1.2075347900390625\n",
      "val loss 1.3518381118774414\n",
      "______________\n",
      "epoch 194 train loss 1.2297953367233276\n",
      "val loss 1.3509907722473145\n",
      "______________\n",
      "epoch 195 train loss 1.1752288341522217\n",
      "val loss 1.3501620292663574\n",
      "______________\n",
      "epoch 196 train loss 1.2098398208618164\n",
      "val loss 1.349364995956421\n",
      "______________\n",
      "epoch 197 train loss 1.1905982494354248\n",
      "val loss 1.3485774993896484\n",
      "______________\n",
      "epoch 198 train loss 1.210198163986206\n",
      "val loss 1.3477858304977417\n",
      "______________\n",
      "epoch 199 train loss 1.2109793424606323\n",
      "val loss 1.3469980955123901\n",
      "______________\n",
      "epoch 200 train loss 1.214832067489624\n",
      "val loss 1.346217393875122\n",
      "______________\n",
      "epoch 201 train loss 1.200829267501831\n",
      "val loss 1.3454461097717285\n",
      "______________\n",
      "epoch 202 train loss 1.2164640426635742\n",
      "val loss 1.3446886539459229\n",
      "______________\n",
      "epoch 203 train loss 1.224375605583191\n",
      "val loss 1.3439284563064575\n",
      "______________\n",
      "epoch 204 train loss 1.2021851539611816\n",
      "val loss 1.343152403831482\n",
      "______________\n",
      "epoch 205 train loss 1.1902661323547363\n",
      "val loss 1.3423842191696167\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 206 train loss 1.1793824434280396\n",
      "val loss 1.3416240215301514\n",
      "______________\n",
      "epoch 207 train loss 1.1762053966522217\n",
      "val loss 1.3408727645874023\n",
      "______________\n",
      "epoch 208 train loss 1.202772855758667\n",
      "val loss 1.3401318788528442\n",
      "______________\n",
      "epoch 209 train loss 1.2178564071655273\n",
      "val loss 1.3393971920013428\n",
      "______________\n",
      "epoch 210 train loss 1.204088807106018\n",
      "val loss 1.3386671543121338\n",
      "______________\n",
      "epoch 211 train loss 1.2195024490356445\n",
      "val loss 1.337947130203247\n",
      "______________\n",
      "epoch 212 train loss 1.2011961936950684\n",
      "val loss 1.3372386693954468\n",
      "______________\n",
      "epoch 213 train loss 1.2096660137176514\n",
      "val loss 1.3365379571914673\n",
      "______________\n",
      "epoch 214 train loss 1.2016918659210205\n",
      "val loss 1.3358380794525146\n",
      "______________\n",
      "epoch 215 train loss 1.177890419960022\n",
      "val loss 1.3351479768753052\n",
      "______________\n",
      "epoch 216 train loss 1.1805782318115234\n",
      "val loss 1.334464430809021\n",
      "______________\n",
      "epoch 217 train loss 1.1675283908843994\n",
      "val loss 1.3337929248809814\n",
      "______________\n",
      "epoch 218 train loss 1.1761064529418945\n",
      "val loss 1.3331185579299927\n",
      "______________\n",
      "epoch 219 train loss 1.1678911447525024\n",
      "val loss 1.3324716091156006\n",
      "______________\n",
      "epoch 220 train loss 1.1745376586914062\n",
      "val loss 1.3318220376968384\n",
      "______________\n",
      "epoch 221 train loss 1.1846318244934082\n",
      "val loss 1.3311874866485596\n",
      "______________\n",
      "epoch 222 train loss 1.1911693811416626\n",
      "val loss 1.330542802810669\n",
      "______________\n",
      "epoch 223 train loss 1.157496690750122\n",
      "val loss 1.3298883438110352\n",
      "______________\n",
      "epoch 224 train loss 1.1915240287780762\n",
      "val loss 1.32924485206604\n",
      "______________\n",
      "epoch 225 train loss 1.176038384437561\n",
      "val loss 1.3286397457122803\n",
      "______________\n",
      "epoch 226 train loss 1.1674449443817139\n",
      "val loss 1.3280510902404785\n",
      "______________\n",
      "epoch 227 train loss 1.1680055856704712\n",
      "val loss 1.3274829387664795\n",
      "______________\n",
      "epoch 228 train loss 1.1557106971740723\n",
      "val loss 1.3269188404083252\n",
      "______________\n",
      "epoch 229 train loss 1.1945838928222656\n",
      "val loss 1.3263468742370605\n",
      "______________\n",
      "epoch 230 train loss 1.1783931255340576\n",
      "val loss 1.3257815837860107\n",
      "______________\n",
      "epoch 231 train loss 1.165202260017395\n",
      "val loss 1.32522451877594\n",
      "______________\n",
      "epoch 232 train loss 1.201316475868225\n",
      "val loss 1.324664831161499\n",
      "______________\n",
      "epoch 233 train loss 1.1800963878631592\n",
      "val loss 1.3241007328033447\n",
      "______________\n",
      "epoch 234 train loss 1.178048014640808\n",
      "val loss 1.3235297203063965\n",
      "______________\n",
      "epoch 235 train loss 1.1573576927185059\n",
      "val loss 1.322962760925293\n",
      "______________\n",
      "epoch 236 train loss 1.1553709506988525\n",
      "val loss 1.322394847869873\n",
      "______________\n",
      "epoch 237 train loss 1.1739819049835205\n",
      "val loss 1.3218519687652588\n",
      "______________\n",
      "epoch 238 train loss 1.1611050367355347\n",
      "val loss 1.3212852478027344\n",
      "______________\n",
      "epoch 239 train loss 1.1793590784072876\n",
      "val loss 1.3207244873046875\n",
      "______________\n",
      "epoch 240 train loss 1.1333304643630981\n",
      "val loss 1.3201942443847656\n",
      "______________\n",
      "epoch 241 train loss 1.1383618116378784\n",
      "val loss 1.319688320159912\n",
      "______________\n",
      "epoch 242 train loss 1.136831521987915\n",
      "val loss 1.3191814422607422\n",
      "______________\n",
      "epoch 243 train loss 1.1370937824249268\n",
      "val loss 1.3186779022216797\n",
      "______________\n",
      "epoch 244 train loss 1.1472585201263428\n",
      "val loss 1.3181931972503662\n",
      "______________\n",
      "epoch 245 train loss 1.1465985774993896\n",
      "val loss 1.317710280418396\n",
      "______________\n",
      "epoch 246 train loss 1.1395403146743774\n",
      "val loss 1.3172235488891602\n",
      "______________\n",
      "epoch 247 train loss 1.1291723251342773\n",
      "val loss 1.3167493343353271\n",
      "______________\n",
      "epoch 248 train loss 1.1545641422271729\n",
      "val loss 1.3162763118743896\n",
      "______________\n",
      "epoch 249 train loss 1.1478533744812012\n",
      "val loss 1.3158245086669922\n",
      "______________\n",
      "epoch 250 train loss 1.1280107498168945\n",
      "val loss 1.3153754472732544\n",
      "______________\n",
      "epoch 251 train loss 1.128538727760315\n",
      "val loss 1.3149232864379883\n",
      "______________\n",
      "epoch 252 train loss 1.164159893989563\n",
      "val loss 1.314484715461731\n",
      "______________\n",
      "epoch 253 train loss 1.1387765407562256\n",
      "val loss 1.314035415649414\n",
      "______________\n",
      "epoch 254 train loss 1.140896201133728\n",
      "val loss 1.3135788440704346\n",
      "______________\n",
      "epoch 255 train loss 1.1147983074188232\n",
      "val loss 1.3131206035614014\n",
      "______________\n",
      "epoch 256 train loss 1.1236510276794434\n",
      "val loss 1.3126747608184814\n",
      "______________\n",
      "epoch 257 train loss 1.1462382078170776\n",
      "val loss 1.312240719795227\n",
      "______________\n",
      "epoch 258 train loss 1.1416089534759521\n",
      "val loss 1.31180739402771\n",
      "______________\n",
      "epoch 259 train loss 1.0994666814804077\n",
      "val loss 1.3113782405853271\n",
      "______________\n",
      "epoch 260 train loss 1.1769018173217773\n",
      "val loss 1.3109495639801025\n",
      "______________\n",
      "epoch 261 train loss 1.1566096544265747\n",
      "val loss 1.3105602264404297\n",
      "______________\n",
      "epoch 262 train loss 1.1255056858062744\n",
      "val loss 1.3101767301559448\n",
      "______________\n",
      "epoch 263 train loss 1.1442592144012451\n",
      "val loss 1.3097959756851196\n",
      "______________\n",
      "epoch 264 train loss 1.1259751319885254\n",
      "val loss 1.3094024658203125\n",
      "______________\n",
      "epoch 265 train loss 1.1270861625671387\n",
      "val loss 1.3089908361434937\n",
      "______________\n",
      "epoch 266 train loss 1.1243736743927002\n",
      "val loss 1.308589220046997\n",
      "______________\n",
      "epoch 267 train loss 1.159569263458252\n",
      "val loss 1.3081774711608887\n",
      "______________\n",
      "epoch 268 train loss 1.099316120147705\n",
      "val loss 1.3077561855316162\n",
      "______________\n",
      "epoch 269 train loss 1.1179225444793701\n",
      "val loss 1.307367205619812\n",
      "______________\n",
      "epoch 270 train loss 1.110067367553711\n",
      "val loss 1.306969165802002\n",
      "______________\n",
      "epoch 271 train loss 1.1149028539657593\n",
      "val loss 1.3065736293792725\n",
      "______________\n",
      "epoch 272 train loss 1.120177984237671\n",
      "val loss 1.306188702583313\n",
      "______________\n",
      "epoch 273 train loss 1.117826223373413\n",
      "val loss 1.3058290481567383\n",
      "______________\n",
      "epoch 274 train loss 1.1077194213867188\n",
      "val loss 1.305475115776062\n",
      "______________\n",
      "epoch 275 train loss 1.1453189849853516\n",
      "val loss 1.3051345348358154\n",
      "______________\n",
      "epoch 276 train loss 1.130696177482605\n",
      "val loss 1.3047822713851929\n",
      "______________\n",
      "epoch 277 train loss 1.1467193365097046\n",
      "val loss 1.3044154644012451\n",
      "______________\n",
      "epoch 278 train loss 1.0989935398101807\n",
      "val loss 1.304070234298706\n",
      "______________\n",
      "epoch 279 train loss 1.1216018199920654\n",
      "val loss 1.3037388324737549\n",
      "______________\n",
      "epoch 280 train loss 1.125454306602478\n",
      "val loss 1.3034138679504395\n",
      "______________\n",
      "epoch 281 train loss 1.117647409439087\n",
      "val loss 1.303086519241333\n",
      "______________\n",
      "epoch 282 train loss 1.1220837831497192\n",
      "val loss 1.3027535676956177\n",
      "______________\n",
      "epoch 283 train loss 1.1440380811691284\n",
      "val loss 1.3024239540100098\n",
      "______________\n",
      "epoch 284 train loss 1.0981101989746094\n",
      "val loss 1.3020939826965332\n",
      "______________\n",
      "epoch 285 train loss 1.1151618957519531\n",
      "val loss 1.3017544746398926\n",
      "______________\n",
      "epoch 286 train loss 1.105591058731079\n",
      "val loss 1.3013970851898193\n",
      "______________\n",
      "epoch 287 train loss 1.0952503681182861\n",
      "val loss 1.3010618686676025\n",
      "______________\n",
      "epoch 288 train loss 1.0763803720474243\n",
      "val loss 1.3007352352142334\n",
      "______________\n",
      "epoch 289 train loss 1.0865602493286133\n",
      "val loss 1.3004050254821777\n",
      "______________\n",
      "epoch 290 train loss 1.0873439311981201\n",
      "val loss 1.3000845909118652\n",
      "______________\n",
      "epoch 291 train loss 1.082650065422058\n",
      "val loss 1.2997733354568481\n",
      "______________\n",
      "epoch 292 train loss 1.1017876863479614\n",
      "val loss 1.2994729280471802\n",
      "______________\n",
      "epoch 293 train loss 1.1318984031677246\n",
      "val loss 1.2991943359375\n",
      "______________\n",
      "epoch 294 train loss 1.1179914474487305\n",
      "val loss 1.2989091873168945\n",
      "______________\n",
      "epoch 295 train loss 1.1001421213150024\n",
      "val loss 1.2986195087432861\n",
      "______________\n",
      "epoch 296 train loss 1.097166657447815\n",
      "val loss 1.298361897468567\n",
      "______________\n",
      "epoch 297 train loss 1.1096420288085938\n",
      "val loss 1.2980890274047852\n",
      "______________\n",
      "epoch 298 train loss 1.1076996326446533\n",
      "val loss 1.297821044921875\n",
      "______________\n",
      "epoch 299 train loss 1.0705353021621704\n",
      "val loss 1.2975624799728394\n",
      "______________\n",
      "epoch 300 train loss 1.0824029445648193\n",
      "val loss 1.2973312139511108\n",
      "______________\n",
      "epoch 301 train loss 1.082268476486206\n",
      "val loss 1.297109842300415\n",
      "______________\n",
      "epoch 302 train loss 1.08154296875\n",
      "val loss 1.2968854904174805\n",
      "______________\n",
      "epoch 303 train loss 1.084423303604126\n",
      "val loss 1.2966727018356323\n",
      "______________\n",
      "epoch 304 train loss 1.0810796022415161\n",
      "val loss 1.29647696018219\n",
      "______________\n",
      "epoch 305 train loss 1.1125364303588867\n",
      "val loss 1.2962872982025146\n",
      "______________\n",
      "epoch 306 train loss 1.1055339574813843\n",
      "val loss 1.2960827350616455\n",
      "______________\n",
      "epoch 307 train loss 1.0932062864303589\n",
      "val loss 1.2958697080612183\n",
      "______________\n",
      "epoch 308 train loss 1.092258095741272\n",
      "val loss 1.2956550121307373\n",
      "______________\n",
      "epoch 309 train loss 1.0869076251983643\n",
      "val loss 1.2954463958740234\n",
      "______________\n",
      "epoch 310 train loss 1.1011269092559814\n",
      "val loss 1.2952470779418945\n",
      "______________\n",
      "epoch 311 train loss 1.0996172428131104\n",
      "val loss 1.2950363159179688\n",
      "______________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 312 train loss 1.073913335800171\n",
      "val loss 1.2947970628738403\n",
      "______________\n",
      "epoch 313 train loss 1.091803789138794\n",
      "val loss 1.294551968574524\n",
      "______________\n",
      "epoch 314 train loss 1.0972635746002197\n",
      "val loss 1.2943083047866821\n",
      "______________\n",
      "epoch 315 train loss 1.106309175491333\n",
      "val loss 1.2940764427185059\n",
      "______________\n",
      "epoch 316 train loss 1.1151673793792725\n",
      "val loss 1.293856143951416\n",
      "______________\n",
      "epoch 317 train loss 1.0560593605041504\n",
      "val loss 1.293648362159729\n",
      "______________\n",
      "epoch 318 train loss 1.0739567279815674\n",
      "val loss 1.2934504747390747\n",
      "______________\n",
      "epoch 319 train loss 1.0756667852401733\n",
      "val loss 1.2932730913162231\n",
      "______________\n",
      "epoch 320 train loss 1.089507818222046\n",
      "val loss 1.2930877208709717\n",
      "______________\n",
      "epoch 321 train loss 1.0701841115951538\n",
      "val loss 1.2929078340530396\n",
      "______________\n",
      "epoch 322 train loss 1.0558733940124512\n",
      "val loss 1.2927379608154297\n",
      "______________\n",
      "epoch 323 train loss 1.0734230279922485\n",
      "val loss 1.2925739288330078\n",
      "______________\n",
      "epoch 324 train loss 1.0683472156524658\n",
      "val loss 1.2924230098724365\n",
      "______________\n",
      "epoch 325 train loss 1.0709197521209717\n",
      "val loss 1.2922792434692383\n",
      "______________\n",
      "epoch 326 train loss 1.0936119556427002\n",
      "val loss 1.2921383380889893\n",
      "______________\n",
      "epoch 327 train loss 1.0851950645446777\n",
      "val loss 1.2919851541519165\n",
      "______________\n",
      "epoch 328 train loss 1.0639431476593018\n",
      "val loss 1.2918323278427124\n",
      "______________\n",
      "epoch 329 train loss 1.0555565357208252\n",
      "val loss 1.291686773300171\n",
      "______________\n",
      "epoch 330 train loss 1.0740689039230347\n",
      "val loss 1.2915420532226562\n",
      "______________\n",
      "epoch 331 train loss 1.0521275997161865\n",
      "val loss 1.2913867235183716\n",
      "______________\n",
      "epoch 332 train loss 1.0538675785064697\n",
      "val loss 1.291240930557251\n",
      "______________\n",
      "epoch 333 train loss 1.058880090713501\n",
      "val loss 1.2910822629928589\n",
      "______________\n",
      "epoch 334 train loss 1.06827974319458\n",
      "val loss 1.2909119129180908\n",
      "______________\n",
      "epoch 335 train loss 1.0770466327667236\n",
      "val loss 1.2907319068908691\n",
      "______________\n",
      "epoch 336 train loss 1.0712549686431885\n",
      "val loss 1.2905560731887817\n",
      "______________\n",
      "epoch 337 train loss 1.0671193599700928\n",
      "val loss 1.2903990745544434\n",
      "______________\n",
      "epoch 338 train loss 1.0748772621154785\n",
      "val loss 1.2902559041976929\n",
      "______________\n",
      "epoch 339 train loss 1.0714726448059082\n",
      "val loss 1.290107011795044\n",
      "______________\n",
      "epoch 340 train loss 1.0300573110580444\n",
      "val loss 1.2899532318115234\n",
      "______________\n",
      "epoch 341 train loss 1.0816009044647217\n",
      "val loss 1.2897939682006836\n",
      "______________\n",
      "epoch 342 train loss 1.059725046157837\n",
      "val loss 1.289625883102417\n",
      "______________\n",
      "epoch 343 train loss 1.0581090450286865\n",
      "val loss 1.2894728183746338\n",
      "______________\n",
      "epoch 344 train loss 1.049528956413269\n",
      "val loss 1.2893121242523193\n",
      "______________\n",
      "epoch 345 train loss 1.0716843605041504\n",
      "val loss 1.2891290187835693\n",
      "______________\n",
      "epoch 346 train loss 1.0557295083999634\n",
      "val loss 1.2889642715454102\n",
      "______________\n",
      "epoch 347 train loss 1.0651767253875732\n",
      "val loss 1.2887980937957764\n",
      "______________\n",
      "epoch 348 train loss 1.051936149597168\n",
      "val loss 1.2886388301849365\n",
      "______________\n",
      "epoch 349 train loss 1.075547218322754\n",
      "val loss 1.2884855270385742\n",
      "______________\n",
      "epoch 350 train loss 1.0428141355514526\n",
      "val loss 1.2883180379867554\n",
      "______________\n",
      "epoch 351 train loss 1.064197301864624\n",
      "val loss 1.2881498336791992\n",
      "______________\n",
      "epoch 352 train loss 1.0694761276245117\n",
      "val loss 1.2879869937896729\n",
      "______________\n",
      "epoch 353 train loss 1.0479404926300049\n",
      "val loss 1.28782320022583\n",
      "______________\n",
      "epoch 354 train loss 1.0367152690887451\n",
      "val loss 1.2876746654510498\n",
      "______________\n",
      "epoch 355 train loss 1.039846658706665\n",
      "val loss 1.2875349521636963\n",
      "______________\n",
      "epoch 356 train loss 1.0422694683074951\n",
      "val loss 1.2874038219451904\n",
      "______________\n",
      "epoch 357 train loss 1.0559194087982178\n",
      "val loss 1.2872734069824219\n",
      "______________\n",
      "epoch 358 train loss 1.047702431678772\n",
      "val loss 1.2871544361114502\n",
      "______________\n",
      "epoch 359 train loss 1.0629899501800537\n",
      "val loss 1.287048101425171\n",
      "______________\n",
      "epoch 360 train loss 1.0632059574127197\n",
      "val loss 1.2869306802749634\n",
      "______________\n",
      "epoch 361 train loss 1.0541216135025024\n",
      "val loss 1.2868422269821167\n",
      "______________\n",
      "epoch 362 train loss 1.0645122528076172\n",
      "val loss 1.2867547273635864\n",
      "______________\n",
      "epoch 363 train loss 1.039498209953308\n",
      "val loss 1.2866593599319458\n",
      "______________\n",
      "epoch 364 train loss 1.0725929737091064\n",
      "val loss 1.286582589149475\n",
      "______________\n",
      "epoch 365 train loss 1.042006015777588\n",
      "val loss 1.286461591720581\n",
      "______________\n",
      "epoch 366 train loss 1.053274154663086\n",
      "val loss 1.2863352298736572\n",
      "______________\n",
      "epoch 367 train loss 1.0336400270462036\n",
      "val loss 1.2861965894699097\n",
      "______________\n",
      "epoch 368 train loss 1.06661057472229\n",
      "val loss 1.286078691482544\n",
      "______________\n",
      "epoch 369 train loss 1.0525751113891602\n",
      "val loss 1.2859430313110352\n",
      "______________\n",
      "epoch 370 train loss 1.0366226434707642\n",
      "val loss 1.2858178615570068\n",
      "______________\n",
      "epoch 371 train loss 1.023072600364685\n",
      "val loss 1.2856857776641846\n",
      "______________\n",
      "epoch 372 train loss 1.045973777770996\n",
      "val loss 1.2855507135391235\n",
      "______________\n",
      "epoch 373 train loss 1.0393633842468262\n",
      "val loss 1.2854220867156982\n",
      "______________\n",
      "epoch 374 train loss 1.0196584463119507\n",
      "val loss 1.2853052616119385\n",
      "______________\n",
      "epoch 375 train loss 1.0236693620681763\n",
      "val loss 1.2851951122283936\n",
      "______________\n",
      "epoch 376 train loss 1.0329786539077759\n",
      "val loss 1.285090684890747\n",
      "______________\n",
      "epoch 377 train loss 1.0515841245651245\n",
      "val loss 1.2849833965301514\n",
      "______________\n",
      "epoch 378 train loss 1.012044906616211\n",
      "val loss 1.2848838567733765\n",
      "______________\n",
      "epoch 379 train loss 1.0367497205734253\n",
      "val loss 1.2847988605499268\n",
      "______________\n",
      "epoch 380 train loss 1.0596692562103271\n",
      "val loss 1.2847192287445068\n",
      "______________\n",
      "epoch 381 train loss 1.0259068012237549\n",
      "val loss 1.2846380472183228\n",
      "______________\n",
      "epoch 382 train loss 1.0279769897460938\n",
      "val loss 1.2845654487609863\n",
      "______________\n",
      "epoch 383 train loss 1.0240803956985474\n",
      "val loss 1.2844911813735962\n",
      "______________\n",
      "epoch 384 train loss 1.0085759162902832\n",
      "val loss 1.284436583518982\n",
      "______________\n",
      "epoch 385 train loss 1.0473006963729858\n",
      "val loss 1.284401297569275\n",
      "______________\n",
      "epoch 386 train loss 1.0271154642105103\n",
      "val loss 1.284359097480774\n",
      "______________\n",
      "epoch 387 train loss 1.0308291912078857\n",
      "val loss 1.2843103408813477\n",
      "______________\n",
      "epoch 388 train loss 1.014292597770691\n",
      "val loss 1.2842562198638916\n",
      "______________\n",
      "epoch 389 train loss 1.0327033996582031\n",
      "val loss 1.2842153310775757\n",
      "______________\n",
      "epoch 390 train loss 1.0168451070785522\n",
      "val loss 1.2841790914535522\n",
      "______________\n",
      "epoch 391 train loss 1.0303975343704224\n",
      "val loss 1.284115195274353\n",
      "______________\n",
      "epoch 392 train loss 1.0122873783111572\n",
      "val loss 1.2840332984924316\n",
      "______________\n",
      "epoch 393 train loss 1.008149266242981\n",
      "val loss 1.28395414352417\n",
      "______________\n",
      "epoch 394 train loss 1.0251469612121582\n",
      "val loss 1.2838670015335083\n",
      "______________\n",
      "epoch 395 train loss 1.0303384065628052\n",
      "val loss 1.2837803363800049\n",
      "______________\n",
      "epoch 396 train loss 1.0128302574157715\n",
      "val loss 1.283686876296997\n",
      "______________\n",
      "epoch 397 train loss 1.029693841934204\n",
      "val loss 1.28360116481781\n",
      "______________\n",
      "epoch 398 train loss 1.0500540733337402\n",
      "val loss 1.2835140228271484\n",
      "______________\n",
      "epoch 399 train loss 1.032001256942749\n",
      "val loss 1.2834408283233643\n",
      "______________\n",
      "epoch 400 train loss 1.0242328643798828\n",
      "val loss 1.2833716869354248\n",
      "______________\n",
      "epoch 401 train loss 1.0247758626937866\n",
      "val loss 1.2833030223846436\n",
      "______________\n",
      "epoch 402 train loss 1.0312519073486328\n",
      "val loss 1.2832387685775757\n",
      "______________\n",
      "epoch 403 train loss 1.0106377601623535\n",
      "val loss 1.2831695079803467\n",
      "______________\n",
      "epoch 404 train loss 1.012209415435791\n",
      "val loss 1.2831082344055176\n",
      "______________\n",
      "epoch 405 train loss 1.0352805852890015\n",
      "val loss 1.2830623388290405\n",
      "______________\n",
      "epoch 406 train loss 1.0289661884307861\n",
      "val loss 1.2830313444137573\n",
      "______________\n",
      "epoch 407 train loss 1.0104178190231323\n",
      "val loss 1.2830225229263306\n",
      "______________\n",
      "epoch 408 train loss 0.995093822479248\n",
      "val loss 1.283011794090271\n",
      "______________\n",
      "epoch 409 train loss 1.0083810091018677\n",
      "val loss 1.2829673290252686\n",
      "______________\n",
      "epoch 410 train loss 1.014957070350647\n",
      "val loss 1.282934546470642\n",
      "______________\n",
      "epoch 411 train loss 1.025625228881836\n",
      "val loss 1.2828954458236694\n",
      "______________\n",
      "epoch 412 train loss 0.9977521896362305\n",
      "val loss 1.2828757762908936\n",
      "______________\n",
      "epoch 413 train loss 1.0157227516174316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 1.2828675508499146\n",
      "______________\n",
      "epoch 414 train loss 1.0388920307159424\n",
      "val loss 1.2828478813171387\n",
      "______________\n",
      "epoch 415 train loss 1.0145819187164307\n",
      "val loss 1.2828361988067627\n",
      "______________\n",
      "epoch 416 train loss 1.0266978740692139\n",
      "val loss 1.2827904224395752\n",
      "______________\n",
      "epoch 417 train loss 1.00140380859375\n",
      "val loss 1.2827345132827759\n",
      "______________\n",
      "epoch 418 train loss 1.0093512535095215\n",
      "val loss 1.282717227935791\n",
      "______________\n",
      "epoch 419 train loss 1.012784719467163\n",
      "val loss 1.2827187776565552\n",
      "______________\n",
      "epoch 420 train loss 1.0083370208740234\n",
      "val loss 1.2827190160751343\n",
      "______________\n",
      "epoch 421 train loss 0.9897663593292236\n",
      "val loss 1.2827210426330566\n",
      "______________\n",
      "epoch 422 train loss 1.0184508562088013\n",
      "val loss 1.2827229499816895\n",
      "______________\n",
      "epoch 423 train loss 0.9804401397705078\n",
      "val loss 1.2827154397964478\n",
      "______________\n",
      "epoch 424 train loss 0.9961447715759277\n",
      "val loss 1.282726764678955\n",
      "______________\n",
      "epoch 425 train loss 1.0275061130523682\n",
      "val loss 1.2827301025390625\n",
      "______________\n",
      "epoch 426 train loss 0.9948256015777588\n",
      "val loss 1.282754898071289\n",
      "______________\n",
      "epoch 427 train loss 1.0099117755889893\n",
      "val loss 1.2827842235565186\n",
      "______________\n",
      "epoch 428 train loss 0.9937073588371277\n",
      "val loss 1.2828198671340942\n",
      "______________\n",
      "epoch 429 train loss 0.9836242198944092\n",
      "val loss 1.2828598022460938\n",
      "______________\n",
      "epoch 430 train loss 0.9994308948516846\n",
      "val loss 1.2828954458236694\n",
      "______________\n",
      "epoch 431 train loss 0.9953392148017883\n",
      "val loss 1.2829244136810303\n",
      "______________\n",
      "epoch 432 train loss 1.0247597694396973\n",
      "val loss 1.2829524278640747\n",
      "______________\n",
      "epoch 433 train loss 1.0015637874603271\n",
      "val loss 1.283012866973877\n",
      "______________\n",
      "epoch 434 train loss 0.9945155382156372\n",
      "val loss 1.2830493450164795\n",
      "______________\n",
      "best loss 1.2827154397964478 {'Overall Survival (4 Years)': {'accuracy': -1, 'mse': 0.093628734, 'auc': 0.7206730769230769}, 'FT': {'accuracy': -1, 'mse': 0.17477857, 'auc': 0.6332236842105263}, 'Aspiration rate Post-therapy': {'accuracy': -1, 'mse': 0.1330121, 'auc': 0.7387820512820512}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EndpointSimulator(\n",
       "  (input_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=83, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (batchnorm): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (outcome_layer): Linear(in_features=500, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = train_state(state=3,epochs=10000,lr=.0001)\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "98c4db7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statedecisions_input30_dims500_dropout0,0.05'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecisionModel(SimulatorBase):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 baseline_input_size,#number of baseline features used\n",
    "                 hidden_layers = [500],\n",
    "                 dropout = 0.05,\n",
    "                 input_dropout=0,\n",
    "                 state = 1,\n",
    "                 eps = 0.01,\n",
    "                 ):\n",
    "        #input will be all states up until treatment 3\n",
    "        input_size = baseline_input_size  + len(Const.dlt1) + len(Const.primary_disease_states)  + len(Const.nodal_disease_states)  + len(Const.ccs)  + len(Const.modifications) + 2\n",
    "            \n",
    "        super(DecisionModel,self).__init__(input_size,hidden_layers=hidden_layers,dropout=dropout,input_dropout=input_dropout,eps=eps,state='decisions')\n",
    "        self.final_layer = torch.nn.Linear(hidden_layers[-1],len(Const.decisions))\n",
    "\n",
    "#         self.final_layer = torch.nn.Linear(hidden_layers[-1],1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def add_position_token(self,x,position):\n",
    "        #add 2 binary variables for if the state has already passed\n",
    "        if position == 0:\n",
    "            token = torch.zeros((x.shape[0],2))\n",
    "            x = torch.cat([x,token],dim=1)\n",
    "        if position == 1:\n",
    "            token1 = torch.ones((x.shape[0],1))\n",
    "            token2 = torch.zeros((x.shape[0],1))\n",
    "            x = torch.cat([x,token1,token2],dim=1)\n",
    "        if position == 2:\n",
    "            token1 = torch.zeros((x.shape[0],1))\n",
    "            token2 = torch.ones((x.shape[0],1))\n",
    "            x = torch.cat([x,token1,token2],dim=1)\n",
    "        if position == 3:\n",
    "            token1 = torch.ones((x.shape[0],1))\n",
    "            token2 = torch.ones((x.shape[0],1))\n",
    "            x = torch.cat([x,token1,token2],dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_embedding(self,xbase,xdlt,xpd,xnd,xcc,xmod,position=0):\n",
    "        xbase = self.normalize(xbase)\n",
    "        x = torch.cat([xbase,xdlt,xpd,xnd,xcc,xmod],dim=1)\n",
    "        x = self.add_position_token(x,position)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,xbase,xdlt,xpd,xnd,xcc,xmod,position=0):\n",
    "        #position is 0-2\n",
    "#         [xbase, xdlt, xpd, xnd, xcc,xmod] = x\n",
    "        xbase = self.normalize(xbase)\n",
    "        x = torch.cat([xbase,xdlt,xpd,xnd,xcc,xmod],dim=1)\n",
    "        x = self.input_dropout(x)\n",
    "        x = self.add_position_token(x,position)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "test = DecisionModel(3)\n",
    "test.identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dcaa4176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8709, 0.1051, 0.9089, 0.3796, 0.1637],\n",
      "        [0.9283, 0.7037, 0.7089, 0.0402, 0.7564],\n",
      "        [0.7775, 0.7739, 0.6718, 0.3050, 0.9293]])\n",
      "tensor([[0.8709, 0.1051, 0.9089, 0.0402, 0.1637],\n",
      "        [0.9283, 0.7037, 0.7089, 0.3796, 0.7564],\n",
      "        [0.7775, 0.7739, 0.6718, 0.3050, 0.9293]])\n"
     ]
    }
   ],
   "source": [
    "def shuffle_col(v,col=None):\n",
    "    if col is None:\n",
    "        col = np.random.choice([i for i in range(v.shape[1])])\n",
    "    idx = torch.randperm(v.shape[0])\n",
    "    vv = torch.clone(v)\n",
    "    vv[:,col] = vv[idx,col]\n",
    "    return vv\n",
    "\n",
    "\n",
    "test = torch.rand((3,5))\n",
    "print(test)\n",
    "print(shuffle_col(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5c4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______epoch 0 _____\n",
      "imitation 1.9935460090637207 reward 1.1700947284698486\n",
      "imitation 1.9789232015609741 reward 1.1610314846038818\n",
      "3.1399545669555664 1000000000.0\n",
      "[{'decision': 0, 'accuracy': 0.5684931506849316, 'auc': 0.5355769230769231}, {'decision': 1, 'accuracy': 0.6095890410958904, 'auc': 0.4673793859649123}, {'decision': 2, 'accuracy': 0.8082191780821918, 'auc': 0.5173076923076924}]\n",
      "______epoch 1 _____\n",
      "imitation 1.9583206176757812 reward 1.169677495956421\n",
      "imitation 1.9481981992721558 reward 1.160551905632019\n",
      "3.108750104904175 3.1399545669555664\n",
      "[{'decision': 0, 'accuracy': 0.5958904109589042, 'auc': 0.539423076923077}, {'decision': 1, 'accuracy': 0.678082191780822, 'auc': 0.4723135964912281}, {'decision': 2, 'accuracy': 0.8013698630136986, 'auc': 0.5230769230769231}]\n",
      "______epoch 2 _____\n",
      "imitation 1.9349699020385742 reward 1.1691067218780518\n",
      "imitation 1.91902494430542 reward 1.1600759029388428\n",
      "3.0791008472442627 3.108750104904175\n",
      "[{'decision': 0, 'accuracy': 0.6027397260273972, 'auc': 0.5413461538461539}, {'decision': 1, 'accuracy': 0.6986301369863014, 'auc': 0.4824561403508772}, {'decision': 2, 'accuracy': 0.815068493150685, 'auc': 0.5282051282051282}]\n",
      "______epoch 3 _____\n",
      "imitation 1.8981871604919434 reward 1.1685620546340942\n",
      "imitation 1.8914209604263306 reward 1.1596049070358276\n",
      "3.051025867462158 3.0791008472442627\n",
      "[{'decision': 0, 'accuracy': 0.636986301369863, 'auc': 0.5413461538461538}, {'decision': 1, 'accuracy': 0.7397260273972602, 'auc': 0.4917763157894737}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5358974358974359}]\n",
      "______epoch 4 _____\n",
      "imitation 1.8726909160614014 reward 1.168187141418457\n",
      "imitation 1.8653417825698853 reward 1.1591393947601318\n",
      "3.0244812965393066 3.051025867462158\n",
      "[{'decision': 0, 'accuracy': 0.6438356164383562, 'auc': 0.5423076923076923}, {'decision': 1, 'accuracy': 0.7397260273972602, 'auc': 0.5016447368421053}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5429487179487179}]\n",
      "______epoch 5 _____\n",
      "imitation 1.841133713722229 reward 1.1677250862121582\n",
      "imitation 1.8407838344573975 reward 1.1586809158325195\n",
      "2.999464750289917 3.0244812965393066\n",
      "[{'decision': 0, 'accuracy': 0.6575342465753424, 'auc': 0.5451923076923078}, {'decision': 1, 'accuracy': 0.7465753424657534, 'auc': 0.5115131578947368}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.548076923076923}]\n",
      "______epoch 6 _____\n",
      "imitation 1.8091214895248413 reward 1.1671446561813354\n",
      "imitation 1.8177199363708496 reward 1.1582297086715698\n",
      "2.975949764251709 2.999464750289917\n",
      "[{'decision': 0, 'accuracy': 0.6643835616438356, 'auc': 0.5451923076923078}, {'decision': 1, 'accuracy': 0.7534246575342466, 'auc': 0.5172697368421053}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5512820512820512}]\n",
      "______epoch 7 _____\n",
      "imitation 1.786829948425293 reward 1.1666247844696045\n",
      "imitation 1.7960751056671143 reward 1.1577863693237305\n",
      "2.9538614749908447 2.975949764251709\n",
      "[{'decision': 0, 'accuracy': 0.6712328767123288, 'auc': 0.5485576923076924}, {'decision': 1, 'accuracy': 0.773972602739726, 'auc': 0.527138157894737}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5532051282051281}]\n",
      "______epoch 8 _____\n",
      "imitation 1.7640997171401978 reward 1.1660866737365723\n",
      "imitation 1.7758433818817139 reward 1.1573522090911865\n",
      "2.9331955909729004 2.9538614749908447\n",
      "[{'decision': 0, 'accuracy': 0.7054794520547946, 'auc': 0.548076923076923}, {'decision': 1, 'accuracy': 0.773972602739726, 'auc': 0.5328947368421053}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5583333333333333}]\n",
      "______epoch 9 _____\n",
      "imitation 1.7462868690490723 reward 1.1657296419143677\n",
      "imitation 1.756983995437622 reward 1.1569271087646484\n",
      "2.9139111042022705 2.9331955909729004\n",
      "[{'decision': 0, 'accuracy': 0.7191780821917808, 'auc': 0.5495192307692307}, {'decision': 1, 'accuracy': 0.773972602739726, 'auc': 0.540296052631579}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5641025641025641}]\n",
      "______epoch 10 _____\n",
      "imitation 1.7233474254608154 reward 1.1653025150299072\n",
      "imitation 1.739418864250183 reward 1.1565109491348267\n",
      "2.8959298133850098 2.9139111042022705\n",
      "[{'decision': 0, 'accuracy': 0.7465753424657534, 'auc': 0.5475961538461538}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5474232456140351}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5653846153846154}]\n",
      "______epoch 11 _____\n",
      "imitation 1.7075474262237549 reward 1.1648457050323486\n",
      "imitation 1.7230956554412842 reward 1.1561055183410645\n",
      "2.8792011737823486 2.8959298133850098\n",
      "[{'decision': 0, 'accuracy': 0.7602739726027398, 'auc': 0.5495192307692307}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5504385964912281}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5692307692307692}]\n",
      "______epoch 12 _____\n",
      "imitation 1.6767079830169678 reward 1.1645472049713135\n",
      "imitation 1.707981824874878 reward 1.1557111740112305\n",
      "2.8636929988861084 2.8792011737823486\n",
      "[{'decision': 0, 'accuracy': 0.7671232876712328, 'auc': 0.55}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5545504385964912}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5717948717948718}]\n",
      "______epoch 13 _____\n",
      "imitation 1.6600592136383057 reward 1.16398024559021\n",
      "imitation 1.6940422058105469 reward 1.1553294658660889\n",
      "2.8493716716766357 2.8636929988861084\n",
      "[{'decision': 0, 'accuracy': 0.7945205479452054, 'auc': 0.5509615384615384}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5594846491228069}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5743589743589743}]\n",
      "______epoch 14 _____\n",
      "imitation 1.6427826881408691 reward 1.163636565208435\n",
      "imitation 1.6811853647232056 reward 1.1549599170684814\n",
      "2.8361454010009766 2.8493716716766357\n",
      "[{'decision': 0, 'accuracy': 0.8287671232876712, 'auc': 0.5490384615384615}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5638706140350878}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.576602564102564}]\n",
      "______epoch 15 _____\n",
      "imitation 1.6285192966461182 reward 1.1632585525512695\n",
      "imitation 1.6693825721740723 reward 1.1546028852462769\n",
      "2.8239855766296387 2.8361454010009766\n",
      "[{'decision': 0, 'accuracy': 0.8493150684931506, 'auc': 0.5514423076923076}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5633223684210527}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5807692307692307}]\n",
      "______epoch 16 _____\n",
      "imitation 1.6127564907073975 reward 1.1629879474639893\n",
      "imitation 1.6585705280303955 reward 1.1542571783065796\n",
      "2.8128275871276855 2.8239855766296387\n",
      "[{'decision': 0, 'accuracy': 0.8561643835616438, 'auc': 0.5548076923076922}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5635964912280702}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5830128205128204}]\n",
      "______epoch 17 _____\n",
      "imitation 1.599839448928833 reward 1.1626942157745361\n",
      "imitation 1.6487067937850952 reward 1.1539239883422852\n",
      "2.80263090133667 2.8128275871276855\n",
      "[{'decision': 0, 'accuracy': 0.863013698630137, 'auc': 0.5576923076923076}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5657894736842106}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5830128205128204}]\n",
      "______epoch 18 _____\n",
      "imitation 1.5848100185394287 reward 1.1621497869491577\n",
      "imitation 1.639723777770996 reward 1.1536037921905518\n",
      "2.793327569961548 2.80263090133667\n",
      "[{'decision': 0, 'accuracy': 0.8561643835616438, 'auc': 0.560576923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5688048245614036}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5842948717948718}]\n",
      "______epoch 19 _____\n",
      "imitation 1.5847132205963135 reward 1.1619688272476196\n",
      "imitation 1.631565809249878 reward 1.1532957553863525\n",
      "2.7848615646362305 2.793327569961548\n",
      "[{'decision': 0, 'accuracy': 0.8561643835616438, 'auc': 0.5624999999999999}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5693530701754386}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5878205128205128}]\n",
      "______epoch 20 _____\n",
      "imitation 1.5582530498504639 reward 1.161636471748352\n",
      "imitation 1.624163031578064 reward 1.1530005931854248\n",
      "2.777163505554199 2.7848615646362305\n",
      "[{'decision': 0, 'accuracy': 0.8698630136986302, 'auc': 0.564423076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5715460526315789}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5903846153846154}]\n",
      "______epoch 21 _____\n",
      "imitation 1.5631420612335205 reward 1.1612379550933838\n",
      "imitation 1.6174731254577637 reward 1.1527185440063477\n",
      "2.7701916694641113 2.777163505554199\n",
      "[{'decision': 0, 'accuracy': 0.8698630136986302, 'auc': 0.5668269230769232}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5745614035087719}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.592948717948718}]\n",
      "______epoch 22 _____\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imitation 1.5459859371185303 reward 1.1611050367355347\n",
      "imitation 1.6114351749420166 reward 1.1524486541748047\n",
      "2.7638838291168213 2.7701916694641113\n",
      "[{'decision': 0, 'accuracy': 0.8698630136986302, 'auc': 0.5682692307692307}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5764802631578947}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.5961538461538461}]\n",
      "______epoch 23 _____\n",
      "imitation 1.5437557697296143 reward 1.1607632637023926\n",
      "imitation 1.6060019731521606 reward 1.152190923690796\n",
      "2.758193016052246 2.7638838291168213\n",
      "[{'decision': 0, 'accuracy': 0.8698630136986302, 'auc': 0.5682692307692309}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5767543859649122}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6000000000000001}]\n",
      "______epoch 24 _____\n",
      "imitation 1.5284197330474854 reward 1.160513162612915\n",
      "imitation 1.6011345386505127 reward 1.1519453525543213\n",
      "2.753079891204834 2.758193016052246\n",
      "[{'decision': 0, 'accuracy': 0.8698630136986302, 'auc': 0.5692307692307692}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5770285087719298}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6038461538461539}]\n",
      "______epoch 25 _____\n",
      "imitation 1.536933183670044 reward 1.1601309776306152\n",
      "imitation 1.5967768430709839 reward 1.1517118215560913\n",
      "2.748488664627075 2.753079891204834\n",
      "[{'decision': 0, 'accuracy': 0.8698630136986302, 'auc': 0.56875}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5775767543859649}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6067307692307693}]\n",
      "______epoch 26 _____\n",
      "imitation 1.5135014057159424 reward 1.1599106788635254\n",
      "imitation 1.5928629636764526 reward 1.1514893770217896\n",
      "2.744352340698242 2.748488664627075\n",
      "[{'decision': 0, 'accuracy': 0.8835616438356164, 'auc': 0.5677884615384616}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5770285087719298}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6070512820512821}]\n",
      "______epoch 27 _____\n",
      "imitation 1.5145654678344727 reward 1.159777283668518\n",
      "imitation 1.5893566608428955 reward 1.151278018951416\n",
      "2.7406346797943115 2.744352340698242\n",
      "[{'decision': 0, 'accuracy': 0.8835616438356164, 'auc': 0.5663461538461539}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5764802631578947}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.608974358974359}]\n",
      "______epoch 28 _____\n",
      "imitation 1.5053950548171997 reward 1.159649133682251\n",
      "imitation 1.5862281322479248 reward 1.1510775089263916\n",
      "2.7373056411743164 2.7406346797943115\n",
      "[{'decision': 0, 'accuracy': 0.8835616438356164, 'auc': 0.5668269230769231}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5773026315789473}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6112179487179488}]\n",
      "______epoch 29 _____\n",
      "imitation 1.494465947151184 reward 1.1593687534332275\n",
      "imitation 1.583419919013977 reward 1.1508874893188477\n",
      "2.734307289123535 2.7373056411743164\n",
      "[{'decision': 0, 'accuracy': 0.8835616438356164, 'auc': 0.5697115384615385}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5778508771929824}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.614423076923077}]\n",
      "______epoch 30 _____\n",
      "imitation 1.4960997104644775 reward 1.1591438055038452\n",
      "imitation 1.5809203386306763 reward 1.1507080793380737\n",
      "2.73162841796875 2.734307289123535\n",
      "[{'decision': 0, 'accuracy': 0.8835616438356164, 'auc': 0.5721153846153847}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5797697368421053}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6150641025641026}]\n",
      "______epoch 31 _____\n",
      "imitation 1.4826226234436035 reward 1.1590166091918945\n",
      "imitation 1.578686237335205 reward 1.1505379676818848\n",
      "2.72922420501709 2.73162841796875\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5716346153846154}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5811403508771931}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6173076923076923}]\n",
      "______epoch 32 _____\n",
      "imitation 1.4943082332611084 reward 1.1587220430374146\n",
      "imitation 1.5766873359680176 reward 1.1503775119781494\n",
      "2.727064847946167 2.72922420501709\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5725961538461539}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5814144736842106}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6189102564102564}]\n",
      "______epoch 33 _____\n",
      "imitation 1.4852205514907837 reward 1.1585962772369385\n",
      "imitation 1.5748945474624634 reward 1.1502259969711304\n",
      "2.7251205444335938 2.727064847946167\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.573076923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5819627192982455}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6192307692307693}]\n",
      "______epoch 34 _____\n",
      "imitation 1.483410120010376 reward 1.1584426164627075\n",
      "imitation 1.5732767581939697 reward 1.1500831842422485\n",
      "2.723360061645508 2.7251205444335938\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5721153846153847}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5825109649122807}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6198717948717949}]\n",
      "______epoch 35 _____\n",
      "imitation 1.4757221937179565 reward 1.1583284139633179\n",
      "imitation 1.571799397468567 reward 1.1499488353729248\n",
      "2.7217483520507812 2.723360061645508\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.573076923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.581140350877193}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6227564102564103}]\n",
      "______epoch 36 _____\n",
      "imitation 1.4703826904296875 reward 1.1581896543502808\n",
      "imitation 1.5704635381698608 reward 1.1498219966888428\n",
      "2.720285415649414 2.7217483520507812\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5725961538461539}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5830592105263158}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6253205128205128}]\n",
      "______epoch 37 _____\n",
      "imitation 1.4697285890579224 reward 1.158000111579895\n",
      "imitation 1.569214105606079 reward 1.149702548980713\n",
      "2.718916654586792 2.720285415649414\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5697115384615385}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5833333333333334}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6282051282051282}]\n",
      "______epoch 38 _____\n",
      "imitation 1.4686239957809448 reward 1.1579009294509888\n",
      "imitation 1.568053960800171 reward 1.1495903730392456\n",
      "2.717644214630127 2.718916654586792\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5706730769230769}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5838815789473685}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6282051282051282}]\n",
      "______epoch 39 _____\n",
      "imitation 1.4569250345230103 reward 1.1578319072723389\n",
      "imitation 1.5670020580291748 reward 1.1494845151901245\n",
      "2.7164864540100098 2.717644214630127\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5711538461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614036}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6294871794871795}]\n",
      "______epoch 40 _____\n",
      "imitation 1.4590096473693848 reward 1.1576732397079468\n",
      "imitation 1.5660150051116943 reward 1.1493855714797974\n",
      "2.7154006958007812 2.7164864540100098\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5692307692307693}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5852521929824561}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6317307692307692}]\n",
      "______epoch 41 _____\n",
      "imitation 1.4544551372528076 reward 1.1576205492019653\n",
      "imitation 1.5650608539581299 reward 1.1492928266525269\n",
      "2.714353561401367 2.7154006958007812\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5721153846153847}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5849780701754387}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6336538461538462}]\n",
      "______epoch 42 _____\n",
      "imitation 1.4575371742248535 reward 1.1575490236282349\n",
      "imitation 1.5641303062438965 reward 1.1492059230804443\n",
      "2.713336229324341 2.714353561401367\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5725961538461538}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6342948717948718}]\n",
      "______epoch 43 _____\n",
      "imitation 1.4565186500549316 reward 1.1573941707611084\n",
      "imitation 1.5632247924804688 reward 1.1491245031356812\n",
      "2.7123494148254395 2.713336229324341\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5745192307692308}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5841557017543858}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6349358974358974}]\n",
      "______epoch 44 _____\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imitation 1.4527946710586548 reward 1.1573429107666016\n",
      "imitation 1.5623259544372559 reward 1.1490483283996582\n",
      "2.711374282836914 2.7123494148254395\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5764423076923078}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6339743589743589}]\n",
      "______epoch 45 _____\n",
      "imitation 1.4497404098510742 reward 1.15727961063385\n",
      "imitation 1.5614336729049683 reward 1.1489769220352173\n",
      "2.7104105949401855 2.711374282836914\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5783653846153847}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5838815789473685}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6333333333333333}]\n",
      "______epoch 46 _____\n",
      "imitation 1.438704013824463 reward 1.1572072505950928\n",
      "imitation 1.5605117082595825 reward 1.148910403251648\n",
      "2.7094221115112305 2.7104105949401855\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5793269230769231}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6336538461538461}]\n",
      "______epoch 47 _____\n",
      "imitation 1.4469999074935913 reward 1.1570231914520264\n",
      "imitation 1.5595502853393555 reward 1.148848533630371\n",
      "2.7083988189697266 2.7094221115112305\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.58125}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5833333333333333}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.635576923076923}]\n",
      "______epoch 48 _____\n",
      "imitation 1.445295810699463 reward 1.1569710969924927\n",
      "imitation 1.5585825443267822 reward 1.148790717124939\n",
      "2.7073731422424316 2.7083988189697266\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5817307692307693}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5827850877192982}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6352564102564102}]\n",
      "______epoch 49 _____\n",
      "imitation 1.435987114906311 reward 1.1569666862487793\n",
      "imitation 1.5575884580612183 reward 1.1487371921539307\n",
      "2.7063255310058594 2.7073731422424316\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5836538461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5830592105263157}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6371794871794871}]\n",
      "______epoch 50 _____\n",
      "imitation 1.4301846027374268 reward 1.1569063663482666\n",
      "imitation 1.5565581321716309 reward 1.1486878395080566\n",
      "2.7052459716796875 2.7063255310058594\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5841346153846154}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5825109649122806}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6371794871794871}]\n",
      "______epoch 51 _____\n",
      "imitation 1.4313814640045166 reward 1.1568775177001953\n",
      "imitation 1.5555124282836914 reward 1.14864182472229\n",
      "2.7041542530059814 2.7052459716796875\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5860576923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5819627192982456}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6381410256410256}]\n",
      "______epoch 52 _____\n",
      "imitation 1.4314823150634766 reward 1.156862735748291\n",
      "imitation 1.5544312000274658 reward 1.14859938621521\n",
      "2.703030586242676 2.7041542530059814\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5879807692307693}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5819627192982456}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.639102564102564}]\n",
      "______epoch 53 _____\n",
      "imitation 1.4252437353134155 reward 1.156785249710083\n",
      "imitation 1.55332350730896 reward 1.148559808731079\n",
      "2.701883316040039 2.703030586242676\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5884615384615385}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5825109649122807}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.641025641025641}]\n",
      "______epoch 54 _____\n",
      "imitation 1.4316341876983643 reward 1.1567349433898926\n",
      "imitation 1.552173137664795 reward 1.1485233306884766\n",
      "2.7006964683532715 2.701883316040039\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5879807692307693}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5819627192982456}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6403846153846154}]\n",
      "______epoch 55 _____\n",
      "imitation 1.432565689086914 reward 1.1566635370254517\n",
      "imitation 1.5509905815124512 reward 1.1484893560409546\n",
      "2.6994800567626953 2.7006964683532715\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5894230769230769}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5830592105263157}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6416666666666667}]\n",
      "______epoch 56 _____\n",
      "imitation 1.4130768775939941 reward 1.1566987037658691\n",
      "imitation 1.5497581958770752 reward 1.1484582424163818\n",
      "2.698216438293457 2.6994800567626953\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5899038461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5830592105263157}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6435897435897435}]\n",
      "______epoch 57 _____\n",
      "imitation 1.4189860820770264 reward 1.156638264656067\n",
      "imitation 1.5485060214996338 reward 1.1484295129776\n",
      "2.6969356536865234 2.698216438293457\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5899038461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5833333333333333}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6442307692307692}]\n",
      "______epoch 58 _____\n",
      "imitation 1.4256908893585205 reward 1.1565836668014526\n",
      "imitation 1.5472335815429688 reward 1.1484031677246094\n",
      "2.695636749267578 2.6969356536865234\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5908653846153846}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614036}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6448717948717949}]\n",
      "______epoch 59 _____\n",
      "imitation 1.414340853691101 reward 1.1566131114959717\n",
      "imitation 1.5459305047988892 reward 1.148378849029541\n",
      "2.6943092346191406 2.695636749267578\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5918269230769231}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5849780701754386}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6461538461538462}]\n",
      "______epoch 60 _____\n",
      "imitation 1.41684889793396 reward 1.1565914154052734\n",
      "imitation 1.5445992946624756 reward 1.1483567953109741\n",
      "2.69295597076416 2.6943092346191406\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5918269230769231}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5847039473684211}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6474358974358975}]\n",
      "______epoch 61 _____\n",
      "imitation 1.4110691547393799 reward 1.1565442085266113\n",
      "imitation 1.5432379245758057 reward 1.1483367681503296\n",
      "2.6915745735168457 2.69295597076416\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5923076923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6480769230769231}]\n",
      "______epoch 62 _____\n",
      "imitation 1.4121367931365967 reward 1.1564699411392212\n",
      "imitation 1.541857361793518 reward 1.1483185291290283\n",
      "2.690176010131836 2.6915745735168457\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5932692307692308}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6493589743589744}]\n",
      "______epoch 63 _____\n",
      "imitation 1.402470588684082 reward 1.1565581560134888\n",
      "imitation 1.540450930595398 reward 1.1483018398284912\n",
      "2.6887526512145996 2.690176010131836\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5951923076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.584703947368421}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6506410256410257}]\n",
      "______epoch 64 _____\n",
      "imitation 1.4070020914077759 reward 1.1564981937408447\n",
      "imitation 1.538988709449768 reward 1.1482869386672974\n",
      "2.6872756481170654 2.6887526512145996\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5971153846153846}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.584703947368421}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6522435897435898}]\n",
      "______epoch 65 _____\n",
      "imitation 1.4056587219238281 reward 1.156423807144165\n",
      "imitation 1.5375163555145264 reward 1.1482734680175781\n",
      "2.6857898235321045 2.6872756481170654\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.5975961538461538}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6528846153846154}]\n",
      "______epoch 66 _____\n",
      "imitation 1.397595763206482 reward 1.1564710140228271\n",
      "imitation 1.5360219478607178 reward 1.148261547088623\n",
      "2.684283494949341 2.6857898235321045\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6004807692307692}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5849780701754387}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.655448717948718}]\n",
      "______epoch 67 _____\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imitation 1.4009910821914673 reward 1.1564899682998657\n",
      "imitation 1.534541130065918 reward 1.1482508182525635\n",
      "2.6827919483184814 2.684283494949341\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6004807692307692}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5858004385964912}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6560897435897436}]\n",
      "______epoch 68 _____\n",
      "imitation 1.387574553489685 reward 1.1564518213272095\n",
      "imitation 1.5330464839935303 reward 1.1482411623001099\n",
      "2.6812877655029297 2.6827919483184814\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6009615384615385}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5866228070175439}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6564102564102565}]\n",
      "______epoch 69 _____\n",
      "imitation 1.3930456638336182 reward 1.1564115285873413\n",
      "imitation 1.5315347909927368 reward 1.1482328176498413\n",
      "2.679767608642578 2.6812877655029297\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6014423076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5863486842105263}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6564102564102564}]\n",
      "______epoch 70 _____\n",
      "imitation 1.3904160261154175 reward 1.1564435958862305\n",
      "imitation 1.5300053358078003 reward 1.1482254266738892\n",
      "2.6782307624816895 2.679767608642578\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6028846153846154}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5858004385964912}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6576923076923077}]\n",
      "______epoch 71 _____\n",
      "imitation 1.3876216411590576 reward 1.156423568725586\n",
      "imitation 1.5284570455551147 reward 1.1482189893722534\n",
      "2.676676034927368 2.6782307624816895\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6038461538461538}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.584703947368421}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6564102564102564}]\n",
      "______epoch 72 _____\n",
      "imitation 1.3895851373672485 reward 1.1563631296157837\n",
      "imitation 1.5268898010253906 reward 1.1482133865356445\n",
      "2.675103187561035 2.676676034927368\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6038461538461539}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5844298245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6557692307692308}]\n",
      "______epoch 73 _____\n",
      "imitation 1.3896042108535767 reward 1.1564139127731323\n",
      "imitation 1.52533757686615 reward 1.148208737373352\n",
      "2.673546314239502 2.675103187561035\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6033653846153847}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5849780701754386}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6551282051282051}]\n",
      "______epoch 74 _____\n",
      "imitation 1.3787041902542114 reward 1.156442642211914\n",
      "imitation 1.5237815380096436 reward 1.148205041885376\n",
      "2.6719865798950195 2.673546314239502\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6024038461538461}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5860745614035088}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6557692307692308}]\n",
      "______epoch 75 _____\n",
      "imitation 1.3737022876739502 reward 1.156449794769287\n",
      "imitation 1.5222011804580688 reward 1.1482019424438477\n",
      "2.670403003692627 2.6719865798950195\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6033653846153846}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5868969298245614}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6560897435897436}]\n",
      "______epoch 76 _____\n",
      "imitation 1.375198483467102 reward 1.156402349472046\n",
      "imitation 1.5206185579299927 reward 1.1481995582580566\n",
      "2.6688179969787598 2.670403003692627\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6048076923076924}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.587171052631579}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.657051282051282}]\n",
      "______epoch 77 _____\n",
      "imitation 1.3717687129974365 reward 1.1564569473266602\n",
      "imitation 1.519020915031433 reward 1.148197889328003\n",
      "2.6672186851501465 2.6688179969787598\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6057692307692308}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5879934210526315}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6580128205128205}]\n",
      "______epoch 78 _____\n",
      "imitation 1.3793079853057861 reward 1.156365990638733\n",
      "imitation 1.5174267292022705 reward 1.1481966972351074\n",
      "2.665623426437378 2.6672186851501465\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6072115384615384}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5888157894736843}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.658974358974359}]\n",
      "______epoch 79 _____\n",
      "imitation 1.3591890335083008 reward 1.1564338207244873\n",
      "imitation 1.5158288478851318 reward 1.1481959819793701\n",
      "2.664024829864502 2.665623426437378\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6091346153846153}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.590186403508772}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.658974358974359}]\n",
      "______epoch 80 _____\n",
      "imitation 1.380520224571228 reward 1.1563776731491089\n",
      "imitation 1.5141773223876953 reward 1.1481961011886597\n",
      "2.6623735427856445 2.664024829864502\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6105769230769231}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5904605263157895}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6599358974358974}]\n",
      "______epoch 81 _____\n",
      "imitation 1.360852837562561 reward 1.1564205884933472\n",
      "imitation 1.5125529766082764 reward 1.1481965780258179\n",
      "2.6607494354248047 2.6623735427856445\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6134615384615385}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.591282894736842}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6612179487179487}]\n",
      "______epoch 82 _____\n",
      "imitation 1.357033610343933 reward 1.1564280986785889\n",
      "imitation 1.5109270811080933 reward 1.1481975317001343\n",
      "2.6591246128082275 2.6607494354248047\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.614423076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5918311403508771}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6621794871794872}]\n",
      "______epoch 83 _____\n",
      "imitation 1.3658050298690796 reward 1.1563656330108643\n",
      "imitation 1.5093350410461426 reward 1.1481987237930298\n",
      "2.657533645629883 2.6591246128082275\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6163461538461539}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5926535087719298}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6637820512820513}]\n",
      "______epoch 84 _____\n",
      "imitation 1.3574035167694092 reward 1.1564451456069946\n",
      "imitation 1.507746696472168 reward 1.1482003927230835\n",
      "2.655947208404541 2.657533645629883\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6168269230769231}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5923793859649122}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6650641025641025}]\n",
      "______epoch 85 _____\n",
      "imitation 1.3445429801940918 reward 1.1565053462982178\n",
      "imitation 1.5061604976654053 reward 1.1482021808624268\n",
      "2.654362678527832 2.655947208404541\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6173076923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5929276315789473}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6641025641025641}]\n",
      "______epoch 86 _____\n",
      "imitation 1.3433126211166382 reward 1.1564898490905762\n",
      "imitation 1.5045907497406006 reward 1.1482040882110596\n",
      "2.65279483795166 2.654362678527832\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6177884615384616}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5940241228070176}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6650641025641026}]\n",
      "______epoch 87 _____\n",
      "imitation 1.3512279987335205 reward 1.156456470489502\n",
      "imitation 1.5030243396759033 reward 1.1482062339782715\n",
      "2.651230573654175 2.65279483795166\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6182692307692308}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5953947368421053}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6650641025641026}]\n",
      "______epoch 88 _____\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imitation 1.3520839214324951 reward 1.1564733982086182\n",
      "imitation 1.5014947652816772 reward 1.1482086181640625\n",
      "2.6497035026550293 2.651230573654175\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6197115384615385}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5964912280701755}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6650641025641026}]\n",
      "______epoch 89 _____\n",
      "imitation 1.345836877822876 reward 1.156463384628296\n",
      "imitation 1.4999616146087646 reward 1.1482112407684326\n",
      "2.6481728553771973 2.6497035026550293\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6201923076923076}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5967653508771931}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6647435897435897}]\n",
      "______epoch 90 _____\n",
      "imitation 1.3436801433563232 reward 1.156445860862732\n",
      "imitation 1.4984498023986816 reward 1.1482139825820923\n",
      "2.6466636657714844 2.6481728553771973\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6211538461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5973135964912281}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6663461538461539}]\n",
      "______epoch 91 _____\n",
      "imitation 1.3360037803649902 reward 1.1564580202102661\n",
      "imitation 1.4969549179077148 reward 1.148216962814331\n",
      "2.645171880722046 2.6466636657714844\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6211538461538461}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.596217105263158}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6669871794871796}]\n",
      "______epoch 92 _____\n",
      "imitation 1.3363088369369507 reward 1.1564898490905762\n",
      "imitation 1.4954854249954224 reward 1.148220181465149\n",
      "2.6437056064605713 2.645171880722046\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6211538461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5967653508771931}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6666666666666667}]\n",
      "______epoch 93 _____\n",
      "imitation 1.3263049125671387 reward 1.1564526557922363\n",
      "imitation 1.4940377473831177 reward 1.1482235193252563\n",
      "2.642261266708374 2.6437056064605713\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6211538461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5967653508771931}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.666025641025641}]\n",
      "______epoch 94 _____\n",
      "imitation 1.3224459886550903 reward 1.1564608812332153\n",
      "imitation 1.4926141500473022 reward 1.1482272148132324\n",
      "2.640841484069824 2.642261266708374\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6235576923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.5978618421052633}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6657051282051282}]\n",
      "______epoch 95 _____\n",
      "imitation 1.3266103267669678 reward 1.1564115285873413\n",
      "imitation 1.4911856651306152 reward 1.1482309103012085\n",
      "2.6394166946411133 2.640841484069824\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6245192307692308}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.597861842105263}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6666666666666666}]\n",
      "______epoch 96 _____\n",
      "imitation 1.3258864879608154 reward 1.1564772129058838\n",
      "imitation 1.4897639751434326 reward 1.1482347249984741\n",
      "2.637998580932617 2.6394166946411133\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6240384615384615}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6000548245614035}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6657051282051283}]\n",
      "______epoch 97 _____\n",
      "imitation 1.3176944255828857 reward 1.15653395652771\n",
      "imitation 1.4883677959442139 reward 1.1482386589050293\n",
      "2.636606454849243 2.637998580932617\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6259615384615386}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6006030701754386}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6657051282051283}]\n",
      "______epoch 98 _____\n",
      "imitation 1.3213493824005127 reward 1.15653395652771\n",
      "imitation 1.4869972467422485 reward 1.1482425928115845\n",
      "2.635239839553833 2.636606454849243\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6269230769230769}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6006030701754386}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6657051282051283}]\n",
      "______epoch 99 _____\n",
      "imitation 1.3263585567474365 reward 1.1565159559249878\n",
      "imitation 1.4856497049331665 reward 1.14824640750885\n",
      "2.6338961124420166 2.635239839553833\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6278846153846154}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6011513157894736}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6666666666666667}]\n",
      "______epoch 100 _____\n",
      "imitation 1.3157620429992676 reward 1.1565496921539307\n",
      "imitation 1.4843144416809082 reward 1.1482503414154053\n",
      "2.6325647830963135 2.6338961124420166\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6298076923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6016995614035087}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6676282051282051}]\n",
      "______epoch 101 _____\n",
      "imitation 1.3176547288894653 reward 1.15645432472229\n",
      "imitation 1.483003854751587 reward 1.1482540369033813\n",
      "2.631258010864258 2.6325647830963135\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6322115384615385}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6025219298245614}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.667948717948718}]\n",
      "______epoch 102 _____\n",
      "imitation 1.3160405158996582 reward 1.156491994857788\n",
      "imitation 1.48170006275177 reward 1.1482579708099365\n",
      "2.629958152770996 2.631258010864258\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6326923076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6030701754385965}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6685897435897435}]\n",
      "______epoch 103 _____\n",
      "imitation 1.3104366064071655 reward 1.1564816236495972\n",
      "imitation 1.480420708656311 reward 1.148261547088623\n",
      "2.6286821365356445 2.629958152770996\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6350961538461539}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6025219298245613}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6689102564102564}]\n",
      "______epoch 104 _____\n",
      "imitation 1.3074581623077393 reward 1.1565860509872437\n",
      "imitation 1.4791538715362549 reward 1.1482651233673096\n",
      "2.6274189949035645 2.6286821365356445\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6379807692307692}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6030701754385965}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6689102564102564}]\n",
      "______epoch 105 _____\n",
      "imitation 1.308406114578247 reward 1.1565687656402588\n",
      "imitation 1.4779061079025269 reward 1.148268461227417\n",
      "2.6261744499206543 2.6274189949035645\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6394230769230769}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6038925438596492}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6692307692307692}]\n",
      "______epoch 106 _____\n",
      "imitation 1.3064630031585693 reward 1.1565589904785156\n",
      "imitation 1.4766658544540405 reward 1.148271918296814\n",
      "2.6249377727508545 2.6261744499206543\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6399038461538461}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6036184210526315}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6701923076923078}]\n",
      "______epoch 107 _____\n",
      "imitation 1.2923346757888794 reward 1.1565983295440674\n",
      "imitation 1.4754366874694824 reward 1.1482751369476318\n",
      "2.6237118244171143 2.6249377727508545\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6423076923076922}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6038925438596492}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6711538461538462}]\n",
      "______epoch 108 _____\n",
      "imitation 1.2955726385116577 reward 1.1566352844238281\n",
      "imitation 1.4742411375045776 reward 1.1482778787612915\n",
      "2.622519016265869 2.6237118244171143\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6437499999999999}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6047149122807016}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6717948717948719}]\n",
      "______epoch 109 _____\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imitation 1.2933346033096313 reward 1.156572699546814\n",
      "imitation 1.4730502367019653 reward 1.1482806205749512\n",
      "2.621330738067627 2.622519016265869\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6451923076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6052631578947368}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.671474358974359}]\n",
      "______epoch 110 _____\n",
      "imitation 1.2957953214645386 reward 1.1565717458724976\n",
      "imitation 1.4718806743621826 reward 1.1482834815979004\n",
      "2.620164155960083 2.621330738067627\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.645673076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6055372807017544}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6708333333333333}]\n",
      "______epoch 111 _____\n",
      "imitation 1.2965202331542969 reward 1.1565231084823608\n",
      "imitation 1.4707262516021729 reward 1.1482864618301392\n",
      "2.6190128326416016 2.620164155960083\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6475961538461539}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6060855263157895}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6711538461538461}]\n",
      "______epoch 112 _____\n",
      "imitation 1.2885591983795166 reward 1.1566284894943237\n",
      "imitation 1.4696085453033447 reward 1.1482892036437988\n",
      "2.6178977489471436 2.6190128326416016\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6485576923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6071820175438596}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6705128205128206}]\n",
      "______epoch 113 _____\n",
      "imitation 1.2897555828094482 reward 1.156629204750061\n",
      "imitation 1.4685081243515015 reward 1.1482917070388794\n",
      "2.616799831390381 2.6178977489471436\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6504807692307693}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6069078947368421}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6708333333333334}]\n",
      "______epoch 114 _____\n",
      "imitation 1.278281807899475 reward 1.1566472053527832\n",
      "imitation 1.4674460887908936 reward 1.14829421043396\n",
      "2.6157402992248535 2.616799831390381\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6524038461538462}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.606359649122807}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6714743589743589}]\n",
      "______epoch 115 _____\n",
      "imitation 1.2850760221481323 reward 1.1566811800003052\n",
      "imitation 1.466423749923706 reward 1.1482961177825928\n",
      "2.614719867706299 2.6157402992248535\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6543269230769231}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6066337719298245}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6711538461538461}]\n",
      "______epoch 116 _____\n",
      "imitation 1.2775007486343384 reward 1.15665602684021\n",
      "imitation 1.46542489528656 reward 1.1482977867126465\n",
      "2.613722801208496 2.614719867706299\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.65625}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6074561403508772}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6714743589743589}]\n",
      "______epoch 117 _____\n",
      "imitation 1.2803285121917725 reward 1.156553864479065\n",
      "imitation 1.4644197225570679 reward 1.1482998132705688\n",
      "2.6127195358276367 2.613722801208496\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6576923076923077}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6099232456140351}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6724358974358975}]\n",
      "______epoch 118 _____\n",
      "imitation 1.2799241542816162 reward 1.1566200256347656\n",
      "imitation 1.4634430408477783 reward 1.1483018398284912\n",
      "2.6117448806762695 2.6127195358276367\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6581730769230769}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6099232456140351}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6724358974358974}]\n",
      "______epoch 119 _____\n",
      "imitation 1.2795348167419434 reward 1.1566153764724731\n",
      "imitation 1.4624775648117065 reward 1.148303747177124\n",
      "2.610781192779541 2.6117448806762695\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6596153846153847}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6101973684210527}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6730769230769231}]\n",
      "______epoch 120 _____\n",
      "imitation 1.2777783870697021 reward 1.1566870212554932\n",
      "imitation 1.4614964723587036 reward 1.1483056545257568\n",
      "2.60980224609375 2.610781192779541\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6605769230769232}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6112938596491229}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6743589743589744}]\n",
      "______epoch 121 _____\n",
      "imitation 1.2865469455718994 reward 1.1565828323364258\n",
      "imitation 1.4605578184127808 reward 1.1483075618743896\n",
      "2.608865261077881 2.60980224609375\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6615384615384616}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6123903508771931}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6743589743589744}]\n",
      "______epoch 122 _____\n",
      "imitation 1.2795007228851318 reward 1.1566309928894043\n",
      "imitation 1.459633469581604 reward 1.1483091115951538\n",
      "2.607942581176758 2.608865261077881\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6629807692307692}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6126644736842106}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6746794871794872}]\n",
      "______epoch 123 _____\n",
      "imitation 1.2643816471099854 reward 1.1566332578659058\n",
      "imitation 1.4587407112121582 reward 1.1483105421066284\n",
      "2.607051372528076 2.607942581176758\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6629807692307692}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6143092105263158}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6759615384615385}]\n",
      "______epoch 124 _____\n",
      "imitation 1.2649924755096436 reward 1.1566232442855835\n",
      "imitation 1.4578629732131958 reward 1.148311734199524\n",
      "2.6061747074127197 2.607051372528076\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6644230769230769}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6151315789473684}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6766025641025641}]\n",
      "______epoch 125 _____\n",
      "imitation 1.2629636526107788 reward 1.1566451787948608\n",
      "imitation 1.4569900035858154 reward 1.148313045501709\n",
      "2.6053030490875244 2.6061747074127197\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6663461538461538}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6148574561403509}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6766025641025641}]\n",
      "______epoch 126 _____\n",
      "imitation 1.2638579607009888 reward 1.1566658020019531\n",
      "imitation 1.456131935119629 reward 1.148314118385315\n",
      "2.6044459342956543 2.6053030490875244\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.666826923076923}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6145833333333334}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.676602564102564}]\n",
      "______epoch 127 _____\n",
      "imitation 1.2588938474655151 reward 1.1566622257232666\n",
      "imitation 1.4553030729293823 reward 1.1483149528503418\n",
      "2.6036181449890137 2.6044459342956543\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6677884615384615}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6156798245614036}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6775641025641026}]\n",
      "______epoch 128 _____\n",
      "imitation 1.2686526775360107 reward 1.1566054821014404\n",
      "imitation 1.454483151435852 reward 1.1483163833618164\n",
      "2.602799415588379 2.6036181449890137\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.66875}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.615953947368421}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.6775641025641026}]\n",
      "______epoch 129 _____\n",
      "imitation 1.2524242401123047 reward 1.1567288637161255\n",
      "imitation 1.453678846359253 reward 1.1483174562454224\n",
      "2.601996421813965 2.602799415588379\n",
      "[{'decision': 0, 'accuracy': 0.8904109589041096, 'auc': 0.6692307692307692}, {'decision': 1, 'accuracy': 0.7808219178082192, 'auc': 0.6167763157894737}, {'decision': 2, 'accuracy': 0.821917808219178, 'auc': 0.676923076923077}]\n",
      "______epoch 130 _____\n",
      "imitation 1.2498031854629517 reward 1.1567249298095703\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def train_decision_model(\n",
    "    tmodel1,\n",
    "    tmodel2,\n",
    "    tmodel3,\n",
    "    use_default_split=True,\n",
    "    use_bagging_split=False,\n",
    "    lr=.0001,\n",
    "    epochs=1000,\n",
    "    patience=100,\n",
    "    weights=[1,1,1], #realtive weight of survival, feeding tube, and aspiration\n",
    "    imitation_weight=1,\n",
    "    shufflecol_chance = 0.1,\n",
    "    reward_weight=10,\n",
    "    split=.7,\n",
    "    resample_training=False,\n",
    "    save_path='../data/models/',\n",
    "    file_suffix='',\n",
    "):\n",
    "    \n",
    "    tmodel1.eval()\n",
    "    tmodel2.eval()\n",
    "    tmodel3.eval()\n",
    "\n",
    "    \n",
    "    train_ids, test_ids = get_tt_split(use_default_split=use_default_split,use_bagging_split=use_bagging_split,resample_training=resample_training)\n",
    "    \n",
    "    dataset = DTDataset()\n",
    "    \n",
    "    data = dataset.processed_df.copy()\n",
    "    \n",
    "    def get_dlt(state):\n",
    "        if state == 2:\n",
    "            return data[Const.dlt2].copy()\n",
    "        d = data[Const.dlt1].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_pd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.primary_disease_states2].copy()\n",
    "        d = data[Const.primary_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_nd(state):\n",
    "        if state == 2:\n",
    "            return data[Const.nodal_disease_states2].copy()\n",
    "        d = data[Const.nodal_disease_states].copy()\n",
    "        if state < 1:\n",
    "            d.values[:,:] = 0\n",
    "        return d\n",
    "    \n",
    "    def get_cc(state):\n",
    "        res = data[Const.ccs].copy()\n",
    "        if state == 1:\n",
    "            res.values[:,:] = np.zeros(res.values.shape)\n",
    "        return res\n",
    "    \n",
    "    def get_mod(state):\n",
    "        res = data[Const.modifications].copy()\n",
    "        return res\n",
    "        \n",
    "    outcomedf = data[Const.outcomes]\n",
    "    baseline = dataset.get_state('baseline')\n",
    "    \n",
    "    def formatdf(d,dids=train_ids):\n",
    "        d = df_to_torch(d.loc[dids])\n",
    "        return d\n",
    "    \n",
    "    def makegrad(v):\n",
    "        if not v.requires_grad:\n",
    "            v.requires_grad=True\n",
    "        return v\n",
    "    \n",
    "    model = DecisionModel(baseline.shape[1])\n",
    "\n",
    "    hashcode = str(hash(','.join([str(i) for i in train_ids])))\n",
    "    \n",
    "    save_file = save_path + 'model_' + model.identifier +'_hash' + hashcode + file_suffix + '.tar'\n",
    "    model.fit_normalizer(df_to_torch(baseline.loc[train_ids]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "    def outcome_loss(ypred):\n",
    "        #convert survival to death\n",
    "        loss = torch.mul(torch.mean(-1*(ypred[:,0] - 1)),weights[0])\n",
    "        for i,weight in enumerate(weights[1:]):\n",
    "            newloss = torch.mean(ypred[:,i])*weight\n",
    "            loss = torch.add(loss,torch.mul(newloss,weight))\n",
    "        return loss\n",
    "    \n",
    "    mse = torch.nn.MSELoss()\n",
    "    nllloss = torch.nn.NLLLoss()\n",
    "    bce = torch.nn.BCELoss()\n",
    "    \n",
    "    def compare_decisions(d1,d2,d3,ids):\n",
    "#         ypred = np.concatenate([dd.cpu().detach().numpy().reshape(-1,1) for dd in [d1,d2,d3]],axis=1)\n",
    "        ytrue = df_to_torch(outcomedf.loc[ids])\n",
    "        dloss = bce(d1.view(-1),ytrue[:,0])\n",
    "        dloss += bce(d2.view(-1),ytrue[:,1])\n",
    "        dloss += bce(d3,view(-1),ytrue[:,2])\n",
    "        return dloss\n",
    "        \n",
    "    def remove_decisions(df):\n",
    "        cols = [c for c in df.columns if c not in Const.decisions ]\n",
    "        ddf = df[cols]\n",
    "        return ddf\n",
    "    \n",
    "    makeinput = lambda step,dids: df_to_torch(remove_decisions(dataset.get_input_state(step=step,ids=dids)))\n",
    "    \n",
    "    def step(train=True):\n",
    "        if train:\n",
    "            model.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            ids = train_ids\n",
    "        else:\n",
    "            ids = test_ids\n",
    "            model.eval()\n",
    "            \n",
    "        xx1 = makeinput(1,ids)\n",
    "        xx2 = makeinput(2,ids)\n",
    "        xx3 = makeinput(3,ids)\n",
    "        ytrain = df_to_torch(outcomedf.loc[ids])\n",
    "\n",
    "        baseline_train_base = formatdf(baseline,ids)\n",
    "        xxtrain = [baseline, get_dlt(0),get_pd(0),get_nd(0),get_cc(0),get_mod(0)]\n",
    "        xxtrain = [formatdf(xx,ids) for xx in xxtrain]\n",
    "            \n",
    "        baseline_train = torch.clone(baseline_train_base)\n",
    "        if train and shufflecol_chance > 0.0001:\n",
    "            for col in range(baseline_train_base.shape[1]): \n",
    "                if np.random.random() < shufflecol_chance:\n",
    "                    baseline_train = shuffle_col(baseline_train,col)\n",
    "            \n",
    "            \n",
    "        decision1 = model(*xxtrain,position=0)[:,0]\n",
    "        imitation_loss1 = bce(decision1,ytrain[:,0])\n",
    "\n",
    "        xi1 = torch.cat([xx1,decision1.view(-1,1)],axis=1)\n",
    "        [ypd1, ynd1, ymod, ydlt1] = tmodel1(xi1)\n",
    "        x1 = [baseline_train,ydlt1,ypd1,ynd1,formatdf(get_cc(1),ids),ymod]\n",
    "            \n",
    "        decision2 = model(*x1,position=1)[:,1]\n",
    "        imitation_loss2 =  bce(decision2,ytrain[:,1])\n",
    "\n",
    "        xi2 = torch.cat([xx2,decision1.view(-1,1),decision2.view(-1,1)],axis=1)\n",
    "        [ypd2,ynd2,ycc,ydlt2] = tmodel2(xi2)\n",
    "        x2 = [baseline_train,ydlt2,ypd2,ynd2,ycc,ymod]\n",
    "            \n",
    "        decision3 = model(*x2,position=2)[:,2]\n",
    "        imitation_loss3 = bce(decision3,ytrain[:,2])\n",
    "        \n",
    "        xi3 = torch.cat([xx3,decision1.view(-1,1),decision2.view(-1,1),decision3.view(-1,1)],axis=1)\n",
    "        outcomes = tmodel3(xi3)\n",
    "\n",
    "        reward_loss = outcome_loss(outcomes)\n",
    "        loss = torch.add(imitation_loss1,imitation_loss2)\n",
    "        loss = torch.add(loss,imitation_loss3)\n",
    "        loss = torch.mul(loss,imitation_weight/3)\n",
    "        loss = torch.add(loss,torch.mul(reward_loss,reward_weight))\n",
    "        losses = [imitation_loss1+imitation_loss2+imitation_loss3,reward_loss]\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return losses\n",
    "        else:\n",
    "            scores = []\n",
    "            for i,decision in enumerate([decision1,decision2,decision3]):\n",
    "                dec = decision.cpu().detach().numpy()\n",
    "                dec0 = (dec > .5).astype(int)\n",
    "                out = ytrain[:,i].cpu().detach().numpy()\n",
    "                acc = accuracy_score(out,dec > .5)\n",
    "                auc = roc_auc_score(out,dec)\n",
    "                scores.append({'decision': i,'accuracy': acc,'auc': auc})\n",
    "            return losses, scores\n",
    "        \n",
    "    best_val_loss = torch.tensor(1000000000.0)\n",
    "    steps_since_improvement = 0\n",
    "    best_val_score = {}\n",
    "    for epoch in range(epochs):\n",
    "        print('______epoch',str(epoch),'_____')\n",
    "        losses = step(True)\n",
    "        print('imitation',losses[0].item(),'reward',losses[1].item())\n",
    "        val_losses,val_metrics = step(False)\n",
    "        vl = val_losses[0] + val_losses[1]\n",
    "        print('imitation',val_losses[0].item(),'reward',val_losses[1].item())\n",
    "        print(vl.item(),best_val_loss.item())\n",
    "        print(val_metrics)\n",
    "        if vl < best_val_loss:\n",
    "            best_val_loss = vl\n",
    "            best_val_score = val_metrics\n",
    "            steps_since_improvement = 0\n",
    "            torch.save(model.state_dict(),save_file)\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    print('++++++++++Final+++++++++++')\n",
    "    print('best',best_val_loss)\n",
    "    print(best_val_score)\n",
    "    model.load_state_dict(torch.load(save_file))\n",
    "    model.eval()\n",
    "    return model, best_val_score\n",
    "\n",
    "decision_model, _ = train_decision_model(model,model2,model3,imitation_weight=.1,reward_weight=1,lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6481203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DtypeWarning: Columns (55) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.3672],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.4814],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.5516],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0022,  0.4200],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0062,  0.4519],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.5110]],\n",
       "        dtype=torch.float64, grad_fn=<MulBackward0>),\n",
       " tensor([[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "         ...,\n",
       "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "         [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "         [-0., -0., -0.,  ..., -0., -0., -0.]], dtype=torch.float64,\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]], dtype=torch.float64, grad_fn=<MulBackward0>),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]], dtype=torch.float64, grad_fn=<MulBackward0>),\n",
       " tensor([[ 0.0000,  0.0000, -0.0048,  0.0000],\n",
       "         [ 0.0000,  0.0049, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0025, -0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0028,  0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0051, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0038, -0.0000,  0.0000]], dtype=torch.float64,\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0125, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0097, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0087, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0122, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        dtype=torch.float64, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "ig = IntegratedGradients(decision_model)\n",
    "ds = DTDataset()\n",
    "states = DTDataset().get_states()\n",
    "x = [states['baseline'],states['dlt1'],states['pd_states1'],states['nd_states1'],states['ccs'],states['modifications']]\n",
    "x = tuple([df_to_torch(xx) for xx in x])\n",
    "base = tuple([torch.zeros(xx.shape,requires_grad=True) for xx in x])\n",
    "attributions = ig.attribute(x,base,target=1,method='gausslegendre')\n",
    "attributions\n",
    "# test = pd.DataFrame(attributions,columns = xtestdf.columns, index=xtestdf.index)\n",
    "# test.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9d970691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1A</th>\n",
       "      <th>1A1B</th>\n",
       "      <th>1A6</th>\n",
       "      <th>1B</th>\n",
       "      <th>1B2A</th>\n",
       "      <th>1B3</th>\n",
       "      <th>2A</th>\n",
       "      <th>2A2B</th>\n",
       "      <th>2A3</th>\n",
       "      <th>2B</th>\n",
       "      <th>...</th>\n",
       "      <th>ln_cluster_3</th>\n",
       "      <th>ln_cluster_4</th>\n",
       "      <th>packs_per_year</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>subsite_BOT</th>\n",
       "      <th>subsite_GPS</th>\n",
       "      <th>subsite_NOS</th>\n",
       "      <th>subsite_Soft palate</th>\n",
       "      <th>subsite_Tonsil</th>\n",
       "      <th>total_dose</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>66.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536 rows  62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1A  1A1B  1A6   1B  1B2A  1B3   2A  2A2B  2A3   2B  ...  ln_cluster_3  \\\n",
       "id                                                          ...                 \n",
       "3      0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  1.0  1.0  ...             0   \n",
       "5      0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...             0   \n",
       "6      0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  1.0  1.0  ...             0   \n",
       "7      0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  1.0  1.0  ...             0   \n",
       "8      0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  0.0  1.0  ...             0   \n",
       "...    ...   ...  ...  ...   ...  ...  ...   ...  ...  ...  ...           ...   \n",
       "10201  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0  ...             0   \n",
       "10202  0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  1.0  1.0  ...             0   \n",
       "10203  0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  0.0  1.0  ...             0   \n",
       "10204  0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  0.0  1.0  ...             0   \n",
       "10205  0.0   0.0  0.0  0.0   0.0  0.0  1.0   1.0  0.0  1.0  ...             0   \n",
       "\n",
       "       ln_cluster_4  packs_per_year  smoking_status  subsite_BOT  subsite_GPS  \\\n",
       "id                                                                              \n",
       "3                 0             0.0             0.0            1            0   \n",
       "5                 0            38.0             1.0            1            0   \n",
       "6                 0            35.0             1.0            1            0   \n",
       "7                 0             0.0             1.0            0            0   \n",
       "8                 0             0.0             0.0            0            0   \n",
       "...             ...             ...             ...          ...          ...   \n",
       "10201             0            30.0             1.0            1            0   \n",
       "10202             0            30.0             1.0            0            0   \n",
       "10203             0             0.0             0.0            0            0   \n",
       "10204             0             5.0             0.5            0            0   \n",
       "10205             0             0.0             0.0            1            0   \n",
       "\n",
       "       subsite_NOS  subsite_Soft palate  subsite_Tonsil  total_dose  \n",
       "id                                                                   \n",
       "3                0                    0               0       66.00  \n",
       "5                0                    0               0       72.00  \n",
       "6                0                    0               0       70.00  \n",
       "7                1                    0               0       70.00  \n",
       "8                0                    0               1       66.00  \n",
       "...            ...                  ...             ...         ...  \n",
       "10201            0                    0               0       70.00  \n",
       "10202            1                    0               0       72.00  \n",
       "10203            0                    0               1       70.00  \n",
       "10204            0                    0               1       69.96  \n",
       "10205            0                    0               0       69.96  \n",
       "\n",
       "[536 rows x 62 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states['baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakup_state_models(state_model):\n",
    "    #for state 1 and 2\n",
    "    models = {}\n",
    "    models['pd'] = lambda x: state_model(x)[0]\n",
    "    models['nd'] = lambda x: state_model(x)[1]\n",
    "    models['chemo'] = lambda x: state_model(x)[2]\n",
    "    for i,dlt in enumerate(Const.dlt1):\n",
    "        models[dlt] = lambda x: state_model(x)[3][:,i]\n",
    "    return models\n",
    "\n",
    "def breakup_outcome_models(omodel):\n",
    "    models = {}\n",
    "    for i,name in enumerate(Const.outcomes):\n",
    "        models[name] = lambda x: omodel(x)[:,i].reshape(-1,1)\n",
    "    return models\n",
    "\n",
    "def get_all_models(m1,m2,m3):\n",
    "    state1_models = breakup_state_models(m1)\n",
    "    state2_models = breakup_state_models(m2)\n",
    "    state3_models = breakup_outcome_models(m3)\n",
    "    all_models = {}\n",
    "    for i,sm in enumerate([state1_models,state2_models,state3_models]):\n",
    "        for ii,m in sm.items():\n",
    "            all_models[ii +  '_state' + str(i+1)] = m\n",
    "    return all_models\n",
    "\n",
    "all_models = get_all_models(model,model2,model3)\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ytrue(name,df):\n",
    "    outcomes=None\n",
    "    value = None\n",
    "    if name == 'pd_state1':\n",
    "        outcomes = df[Const.primary_disease_states]\n",
    "    elif name == 'pd_state2':\n",
    "        outcomes = df[Const.primary_disease_states2]\n",
    "    elif name == 'nd_state1':\n",
    "        outcomes = df[Const.nodal_disease_states]\n",
    "    elif name == 'nd_state2':\n",
    "        outcomes = df[Const.nodal_disease_states2]\n",
    "    elif name == 'chemo_state1':\n",
    "        outcomes = df[Const.modifications]\n",
    "    elif name == 'chemo_state2':\n",
    "        outcomes = df[Const.ccs]   \n",
    "    if outcomes is not None:\n",
    "        value = outcomes.idxmax(axis=1)\n",
    "    if 'DLT' in name:\n",
    "        newname = name.replace('_state', ' ').replace('1','').strip()\n",
    "        value = df[newname]\n",
    "    if name.replace('_state3','') in Const.outcomes:\n",
    "        value = df[name.replace('_state3','')]\n",
    "    if value is None:\n",
    "        print(name,df.columns)\n",
    "    return value\n",
    "\n",
    "def check_impact_of_decisions(model_dict,data):\n",
    "    results = []\n",
    "    #todo: this is wrong fix it\n",
    "    ids = []\n",
    "    df = data.get_data()\n",
    "    outcomedict = {step: pd.concat(data.get_intermediate_outcomes(step=step),axis=1) for step in [1,2,3]}\n",
    "    for decision in Const.decisions:\n",
    "        for name, model in model_dict.items():\n",
    "            step = int(name[-1])\n",
    "            subset0 = dataset.get_input_state(step=step,fixed={decision: 0})\n",
    "            subset1 = dataset.get_input_state(step=step,fixed={decision: 1})\n",
    "            outcomes = outcomedict[step]\n",
    "            ids = subset0.index.values\n",
    "            x0 = df_to_torch(subset0)\n",
    "            x1 = df_to_torch(subset1)\n",
    "            y0 = model(x0).detach().cpu().numpy()\n",
    "            y1 = model(x1).detach().cpu().numpy()\n",
    "            original = data.get_input_state(step=step)\n",
    "            xx = df_to_torch(original)\n",
    "            yy = model(xx).detach().cpu().numpy()\n",
    "            ytrue = get_ytrue(name,outcomes)\n",
    "            if \"DLT\" in name:\n",
    "                y0 = y0.argmax(axis=1).reshape(-1,1)\n",
    "                y1 = y1.argmax(axis=1).reshape(-1,1)\n",
    "                yy = yy.argmax(axis=1).reshape(-1,1)\n",
    "                change = y0 - y1\n",
    "                decision_change = (y0 != y1).astype(int)\n",
    "            elif y0.shape[1] == 1:\n",
    "                change = y1 - y0\n",
    "                decision_change = np.abs((y0 > .5).astype(int) - (y1 > .5).astype(int))\n",
    "            else:\n",
    "                index = np.unravel_index(np.argmax(yy, axis=1), yy.shape)\n",
    "                change = (y0[index] - y1[index]).reshape(-1,1)\n",
    "                decision_change =  (y0.argmax(axis=1).reshape(-1,1) != y1.argmax(axis=1).reshape(-1,1)).astype(int)\n",
    "                yy = yy.argmax(axis=1).reshape(-1,1)\n",
    "                y1 = y1.argmax(axis=1).reshape(-1,1)\n",
    "                y0 = y0.argmax(axis=1).reshape(-1,1)\n",
    "            outcome = name.replace('_state','')\n",
    "            for ii,pid in enumerate(ids):\n",
    "                oo = ytrue.loc[pid]\n",
    "                onew = y0[ii][0]\n",
    "                original_decision = df.loc[pid,decision]\n",
    "                if original_decision > 0:\n",
    "                    onew = y0[ii][0]\n",
    "                oname = Const.name_dict.get(name)\n",
    "                if oname is not None:\n",
    "                    onew = oname[onew]\n",
    "                entry = {'id': pid, 'decision': decision,'outcome': outcome,'original_choice': original_decision, 'original_result': oo, 'alt_result': onew, 'change': change[ii][0], 'decision_change': decision_change[ii][0]}\n",
    "                results.append(entry)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "test = check_impact_of_decisions(all_models,dataset)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db23ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get_data()['SD Primary 2'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ce1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test[test.outcome == 'pd2'].original_result == 'SD Primary 2').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_impact_of_decisions(all_models,dataset).to_csv('../data/decision_impacts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321249c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
